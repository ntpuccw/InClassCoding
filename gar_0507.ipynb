{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f2f3b8c",
   "metadata": {},
   "source": [
    "<font color=skyblue>Neural Network</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454b75b2",
   "metadata": {},
   "source": [
    "<font color=yellow>Loading Wine Data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aad5e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_excel('data/Wine.xlsx')\n",
    "X = np.array(df.iloc[:, :-1]) # 排除最後一欄標籤 N x p\n",
    "y = np.array(df.iloc[:, -1])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "# 70% training, 30% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.30)\n",
    "\n",
    "# Standardize data\n",
    "scaler = StandardScaler()\n",
    "X_train_ = scaler.fit_transform(X_train) # 標準化訓練資料\n",
    "X_test_ = scaler.fit_transform(X_test) # 標準化測試資料"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4736f8b0",
   "metadata": {},
   "source": [
    "<font color=yellow>Neural Network 幾個重要的 Hyper-parameters</font>\n",
    "\n",
    "- hidden layer 的數量\n",
    "- hidden layer 的 neuron 數量\n",
    "- activation function\n",
    "- solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fe9a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.20921516\n",
      "Iteration 2, loss = 1.20352794\n",
      "Iteration 3, loss = 1.19797481\n",
      "Iteration 4, loss = 1.19255331\n",
      "Iteration 5, loss = 1.18726284\n",
      "Iteration 6, loss = 1.18210384\n",
      "Iteration 7, loss = 1.17707722\n",
      "Iteration 8, loss = 1.17218411\n",
      "Iteration 9, loss = 1.16742559\n",
      "Iteration 10, loss = 1.16280268\n",
      "Iteration 11, loss = 1.15831620\n",
      "Iteration 12, loss = 1.15396679\n",
      "Iteration 13, loss = 1.14975488\n",
      "Iteration 14, loss = 1.14568066\n",
      "Iteration 15, loss = 1.14174410\n",
      "Iteration 16, loss = 1.13794491\n",
      "Iteration 17, loss = 1.13428259\n",
      "Iteration 18, loss = 1.13075636\n",
      "Iteration 19, loss = 1.12736524\n",
      "Iteration 20, loss = 1.12410800\n",
      "Iteration 21, loss = 1.12098317\n",
      "Iteration 22, loss = 1.11798910\n",
      "Iteration 23, loss = 1.11512388\n",
      "Iteration 24, loss = 1.11238544\n",
      "Iteration 25, loss = 1.10977151\n",
      "Iteration 26, loss = 1.10727962\n",
      "Iteration 27, loss = 1.10490716\n",
      "Iteration 28, loss = 1.10265137\n",
      "Iteration 29, loss = 1.10050932\n",
      "Iteration 30, loss = 1.09847799\n",
      "Iteration 31, loss = 1.09655423\n",
      "Iteration 32, loss = 1.09473480\n",
      "Iteration 33, loss = 1.09301639\n",
      "Iteration 34, loss = 1.09139561\n",
      "Iteration 35, loss = 1.08986901\n",
      "Iteration 36, loss = 1.08843314\n",
      "Iteration 37, loss = 1.08708448\n",
      "Iteration 38, loss = 1.08581953\n",
      "Iteration 39, loss = 1.08463479\n",
      "Iteration 40, loss = 1.08352677\n",
      "Iteration 41, loss = 1.08249200\n",
      "Iteration 42, loss = 1.08152704\n",
      "Iteration 43, loss = 1.08062854\n",
      "Iteration 44, loss = 1.07979315\n",
      "Iteration 45, loss = 1.07901761\n",
      "Iteration 46, loss = 1.07829875\n",
      "Iteration 47, loss = 1.07763345\n",
      "Iteration 48, loss = 1.07701868\n",
      "Iteration 49, loss = 1.07645151\n",
      "Iteration 50, loss = 1.07592911\n",
      "Iteration 51, loss = 1.07544872\n",
      "Iteration 52, loss = 1.07500771\n",
      "Iteration 53, loss = 1.07460354\n",
      "Iteration 54, loss = 1.07423377\n",
      "Iteration 55, loss = 1.07389608\n",
      "Iteration 56, loss = 1.07358823\n",
      "Iteration 57, loss = 1.07330812\n",
      "Iteration 58, loss = 1.07305373\n",
      "Iteration 59, loss = 1.07282314\n",
      "Iteration 60, loss = 1.07261456\n",
      "Iteration 61, loss = 1.07242626\n",
      "Iteration 62, loss = 1.07225663\n",
      "Iteration 63, loss = 1.07210416\n",
      "Iteration 64, loss = 1.07196742\n",
      "Iteration 65, loss = 1.07184505\n",
      "Iteration 66, loss = 1.07173580\n",
      "Iteration 67, loss = 1.07163845\n",
      "Iteration 68, loss = 1.07155183\n",
      "Iteration 69, loss = 1.07147480\n",
      "Iteration 70, loss = 1.07140611\n",
      "Iteration 71, loss = 1.07134433\n",
      "Iteration 72, loss = 1.07128756\n",
      "Iteration 73, loss = 1.07123288\n",
      "Iteration 74, loss = 1.07117530\n",
      "Iteration 75, loss = 1.07110570\n",
      "Iteration 76, loss = 1.07100674\n",
      "Iteration 77, loss = 1.07084514\n",
      "Iteration 78, loss = 1.07055847\n",
      "Iteration 79, loss = 1.07003771\n",
      "Iteration 80, loss = 1.06913140\n",
      "Iteration 81, loss = 1.06781254\n",
      "Iteration 82, loss = 1.06691232\n",
      "Iteration 83, loss = 1.06790641\n",
      "Iteration 84, loss = 1.06792280\n",
      "Iteration 85, loss = 1.06708728\n",
      "Iteration 86, loss = 1.06634415\n",
      "Iteration 87, loss = 1.06624677\n",
      "Iteration 88, loss = 1.06641953\n",
      "Iteration 89, loss = 1.06642413\n",
      "Iteration 90, loss = 1.06614991\n",
      "Iteration 91, loss = 1.06566316\n",
      "Iteration 92, loss = 1.06516099\n",
      "Iteration 93, loss = 1.06489242\n",
      "Iteration 94, loss = 1.06485774\n",
      "Iteration 95, loss = 1.06470735\n",
      "Iteration 96, loss = 1.06419212\n",
      "Iteration 97, loss = 1.06334014\n",
      "Iteration 98, loss = 1.06218499\n",
      "Iteration 99, loss = 1.06095154\n",
      "Iteration 100, loss = 1.06287476\n",
      "Iteration 101, loss = 1.06027824\n",
      "Iteration 102, loss = 1.05877604\n",
      "Iteration 103, loss = 1.05826267\n",
      "Iteration 104, loss = 1.05772612\n",
      "Iteration 105, loss = 1.05679695\n",
      "Iteration 106, loss = 1.05521763\n",
      "Iteration 107, loss = 1.05298372\n",
      "Iteration 108, loss = 1.05110674\n",
      "Iteration 109, loss = 1.05062148\n",
      "Iteration 110, loss = 1.04896029\n",
      "Iteration 111, loss = 1.04636512\n",
      "Iteration 112, loss = 1.04449478\n",
      "Iteration 113, loss = 1.04294912\n",
      "Iteration 114, loss = 1.04081553\n",
      "Iteration 115, loss = 1.03786768\n",
      "Iteration 116, loss = 1.03437199\n",
      "Iteration 117, loss = 1.03044367\n",
      "Iteration 118, loss = 1.02545559\n",
      "Iteration 119, loss = 1.01950386\n",
      "Iteration 120, loss = 1.01417156\n",
      "Iteration 121, loss = 1.01153418\n",
      "Iteration 122, loss = 1.01479290\n",
      "Iteration 123, loss = 1.01342274\n",
      "Iteration 124, loss = 1.00897745\n",
      "Iteration 125, loss = 1.00526174\n",
      "Iteration 126, loss = 1.00337991\n",
      "Iteration 127, loss = 1.00234430\n",
      "Iteration 128, loss = 1.00148400\n",
      "Iteration 129, loss = 1.00044118\n",
      "Iteration 130, loss = 0.99905850\n",
      "Iteration 131, loss = 0.99740097\n",
      "Iteration 132, loss = 0.99558794\n",
      "Iteration 133, loss = 0.99367111\n",
      "Iteration 134, loss = 0.99167285\n",
      "Iteration 135, loss = 0.98964816\n",
      "Iteration 136, loss = 0.98770217\n",
      "Iteration 137, loss = 0.98598835\n",
      "Iteration 138, loss = 0.98463989\n",
      "Iteration 139, loss = 0.98355218\n",
      "Iteration 140, loss = 0.98231403\n",
      "Iteration 141, loss = 0.98067166\n",
      "Iteration 142, loss = 0.97883174\n",
      "Iteration 143, loss = 0.97712091\n",
      "Iteration 144, loss = 0.97563925\n",
      "Iteration 145, loss = 0.97430122\n",
      "Iteration 146, loss = 0.97299980\n",
      "Iteration 147, loss = 0.97167386\n",
      "Iteration 148, loss = 0.97030410\n",
      "Iteration 149, loss = 0.96889225\n",
      "Iteration 150, loss = 0.96744617\n",
      "Iteration 151, loss = 0.96598072\n",
      "Iteration 152, loss = 0.96452712\n",
      "Iteration 153, loss = 0.96312864\n",
      "Iteration 154, loss = 0.96181192\n",
      "Iteration 155, loss = 0.96055288\n",
      "Iteration 156, loss = 0.95927782\n",
      "Iteration 157, loss = 0.95791386\n",
      "Iteration 158, loss = 0.95642849\n",
      "Iteration 159, loss = 0.95480036\n",
      "Iteration 160, loss = 0.95298470\n",
      "Iteration 161, loss = 0.95105201\n",
      "Iteration 162, loss = 0.94976813\n",
      "Iteration 163, loss = 0.94840172\n",
      "Iteration 164, loss = 0.94654233\n",
      "Iteration 165, loss = 0.94487560\n",
      "Iteration 166, loss = 0.94341556\n",
      "Iteration 167, loss = 0.94182382\n",
      "Iteration 168, loss = 0.94003039\n",
      "Iteration 169, loss = 0.93821704\n",
      "Iteration 170, loss = 0.93661436\n",
      "Iteration 171, loss = 0.93500877\n",
      "Iteration 172, loss = 0.93317768\n",
      "Iteration 173, loss = 0.93130662\n",
      "Iteration 174, loss = 0.92949849\n",
      "Iteration 175, loss = 0.92755549\n",
      "Iteration 176, loss = 0.92525325\n",
      "Iteration 177, loss = 0.92243362\n",
      "Iteration 178, loss = 0.91891704\n",
      "Iteration 179, loss = 0.91463637\n",
      "Iteration 180, loss = 0.90965968\n",
      "Iteration 181, loss = 0.90326192\n",
      "Iteration 182, loss = 0.89387682\n",
      "Iteration 183, loss = 0.88417111\n",
      "Iteration 184, loss = 0.88020545\n",
      "Iteration 185, loss = 0.88078758\n",
      "Iteration 186, loss = 0.88185792\n",
      "Iteration 187, loss = 0.88053481\n",
      "Iteration 188, loss = 0.87697105\n",
      "Iteration 189, loss = 0.87182171\n",
      "Iteration 190, loss = 0.86688334\n",
      "Iteration 191, loss = 0.86356493\n",
      "Iteration 192, loss = 0.86153292\n",
      "Iteration 193, loss = 0.86013829\n",
      "Iteration 194, loss = 0.85896581\n",
      "Iteration 195, loss = 0.85717857\n",
      "Iteration 196, loss = 0.85424700\n",
      "Iteration 197, loss = 0.85076126\n",
      "Iteration 198, loss = 0.84768444\n",
      "Iteration 199, loss = 0.84517956\n",
      "Iteration 200, loss = 0.84303784\n",
      "Iteration 201, loss = 0.84124147\n",
      "Iteration 202, loss = 0.83951336\n",
      "Iteration 203, loss = 0.83757682\n",
      "Iteration 204, loss = 0.83550092\n",
      "Iteration 205, loss = 0.83328843\n",
      "Iteration 206, loss = 0.83079670\n",
      "Iteration 207, loss = 0.82810725\n",
      "Iteration 208, loss = 0.82501074\n",
      "Iteration 209, loss = 0.82070570\n",
      "Iteration 210, loss = 0.81419498\n",
      "Iteration 211, loss = 0.80574202\n",
      "Iteration 212, loss = 0.80255557\n",
      "Iteration 213, loss = 0.80440434\n",
      "Iteration 214, loss = 0.79830929\n",
      "Iteration 215, loss = 0.79472813\n",
      "Iteration 216, loss = 0.79377026\n",
      "Iteration 217, loss = 0.79235374\n",
      "Iteration 218, loss = 0.78987677\n",
      "Iteration 219, loss = 0.78686809\n",
      "Iteration 220, loss = 0.78350083\n",
      "Iteration 221, loss = 0.77981242\n",
      "Iteration 222, loss = 0.77590907\n",
      "Iteration 223, loss = 0.77232838\n",
      "Iteration 224, loss = 0.77195478\n",
      "Iteration 225, loss = 0.76775963\n",
      "Iteration 226, loss = 0.76427937\n",
      "Iteration 227, loss = 0.76170956\n",
      "Iteration 228, loss = 0.75804593\n",
      "Iteration 229, loss = 0.75232867\n",
      "Iteration 230, loss = 0.74694722\n",
      "Iteration 231, loss = 0.74957282\n",
      "Iteration 232, loss = 0.74133579\n",
      "Iteration 233, loss = 0.73852431\n",
      "Iteration 234, loss = 0.73715338\n",
      "Iteration 235, loss = 0.73458474\n",
      "Iteration 236, loss = 0.73163675\n",
      "Iteration 237, loss = 0.72868460\n",
      "Iteration 238, loss = 0.72494723\n",
      "Iteration 239, loss = 0.72092278\n",
      "Iteration 240, loss = 0.71795236\n",
      "Iteration 241, loss = 0.71619755\n",
      "Iteration 242, loss = 0.71390719\n",
      "Iteration 243, loss = 0.71059986\n",
      "Iteration 244, loss = 0.70745674\n",
      "Iteration 245, loss = 0.70502101\n",
      "Iteration 246, loss = 0.70283374\n",
      "Iteration 247, loss = 0.70016036\n",
      "Iteration 248, loss = 0.69710547\n",
      "Iteration 249, loss = 0.69442754\n",
      "Iteration 250, loss = 0.69228131\n",
      "Iteration 251, loss = 0.68991466\n",
      "Iteration 252, loss = 0.68711025\n",
      "Iteration 253, loss = 0.68455400\n",
      "Iteration 254, loss = 0.68239067\n",
      "Iteration 255, loss = 0.68006491\n",
      "Iteration 256, loss = 0.67759604\n",
      "Iteration 257, loss = 0.67523385\n",
      "Iteration 258, loss = 0.67282701\n",
      "Iteration 259, loss = 0.67041402\n",
      "Iteration 260, loss = 0.66820502\n",
      "Iteration 261, loss = 0.66599728\n",
      "Iteration 262, loss = 0.66359097\n",
      "Iteration 263, loss = 0.66133421\n",
      "Iteration 264, loss = 0.65923095\n",
      "Iteration 265, loss = 0.65698248\n",
      "Iteration 266, loss = 0.65473408\n",
      "Iteration 267, loss = 0.65254990\n",
      "Iteration 268, loss = 0.65036776\n",
      "Iteration 269, loss = 0.64831292\n",
      "Iteration 270, loss = 0.64620445\n",
      "Iteration 271, loss = 0.64404676\n",
      "Iteration 272, loss = 0.64201143\n",
      "Iteration 273, loss = 0.63994506\n",
      "Iteration 274, loss = 0.63790145\n",
      "Iteration 275, loss = 0.63587108\n",
      "Iteration 276, loss = 0.63383752\n",
      "Iteration 277, loss = 0.63188660\n",
      "Iteration 278, loss = 0.62989348\n",
      "Iteration 279, loss = 0.62790871\n",
      "Iteration 280, loss = 0.62596100\n",
      "Iteration 281, loss = 0.62401648\n",
      "Iteration 282, loss = 0.62209306\n",
      "Iteration 283, loss = 0.62014580\n",
      "Iteration 284, loss = 0.61822178\n",
      "Iteration 285, loss = 0.61632299\n",
      "Iteration 286, loss = 0.61441481\n",
      "Iteration 287, loss = 0.61252331\n",
      "Iteration 288, loss = 0.61062546\n",
      "Iteration 289, loss = 0.60875506\n",
      "Iteration 290, loss = 0.60687820\n",
      "Iteration 291, loss = 0.60500429\n",
      "Iteration 292, loss = 0.60314569\n",
      "Iteration 293, loss = 0.60129135\n",
      "Iteration 294, loss = 0.59944728\n",
      "Iteration 295, loss = 0.59759853\n",
      "Iteration 296, loss = 0.59576318\n",
      "Iteration 297, loss = 0.59393509\n",
      "Iteration 298, loss = 0.59210759\n",
      "Iteration 299, loss = 0.59028589\n",
      "Iteration 300, loss = 0.58846613\n",
      "Iteration 301, loss = 0.58665547\n",
      "Iteration 302, loss = 0.58484287\n",
      "Iteration 303, loss = 0.58303482\n",
      "Iteration 304, loss = 0.58123155\n",
      "Iteration 305, loss = 0.57942987\n",
      "Iteration 306, loss = 0.57763037\n",
      "Iteration 307, loss = 0.57583015\n",
      "Iteration 308, loss = 0.57403532\n",
      "Iteration 309, loss = 0.57223972\n",
      "Iteration 310, loss = 0.57044545\n",
      "Iteration 311, loss = 0.56865177\n",
      "Iteration 312, loss = 0.56685938\n",
      "Iteration 313, loss = 0.56506755\n",
      "Iteration 314, loss = 0.56327462\n",
      "Iteration 315, loss = 0.56148351\n",
      "Iteration 316, loss = 0.55969157\n",
      "Iteration 317, loss = 0.55789979\n",
      "Iteration 318, loss = 0.55610709\n",
      "Iteration 319, loss = 0.55431472\n",
      "Iteration 320, loss = 0.55252122\n",
      "Iteration 321, loss = 0.55072437\n",
      "Iteration 322, loss = 0.54891995\n",
      "Iteration 323, loss = 0.54709100\n",
      "Iteration 324, loss = 0.54519538\n",
      "Iteration 325, loss = 0.54318274\n",
      "Iteration 326, loss = 0.54155757\n",
      "Iteration 327, loss = 0.53945839\n",
      "Iteration 328, loss = 0.53754342\n",
      "Iteration 329, loss = 0.53576068\n",
      "Iteration 330, loss = 0.53382374\n",
      "Iteration 331, loss = 0.53175531\n",
      "Iteration 332, loss = 0.52975175\n",
      "Iteration 333, loss = 0.52786276\n",
      "Iteration 334, loss = 0.52577475\n",
      "Iteration 335, loss = 0.52374577\n",
      "Iteration 336, loss = 0.52176222\n",
      "Iteration 337, loss = 0.51967766\n",
      "Iteration 338, loss = 0.51759141\n",
      "Iteration 339, loss = 0.51554982\n",
      "Iteration 340, loss = 0.51333651\n",
      "Iteration 341, loss = 0.51108207\n",
      "Iteration 342, loss = 0.50871574\n",
      "Iteration 343, loss = 0.50662501\n",
      "Iteration 344, loss = 0.50452545\n",
      "Iteration 345, loss = 0.50211577\n",
      "Iteration 346, loss = 0.49998331\n",
      "Iteration 347, loss = 0.49776267\n",
      "Iteration 348, loss = 0.49546018\n",
      "Iteration 349, loss = 0.49315817\n",
      "Iteration 350, loss = 0.49078147\n",
      "Iteration 351, loss = 0.48852470\n",
      "Iteration 352, loss = 0.48616381\n",
      "Iteration 353, loss = 0.48369927\n",
      "Iteration 354, loss = 0.48132545\n",
      "Iteration 355, loss = 0.47894568\n",
      "Iteration 356, loss = 0.47645064\n",
      "Iteration 357, loss = 0.47397915\n",
      "Iteration 358, loss = 0.47155326\n",
      "Iteration 359, loss = 0.46906234\n",
      "Iteration 360, loss = 0.46651187\n",
      "Iteration 361, loss = 0.46402024\n",
      "Iteration 362, loss = 0.46151924\n",
      "Iteration 363, loss = 0.45897803\n",
      "Iteration 364, loss = 0.45642491\n",
      "Iteration 365, loss = 0.45389666\n",
      "Iteration 366, loss = 0.45132503\n",
      "Iteration 367, loss = 0.44873069\n",
      "Iteration 368, loss = 0.44616046\n",
      "Iteration 369, loss = 0.44358563\n",
      "Iteration 370, loss = 0.44100421\n",
      "Iteration 371, loss = 0.43846369\n",
      "Iteration 372, loss = 0.43594419\n",
      "Iteration 373, loss = 0.43343383\n",
      "Iteration 374, loss = 0.43099693\n",
      "Iteration 375, loss = 0.42857091\n",
      "Iteration 376, loss = 0.42621631\n",
      "Iteration 377, loss = 0.42391915\n",
      "Iteration 378, loss = 0.42169371\n",
      "Iteration 379, loss = 0.41965143\n",
      "Iteration 380, loss = 0.41791045\n",
      "Iteration 381, loss = 0.41659352\n",
      "Iteration 382, loss = 0.41385290\n",
      "Iteration 383, loss = 0.41121171\n",
      "Iteration 384, loss = 0.40986838\n",
      "Iteration 385, loss = 0.40780521\n",
      "Iteration 386, loss = 0.40536131\n",
      "Iteration 387, loss = 0.40372323\n",
      "Iteration 388, loss = 0.40178894\n",
      "Iteration 389, loss = 0.39957326\n",
      "Iteration 390, loss = 0.39785043\n",
      "Iteration 391, loss = 0.39595079\n",
      "Iteration 392, loss = 0.39386339\n",
      "Iteration 393, loss = 0.39211408\n",
      "Iteration 394, loss = 0.39023450\n",
      "Iteration 395, loss = 0.38823285\n",
      "Iteration 396, loss = 0.38648949\n",
      "Iteration 397, loss = 0.38463789\n",
      "Iteration 398, loss = 0.38271992\n",
      "Iteration 399, loss = 0.38099512\n",
      "Iteration 400, loss = 0.37917683\n",
      "Iteration 401, loss = 0.37732537\n",
      "Iteration 402, loss = 0.37561767\n",
      "Iteration 403, loss = 0.37384283\n",
      "Iteration 404, loss = 0.37205210\n",
      "Iteration 405, loss = 0.37036142\n",
      "Iteration 406, loss = 0.36862826\n",
      "Iteration 407, loss = 0.36686599\n",
      "Iteration 408, loss = 0.36516170\n",
      "Iteration 409, loss = 0.36346275\n",
      "Iteration 410, loss = 0.36175433\n",
      "Iteration 411, loss = 0.36005629\n",
      "Iteration 412, loss = 0.35834773\n",
      "Iteration 413, loss = 0.35668787\n",
      "Iteration 414, loss = 0.35493501\n",
      "Iteration 415, loss = 0.35329652\n",
      "Iteration 416, loss = 0.35156144\n",
      "Iteration 417, loss = 0.34985852\n",
      "Iteration 418, loss = 0.34811106\n",
      "Iteration 419, loss = 0.34641161\n",
      "Iteration 420, loss = 0.34472501\n",
      "Iteration 421, loss = 0.34302319\n",
      "Iteration 422, loss = 0.34135357\n",
      "Iteration 423, loss = 0.33965479\n",
      "Iteration 424, loss = 0.33799253\n",
      "Iteration 425, loss = 0.33634744\n",
      "Iteration 426, loss = 0.33471105\n",
      "Iteration 427, loss = 0.33310074\n",
      "Iteration 428, loss = 0.33149913\n",
      "Iteration 429, loss = 0.32989729\n",
      "Iteration 430, loss = 0.32831231\n",
      "Iteration 431, loss = 0.32674597\n",
      "Iteration 432, loss = 0.32518595\n",
      "Iteration 433, loss = 0.32364331\n",
      "Iteration 434, loss = 0.32211603\n",
      "Iteration 435, loss = 0.32059235\n",
      "Iteration 436, loss = 0.31908985\n",
      "Iteration 437, loss = 0.31759195\n",
      "Iteration 438, loss = 0.31611313\n",
      "Iteration 439, loss = 0.31464487\n",
      "Iteration 440, loss = 0.31319366\n",
      "Iteration 441, loss = 0.31175934\n",
      "Iteration 442, loss = 0.31034498\n",
      "Iteration 443, loss = 0.30894923\n",
      "Iteration 444, loss = 0.30756740\n",
      "Iteration 445, loss = 0.30619732\n",
      "Iteration 446, loss = 0.30481454\n",
      "Iteration 447, loss = 0.30342140\n",
      "Iteration 448, loss = 0.30202967\n",
      "Iteration 449, loss = 0.30065974\n",
      "Iteration 450, loss = 0.29932126\n",
      "Iteration 451, loss = 0.29796929\n",
      "Iteration 452, loss = 0.29649424\n",
      "Iteration 453, loss = 0.29495102\n",
      "Iteration 454, loss = 0.29425789\n",
      "Iteration 455, loss = 0.29240070\n",
      "Iteration 456, loss = 0.29114793\n",
      "Iteration 457, loss = 0.29010768\n",
      "Iteration 458, loss = 0.28898838\n",
      "Iteration 459, loss = 0.28777983\n",
      "Iteration 460, loss = 0.28645540\n",
      "Iteration 461, loss = 0.28506958\n",
      "Iteration 462, loss = 0.28377791\n",
      "Iteration 463, loss = 0.28262673\n",
      "Iteration 464, loss = 0.28158965\n",
      "Iteration 465, loss = 0.28048321\n",
      "Iteration 466, loss = 0.27924819\n",
      "Iteration 467, loss = 0.27805867\n",
      "Iteration 468, loss = 0.27690917\n",
      "Iteration 469, loss = 0.27582052\n",
      "Iteration 470, loss = 0.27474745\n",
      "Iteration 471, loss = 0.27369554\n",
      "Iteration 472, loss = 0.27261924\n",
      "Iteration 473, loss = 0.27153753\n",
      "Iteration 474, loss = 0.27045279\n",
      "Iteration 475, loss = 0.26938336\n",
      "Iteration 476, loss = 0.26833755\n",
      "Iteration 477, loss = 0.26730423\n",
      "Iteration 478, loss = 0.26628705\n",
      "Iteration 479, loss = 0.26526465\n",
      "Iteration 480, loss = 0.26424888\n",
      "Iteration 481, loss = 0.26323107\n",
      "Iteration 482, loss = 0.26223699\n",
      "Iteration 483, loss = 0.26125200\n",
      "Iteration 484, loss = 0.26026537\n",
      "Iteration 485, loss = 0.25927804\n",
      "Iteration 486, loss = 0.25830991\n",
      "Iteration 487, loss = 0.25735876\n",
      "Iteration 488, loss = 0.25641644\n",
      "Iteration 489, loss = 0.25548008\n",
      "Iteration 490, loss = 0.25454470\n",
      "Iteration 491, loss = 0.25361648\n",
      "Iteration 492, loss = 0.25269396\n",
      "Iteration 493, loss = 0.25178564\n",
      "Iteration 494, loss = 0.25088244\n",
      "Iteration 495, loss = 0.24997842\n",
      "Iteration 496, loss = 0.24909249\n",
      "Iteration 497, loss = 0.24821678\n",
      "Iteration 498, loss = 0.24736356\n",
      "Iteration 499, loss = 0.24650811\n",
      "Iteration 500, loss = 0.24561070\n",
      "Iteration 501, loss = 0.24468921\n",
      "Iteration 502, loss = 0.24380801\n",
      "Iteration 503, loss = 0.24286857\n",
      "Iteration 504, loss = 0.24202514\n",
      "Iteration 505, loss = 0.24114791\n",
      "Iteration 506, loss = 0.24022825\n",
      "Iteration 507, loss = 0.23929135\n",
      "Iteration 508, loss = 0.23849318\n",
      "Iteration 509, loss = 0.23757974\n",
      "Iteration 510, loss = 0.23669994\n",
      "Iteration 511, loss = 0.23587787\n",
      "Iteration 512, loss = 0.23499638\n",
      "Iteration 513, loss = 0.23410868\n",
      "Iteration 514, loss = 0.23329633\n",
      "Iteration 515, loss = 0.23241124\n",
      "Iteration 516, loss = 0.23155999\n",
      "Iteration 517, loss = 0.23073533\n",
      "Iteration 518, loss = 0.22986463\n",
      "Iteration 519, loss = 0.22903337\n",
      "Iteration 520, loss = 0.22820205\n",
      "Iteration 521, loss = 0.22734984\n",
      "Iteration 522, loss = 0.22654693\n",
      "Iteration 523, loss = 0.22572225\n",
      "Iteration 524, loss = 0.22494623\n",
      "Iteration 525, loss = 0.22424272\n",
      "Iteration 526, loss = 0.22364183\n",
      "Iteration 527, loss = 0.22323909\n",
      "Iteration 528, loss = 0.22253442\n",
      "Iteration 529, loss = 0.22130262\n",
      "Iteration 530, loss = 0.21947389\n",
      "Iteration 531, loss = 0.21815943\n",
      "Iteration 532, loss = 0.21817605\n",
      "Iteration 533, loss = 0.21696114\n",
      "Iteration 534, loss = 0.21511687\n",
      "Iteration 535, loss = 0.21449328\n",
      "Iteration 536, loss = 0.21357698\n",
      "Iteration 537, loss = 0.21219946\n",
      "Iteration 538, loss = 0.21144790\n",
      "Iteration 539, loss = 0.21004617\n",
      "Iteration 540, loss = 0.20929353\n",
      "Iteration 541, loss = 0.20838683\n",
      "Iteration 542, loss = 0.20710293\n",
      "Iteration 543, loss = 0.20647779\n",
      "Iteration 544, loss = 0.20579844\n",
      "Iteration 545, loss = 0.20467167\n",
      "Iteration 546, loss = 0.20403109\n",
      "Iteration 547, loss = 0.20341054\n",
      "Iteration 548, loss = 0.20245356\n",
      "Iteration 549, loss = 0.20177247\n",
      "Iteration 550, loss = 0.20115595\n",
      "Iteration 551, loss = 0.20035069\n",
      "Iteration 552, loss = 0.19963051\n",
      "Iteration 553, loss = 0.19903634\n",
      "Iteration 554, loss = 0.19831450\n",
      "Iteration 555, loss = 0.19758642\n",
      "Iteration 556, loss = 0.19697043\n",
      "Iteration 557, loss = 0.19632533\n",
      "Iteration 558, loss = 0.19563712\n",
      "Iteration 559, loss = 0.19497729\n",
      "Iteration 560, loss = 0.19434241\n",
      "Iteration 561, loss = 0.19371048\n",
      "Iteration 562, loss = 0.19306574\n",
      "Iteration 563, loss = 0.19241629\n",
      "Iteration 564, loss = 0.19178559\n",
      "Iteration 565, loss = 0.19116049\n",
      "Iteration 566, loss = 0.19050408\n",
      "Iteration 567, loss = 0.18984244\n",
      "Iteration 568, loss = 0.18915041\n",
      "Iteration 569, loss = 0.18836961\n",
      "Iteration 570, loss = 0.18772965\n",
      "Iteration 571, loss = 0.18728026\n",
      "Iteration 572, loss = 0.18656680\n",
      "Iteration 573, loss = 0.18583659\n",
      "Iteration 574, loss = 0.18522809\n",
      "Iteration 575, loss = 0.18466527\n",
      "Iteration 576, loss = 0.18404632\n",
      "Iteration 577, loss = 0.18338362\n",
      "Iteration 578, loss = 0.18272328\n",
      "Iteration 579, loss = 0.18213710\n",
      "Iteration 580, loss = 0.18154431\n",
      "Iteration 581, loss = 0.18094239\n",
      "Iteration 582, loss = 0.18031712\n",
      "Iteration 583, loss = 0.17970067\n",
      "Iteration 584, loss = 0.17908249\n",
      "Iteration 585, loss = 0.17847766\n",
      "Iteration 586, loss = 0.17787899\n",
      "Iteration 587, loss = 0.17729066\n",
      "Iteration 588, loss = 0.17669259\n",
      "Iteration 589, loss = 0.17609259\n",
      "Iteration 590, loss = 0.17548564\n",
      "Iteration 591, loss = 0.17491351\n",
      "Iteration 592, loss = 0.17437078\n",
      "Iteration 593, loss = 0.17382387\n",
      "Iteration 594, loss = 0.17329808\n",
      "Iteration 595, loss = 0.17277336\n",
      "Iteration 596, loss = 0.17220526\n",
      "Iteration 597, loss = 0.17153172\n",
      "Iteration 598, loss = 0.17082815\n",
      "Iteration 599, loss = 0.17016819\n",
      "Iteration 600, loss = 0.16958699\n",
      "Iteration 601, loss = 0.16908468\n",
      "Iteration 602, loss = 0.16858514\n",
      "Iteration 603, loss = 0.16805523\n",
      "Iteration 604, loss = 0.16746552\n",
      "Iteration 605, loss = 0.16685224\n",
      "Iteration 606, loss = 0.16623947\n",
      "Iteration 607, loss = 0.16567929\n",
      "Iteration 608, loss = 0.16516081\n",
      "Iteration 609, loss = 0.16465919\n",
      "Iteration 610, loss = 0.16415170\n",
      "Iteration 611, loss = 0.16361995\n",
      "Iteration 612, loss = 0.16306798\n",
      "Iteration 613, loss = 0.16250605\n",
      "Iteration 614, loss = 0.16194916\n",
      "Iteration 615, loss = 0.16141804\n",
      "Iteration 616, loss = 0.16090781\n",
      "Iteration 617, loss = 0.16041149\n",
      "Iteration 618, loss = 0.15992272\n",
      "Iteration 619, loss = 0.15943431\n",
      "Iteration 620, loss = 0.15894107\n",
      "Iteration 621, loss = 0.15844533\n",
      "Iteration 622, loss = 0.15794141\n",
      "Iteration 623, loss = 0.15743652\n",
      "Iteration 624, loss = 0.15692906\n",
      "Iteration 625, loss = 0.15642477\n",
      "Iteration 626, loss = 0.15592530\n",
      "Iteration 627, loss = 0.15543534\n",
      "Iteration 628, loss = 0.15495034\n",
      "Iteration 629, loss = 0.15447361\n",
      "Iteration 630, loss = 0.15400017\n",
      "Iteration 631, loss = 0.15353226\n",
      "Iteration 632, loss = 0.15306972\n",
      "Iteration 633, loss = 0.15261140\n",
      "Iteration 634, loss = 0.15216274\n",
      "Iteration 635, loss = 0.15172475\n",
      "Iteration 636, loss = 0.15131092\n",
      "Iteration 637, loss = 0.15092860\n",
      "Iteration 638, loss = 0.15061114\n",
      "Iteration 639, loss = 0.15030798\n",
      "Iteration 640, loss = 0.15002393\n",
      "Iteration 641, loss = 0.14951943\n",
      "Iteration 642, loss = 0.14887822\n",
      "Iteration 643, loss = 0.14819450\n",
      "Iteration 644, loss = 0.14770646\n",
      "Iteration 645, loss = 0.14741162\n",
      "Iteration 646, loss = 0.14712885\n",
      "Iteration 647, loss = 0.14672183\n",
      "Iteration 648, loss = 0.14614871\n",
      "Iteration 649, loss = 0.14560590\n",
      "Iteration 650, loss = 0.14521120\n",
      "Iteration 651, loss = 0.14490484\n",
      "Iteration 652, loss = 0.14455906\n",
      "Iteration 653, loss = 0.14409046\n",
      "Iteration 654, loss = 0.14358876\n",
      "Iteration 655, loss = 0.14315387\n",
      "Iteration 656, loss = 0.14280708\n",
      "Iteration 657, loss = 0.14247220\n",
      "Iteration 658, loss = 0.14207402\n",
      "Iteration 659, loss = 0.14162715\n",
      "Iteration 660, loss = 0.14118580\n",
      "Iteration 661, loss = 0.14079812\n",
      "Iteration 662, loss = 0.14045123\n",
      "Iteration 663, loss = 0.14009570\n",
      "Iteration 664, loss = 0.13970819\n",
      "Iteration 665, loss = 0.13929246\n",
      "Iteration 666, loss = 0.13888393\n",
      "Iteration 667, loss = 0.13850418\n",
      "Iteration 668, loss = 0.13814933\n",
      "Iteration 669, loss = 0.13779867\n",
      "Iteration 670, loss = 0.13743356\n",
      "Iteration 671, loss = 0.13705386\n",
      "Iteration 672, loss = 0.13666556\n",
      "Iteration 673, loss = 0.13628450\n",
      "Iteration 674, loss = 0.13591764\n",
      "Iteration 675, loss = 0.13556315\n",
      "Iteration 676, loss = 0.13521476\n",
      "Iteration 677, loss = 0.13486576\n",
      "Iteration 678, loss = 0.13451313\n",
      "Iteration 679, loss = 0.13415482\n",
      "Iteration 680, loss = 0.13379494\n",
      "Iteration 681, loss = 0.13343490\n",
      "Iteration 682, loss = 0.13307753\n",
      "Iteration 683, loss = 0.13272343\n",
      "Iteration 684, loss = 0.13237323\n",
      "Iteration 685, loss = 0.13202641\n",
      "Iteration 686, loss = 0.13168272\n",
      "Iteration 687, loss = 0.13134177\n",
      "Iteration 688, loss = 0.13100338\n",
      "Iteration 689, loss = 0.13066798\n",
      "Iteration 690, loss = 0.13033631\n",
      "Iteration 691, loss = 0.13001038\n",
      "Iteration 692, loss = 0.12969324\n",
      "Iteration 693, loss = 0.12939414\n",
      "Iteration 694, loss = 0.12912129\n",
      "Iteration 695, loss = 0.12890785\n",
      "Iteration 696, loss = 0.12874597\n",
      "Iteration 697, loss = 0.12868634\n",
      "Iteration 698, loss = 0.12848651\n",
      "Iteration 699, loss = 0.12812809\n",
      "Iteration 700, loss = 0.12741393\n",
      "Iteration 701, loss = 0.12678802\n",
      "Iteration 702, loss = 0.12649259\n",
      "Iteration 703, loss = 0.12642601\n",
      "Iteration 704, loss = 0.12629975\n",
      "Iteration 705, loss = 0.12585567\n",
      "Iteration 706, loss = 0.12530455\n",
      "Iteration 707, loss = 0.12492722\n",
      "Iteration 708, loss = 0.12477350\n",
      "Iteration 709, loss = 0.12461144\n",
      "Iteration 710, loss = 0.12423780\n",
      "Iteration 711, loss = 0.12378431\n",
      "Iteration 712, loss = 0.12344431\n",
      "Iteration 713, loss = 0.12324487\n",
      "Iteration 714, loss = 0.12303136\n",
      "Iteration 715, loss = 0.12268987\n",
      "Iteration 716, loss = 0.12230917\n",
      "Iteration 717, loss = 0.12200370\n",
      "Iteration 718, loss = 0.12177730\n",
      "Iteration 719, loss = 0.12153926\n",
      "Iteration 720, loss = 0.12122696\n",
      "Iteration 721, loss = 0.12089012\n",
      "Iteration 722, loss = 0.12059654\n",
      "Iteration 723, loss = 0.12035346\n",
      "Iteration 724, loss = 0.12010931\n",
      "Iteration 725, loss = 0.11982359\n",
      "Iteration 726, loss = 0.11951615\n",
      "Iteration 727, loss = 0.11922558\n",
      "Iteration 728, loss = 0.11896729\n",
      "Iteration 729, loss = 0.11872099\n",
      "Iteration 730, loss = 0.11845880\n",
      "Iteration 731, loss = 0.11817770\n",
      "Iteration 732, loss = 0.11789357\n",
      "Iteration 733, loss = 0.11762465\n",
      "Iteration 734, loss = 0.11737156\n",
      "Iteration 735, loss = 0.11712155\n",
      "Iteration 736, loss = 0.11686370\n",
      "Iteration 737, loss = 0.11659650\n",
      "Iteration 738, loss = 0.11632832\n",
      "Iteration 739, loss = 0.11606701\n",
      "Iteration 740, loss = 0.11581444\n",
      "Iteration 741, loss = 0.11556644\n",
      "Iteration 742, loss = 0.11531724\n",
      "Iteration 743, loss = 0.11506434\n",
      "Iteration 744, loss = 0.11480861\n",
      "Iteration 745, loss = 0.11455365\n",
      "Iteration 746, loss = 0.11430206\n",
      "Iteration 747, loss = 0.11405468\n",
      "Iteration 748, loss = 0.11381044\n",
      "Iteration 749, loss = 0.11356747\n",
      "Iteration 750, loss = 0.11332434\n",
      "Iteration 751, loss = 0.11308047\n",
      "Iteration 752, loss = 0.11283631\n",
      "Iteration 753, loss = 0.11259253\n",
      "Iteration 754, loss = 0.11234999\n",
      "Iteration 755, loss = 0.11210914\n",
      "Iteration 756, loss = 0.11187011\n",
      "Iteration 757, loss = 0.11163276\n",
      "Iteration 758, loss = 0.11139685\n",
      "Iteration 759, loss = 0.11116210\n",
      "Iteration 760, loss = 0.11092831\n",
      "Iteration 761, loss = 0.11069541\n",
      "Iteration 762, loss = 0.11046331\n",
      "Iteration 763, loss = 0.11023213\n",
      "Iteration 764, loss = 0.11000186\n",
      "Iteration 765, loss = 0.10977272\n",
      "Iteration 766, loss = 0.10954473\n",
      "Iteration 767, loss = 0.10931827\n",
      "Iteration 768, loss = 0.10909343\n",
      "Iteration 769, loss = 0.10887098\n",
      "Iteration 770, loss = 0.10865123\n",
      "Iteration 771, loss = 0.10843608\n",
      "Iteration 772, loss = 0.10822636\n",
      "Iteration 773, loss = 0.10802697\n",
      "Iteration 774, loss = 0.10783880\n",
      "Iteration 775, loss = 0.10767333\n",
      "Iteration 776, loss = 0.10752365\n",
      "Iteration 777, loss = 0.10740738\n",
      "Iteration 778, loss = 0.10727189\n",
      "Iteration 779, loss = 0.10712362\n",
      "Iteration 780, loss = 0.10685008\n",
      "Iteration 781, loss = 0.10650583\n",
      "Iteration 782, loss = 0.10611460\n",
      "Iteration 783, loss = 0.10579676\n",
      "Iteration 784, loss = 0.10559072\n",
      "Iteration 785, loss = 0.10546334\n",
      "Iteration 786, loss = 0.10534678\n",
      "Iteration 787, loss = 0.10516420\n",
      "Iteration 788, loss = 0.10491038\n",
      "Iteration 789, loss = 0.10460716\n",
      "Iteration 790, loss = 0.10433314\n",
      "Iteration 791, loss = 0.10412511\n",
      "Iteration 792, loss = 0.10396743\n",
      "Iteration 793, loss = 0.10381544\n",
      "Iteration 794, loss = 0.10362558\n",
      "Iteration 795, loss = 0.10339559\n",
      "Iteration 796, loss = 0.10314235\n",
      "Iteration 797, loss = 0.10290524\n",
      "Iteration 798, loss = 0.10270310\n",
      "Iteration 799, loss = 0.10252820\n",
      "Iteration 800, loss = 0.10235759\n",
      "Iteration 801, loss = 0.10216957\n",
      "Iteration 802, loss = 0.10196136\n",
      "Iteration 803, loss = 0.10173913\n",
      "Iteration 804, loss = 0.10152035\n",
      "Iteration 805, loss = 0.10131630\n",
      "Iteration 806, loss = 0.10112834\n",
      "Iteration 807, loss = 0.10094856\n",
      "Iteration 808, loss = 0.10076670\n",
      "Iteration 809, loss = 0.10057755\n",
      "Iteration 810, loss = 0.10037914\n",
      "Iteration 811, loss = 0.10017605\n",
      "Iteration 812, loss = 0.09997343\n",
      "Iteration 813, loss = 0.09977652\n",
      "Iteration 814, loss = 0.09958653\n",
      "Iteration 815, loss = 0.09940165\n",
      "Iteration 816, loss = 0.09921926\n",
      "Iteration 817, loss = 0.09903673\n",
      "Iteration 818, loss = 0.09885242\n",
      "Iteration 819, loss = 0.09866523\n",
      "Iteration 820, loss = 0.09847635\n",
      "Iteration 821, loss = 0.09828669\n",
      "Iteration 822, loss = 0.09809760\n",
      "Iteration 823, loss = 0.09790969\n",
      "Iteration 824, loss = 0.09772357\n",
      "Iteration 825, loss = 0.09753929\n",
      "Iteration 826, loss = 0.09735659\n",
      "Iteration 827, loss = 0.09717524\n",
      "Iteration 828, loss = 0.09699503\n",
      "Iteration 829, loss = 0.09681574\n",
      "Iteration 830, loss = 0.09663721\n",
      "Iteration 831, loss = 0.09645952\n",
      "Iteration 832, loss = 0.09628268\n",
      "Iteration 833, loss = 0.09610693\n",
      "Iteration 834, loss = 0.09593235\n",
      "Iteration 835, loss = 0.09575970\n",
      "Iteration 836, loss = 0.09558922\n",
      "Iteration 837, loss = 0.09542255\n",
      "Iteration 838, loss = 0.09526005\n",
      "Iteration 839, loss = 0.09510529\n",
      "Iteration 840, loss = 0.09495794\n",
      "Iteration 841, loss = 0.09482522\n",
      "Iteration 842, loss = 0.09470192\n",
      "Iteration 843, loss = 0.09459931\n",
      "Iteration 844, loss = 0.09449072\n",
      "Iteration 845, loss = 0.09438362\n",
      "Iteration 846, loss = 0.09421341\n",
      "Iteration 847, loss = 0.09399684\n",
      "Iteration 848, loss = 0.09370700\n",
      "Iteration 849, loss = 0.09341522\n",
      "Iteration 850, loss = 0.09316467\n",
      "Iteration 851, loss = 0.09298476\n",
      "Iteration 852, loss = 0.09286212\n",
      "Iteration 853, loss = 0.09275999\n",
      "Iteration 854, loss = 0.09264426\n",
      "Iteration 855, loss = 0.09248101\n",
      "Iteration 856, loss = 0.09227823\n",
      "Iteration 857, loss = 0.09205097\n",
      "Iteration 858, loss = 0.09183807\n",
      "Iteration 859, loss = 0.09165890\n",
      "Iteration 860, loss = 0.09151238\n",
      "Iteration 861, loss = 0.09138197\n",
      "Iteration 862, loss = 0.09124646\n",
      "Iteration 863, loss = 0.09109405\n",
      "Iteration 864, loss = 0.09091829\n",
      "Iteration 865, loss = 0.09073088\n",
      "Iteration 866, loss = 0.09054375\n",
      "Iteration 867, loss = 0.09036894\n",
      "Iteration 868, loss = 0.09020929\n",
      "Iteration 869, loss = 0.09006087\n",
      "Iteration 870, loss = 0.08991670\n",
      "Iteration 871, loss = 0.08976944\n",
      "Iteration 872, loss = 0.08961563\n",
      "Iteration 873, loss = 0.08945357\n",
      "Iteration 874, loss = 0.08928704\n",
      "Iteration 875, loss = 0.08911919\n",
      "Iteration 876, loss = 0.08895396\n",
      "Iteration 877, loss = 0.08879311\n",
      "Iteration 878, loss = 0.08863703\n",
      "Iteration 879, loss = 0.08848476\n",
      "Iteration 880, loss = 0.08833475\n",
      "Iteration 881, loss = 0.08818560\n",
      "Iteration 882, loss = 0.08803607\n",
      "Iteration 883, loss = 0.08788580\n",
      "Iteration 884, loss = 0.08773426\n",
      "Iteration 885, loss = 0.08758201\n",
      "Iteration 886, loss = 0.08742891\n",
      "Iteration 887, loss = 0.08727570\n",
      "Iteration 888, loss = 0.08712235\n",
      "Iteration 889, loss = 0.08696948\n",
      "Iteration 890, loss = 0.08681705\n",
      "Iteration 891, loss = 0.08666540\n",
      "Iteration 892, loss = 0.08651440\n",
      "Iteration 893, loss = 0.08636421\n",
      "Iteration 894, loss = 0.08621470\n",
      "Iteration 895, loss = 0.08606596\n",
      "Iteration 896, loss = 0.08591794\n",
      "Iteration 897, loss = 0.08577080\n",
      "Iteration 898, loss = 0.08562458\n",
      "Iteration 899, loss = 0.08547962\n",
      "Iteration 900, loss = 0.08533616\n",
      "Iteration 901, loss = 0.08519506\n",
      "Iteration 902, loss = 0.08505700\n",
      "Iteration 903, loss = 0.08492421\n",
      "Iteration 904, loss = 0.08479829\n",
      "Iteration 905, loss = 0.08468506\n",
      "Iteration 906, loss = 0.08458680\n",
      "Iteration 907, loss = 0.08451642\n",
      "Iteration 908, loss = 0.08446727\n",
      "Iteration 909, loss = 0.08445502\n",
      "Iteration 910, loss = 0.08441882\n",
      "Iteration 911, loss = 0.08435222\n",
      "Iteration 912, loss = 0.08413205\n",
      "Iteration 913, loss = 0.08381490\n",
      "Iteration 914, loss = 0.08345003\n",
      "Iteration 915, loss = 0.08318234\n",
      "Iteration 916, loss = 0.08305823\n",
      "Iteration 917, loss = 0.08302955\n",
      "Iteration 918, loss = 0.08300512\n",
      "Iteration 919, loss = 0.08289090\n",
      "Iteration 920, loss = 0.08268278\n",
      "Iteration 921, loss = 0.08242313\n",
      "Iteration 922, loss = 0.08220987\n",
      "Iteration 923, loss = 0.08208380\n",
      "Iteration 924, loss = 0.08201378\n",
      "Iteration 925, loss = 0.08193633\n",
      "Iteration 926, loss = 0.08180036\n",
      "Iteration 927, loss = 0.08161476\n",
      "Iteration 928, loss = 0.08141669\n",
      "Iteration 929, loss = 0.08125310\n",
      "Iteration 930, loss = 0.08113486\n",
      "Iteration 931, loss = 0.08103774\n",
      "Iteration 932, loss = 0.08092871\n",
      "Iteration 933, loss = 0.08078766\n",
      "Iteration 934, loss = 0.08062475\n",
      "Iteration 935, loss = 0.08046118\n",
      "Iteration 936, loss = 0.08031684\n",
      "Iteration 937, loss = 0.08019390\n",
      "Iteration 938, loss = 0.08008009\n",
      "Iteration 939, loss = 0.07996062\n",
      "Iteration 940, loss = 0.07982672\n",
      "Iteration 941, loss = 0.07968205\n",
      "Iteration 942, loss = 0.07953539\n",
      "Iteration 943, loss = 0.07939602\n",
      "Iteration 944, loss = 0.07926670\n",
      "Iteration 945, loss = 0.07914402\n",
      "Iteration 946, loss = 0.07902205\n",
      "Iteration 947, loss = 0.07889573\n",
      "Iteration 948, loss = 0.07876393\n",
      "Iteration 949, loss = 0.07862829\n",
      "Iteration 950, loss = 0.07849270\n",
      "Iteration 951, loss = 0.07835999\n",
      "Iteration 952, loss = 0.07823122\n",
      "Iteration 953, loss = 0.07810562\n",
      "Iteration 954, loss = 0.07798140\n",
      "Iteration 955, loss = 0.07785672\n",
      "Iteration 956, loss = 0.07773041\n",
      "Iteration 957, loss = 0.07760249\n",
      "Iteration 958, loss = 0.07747347\n",
      "Iteration 959, loss = 0.07734433\n",
      "Iteration 960, loss = 0.07721586\n",
      "Iteration 961, loss = 0.07708862\n",
      "Iteration 962, loss = 0.07696269\n",
      "Iteration 963, loss = 0.07683785\n",
      "Iteration 964, loss = 0.07671374\n",
      "Iteration 965, loss = 0.07659000\n",
      "Iteration 966, loss = 0.07646636\n",
      "Iteration 967, loss = 0.07634261\n",
      "Iteration 968, loss = 0.07621876\n",
      "Iteration 969, loss = 0.07609478\n",
      "Iteration 970, loss = 0.07597081\n",
      "Iteration 971, loss = 0.07584669\n",
      "Iteration 972, loss = 0.07572116\n",
      "Iteration 973, loss = 0.07558607\n",
      "Iteration 974, loss = 0.07543537\n",
      "Iteration 975, loss = 0.07531536\n",
      "Iteration 976, loss = 0.07519598\n",
      "Iteration 977, loss = 0.07507821\n",
      "Iteration 978, loss = 0.07495592\n",
      "Iteration 979, loss = 0.07482370\n",
      "Iteration 980, loss = 0.07472640\n",
      "Iteration 981, loss = 0.07460598\n",
      "Iteration 982, loss = 0.07449790\n",
      "Iteration 983, loss = 0.07442217\n",
      "Iteration 984, loss = 0.07432847\n",
      "Iteration 985, loss = 0.07426428\n",
      "Iteration 986, loss = 0.07421404\n",
      "Iteration 987, loss = 0.07414592\n",
      "Iteration 988, loss = 0.07407258\n",
      "Iteration 989, loss = 0.07394537\n",
      "Iteration 990, loss = 0.07374839\n",
      "Iteration 991, loss = 0.07349566\n",
      "Iteration 992, loss = 0.07325952\n",
      "Iteration 993, loss = 0.07307014\n",
      "Iteration 994, loss = 0.07295019\n",
      "Iteration 995, loss = 0.07288226\n",
      "Iteration 996, loss = 0.07282373\n",
      "Iteration 997, loss = 0.07274331\n",
      "Iteration 998, loss = 0.07262441\n",
      "Iteration 999, loss = 0.07246552\n",
      "Iteration 1000, loss = 0.07228669\n",
      "Iteration 1001, loss = 0.07212457\n",
      "Iteration 1002, loss = 0.07199068\n",
      "Iteration 1003, loss = 0.07188670\n",
      "Iteration 1004, loss = 0.07179934\n",
      "Iteration 1005, loss = 0.07170744\n",
      "Iteration 1006, loss = 0.07159980\n",
      "Iteration 1007, loss = 0.07147466\n",
      "Iteration 1008, loss = 0.07133537\n",
      "Iteration 1009, loss = 0.07119325\n",
      "Iteration 1010, loss = 0.07106199\n",
      "Iteration 1011, loss = 0.07094302\n",
      "Iteration 1012, loss = 0.07083495\n",
      "Iteration 1013, loss = 0.07073315\n",
      "Iteration 1014, loss = 0.07062945\n",
      "Iteration 1015, loss = 0.07051993\n",
      "Iteration 1016, loss = 0.07040412\n",
      "Iteration 1017, loss = 0.07028248\n",
      "Iteration 1018, loss = 0.07015822\n",
      "Iteration 1019, loss = 0.07003599\n",
      "Iteration 1020, loss = 0.06991694\n",
      "Iteration 1021, loss = 0.06980225\n",
      "Iteration 1022, loss = 0.06969152\n",
      "Iteration 1023, loss = 0.06958282\n",
      "Iteration 1024, loss = 0.06947487\n",
      "Iteration 1025, loss = 0.06936648\n",
      "Iteration 1026, loss = 0.06925665\n",
      "Iteration 1027, loss = 0.06914545\n",
      "Iteration 1028, loss = 0.06903312\n",
      "Iteration 1029, loss = 0.06891973\n",
      "Iteration 1030, loss = 0.06880629\n",
      "Iteration 1031, loss = 0.06869282\n",
      "Iteration 1032, loss = 0.06857977\n",
      "Iteration 1033, loss = 0.06846753\n",
      "Iteration 1034, loss = 0.06835595\n",
      "Iteration 1035, loss = 0.06824505\n",
      "Iteration 1036, loss = 0.06813493\n",
      "Iteration 1037, loss = 0.06802527\n",
      "Iteration 1038, loss = 0.06791608\n",
      "Iteration 1039, loss = 0.06780735\n",
      "Iteration 1040, loss = 0.06769890\n",
      "Iteration 1041, loss = 0.06759083\n",
      "Iteration 1042, loss = 0.06748309\n",
      "Iteration 1043, loss = 0.06737571\n",
      "Iteration 1044, loss = 0.06726880\n",
      "Iteration 1045, loss = 0.06716246\n",
      "Iteration 1046, loss = 0.06705684\n",
      "Iteration 1047, loss = 0.06695233\n",
      "Iteration 1048, loss = 0.06684917\n",
      "Iteration 1049, loss = 0.06674828\n",
      "Iteration 1050, loss = 0.06665039\n",
      "Iteration 1051, loss = 0.06655761\n",
      "Iteration 1052, loss = 0.06647140\n",
      "Iteration 1053, loss = 0.06639688\n",
      "Iteration 1054, loss = 0.06633543\n",
      "Iteration 1055, loss = 0.06629770\n",
      "Iteration 1056, loss = 0.06627658\n",
      "Iteration 1057, loss = 0.06628440\n",
      "Iteration 1058, loss = 0.06627356\n",
      "Iteration 1059, loss = 0.06624132\n",
      "Iteration 1060, loss = 0.06609180\n",
      "Iteration 1061, loss = 0.06585796\n",
      "Iteration 1062, loss = 0.06555535\n",
      "Iteration 1063, loss = 0.06530103\n",
      "Iteration 1064, loss = 0.06515482\n",
      "Iteration 1065, loss = 0.06511165\n",
      "Iteration 1066, loss = 0.06511249\n",
      "Iteration 1067, loss = 0.06508044\n",
      "Iteration 1068, loss = 0.06497864\n",
      "Iteration 1069, loss = 0.06479788\n",
      "Iteration 1070, loss = 0.06460033\n",
      "Iteration 1071, loss = 0.06444345\n",
      "Iteration 1072, loss = 0.06435257\n",
      "Iteration 1073, loss = 0.06430351\n",
      "Iteration 1074, loss = 0.06424932\n",
      "Iteration 1075, loss = 0.06415903\n",
      "Iteration 1076, loss = 0.06402470\n",
      "Iteration 1077, loss = 0.06387500\n",
      "Iteration 1078, loss = 0.06374079\n",
      "Iteration 1079, loss = 0.06363952\n",
      "Iteration 1080, loss = 0.06356337\n",
      "Iteration 1081, loss = 0.06348997\n",
      "Iteration 1082, loss = 0.06340101\n",
      "Iteration 1083, loss = 0.06328984\n",
      "Iteration 1084, loss = 0.06316736\n",
      "Iteration 1085, loss = 0.06304789\n",
      "Iteration 1086, loss = 0.06294228\n",
      "Iteration 1087, loss = 0.06285081\n",
      "Iteration 1088, loss = 0.06276565\n",
      "Iteration 1089, loss = 0.06267728\n",
      "Iteration 1090, loss = 0.06257957\n",
      "Iteration 1091, loss = 0.06247388\n",
      "Iteration 1092, loss = 0.06236512\n",
      "Iteration 1093, loss = 0.06225940\n",
      "Iteration 1094, loss = 0.06215984\n",
      "Iteration 1095, loss = 0.06206602\n",
      "Iteration 1096, loss = 0.06197488\n",
      "Iteration 1097, loss = 0.06188268\n",
      "Iteration 1098, loss = 0.06178731\n",
      "Iteration 1099, loss = 0.06168845\n",
      "Iteration 1100, loss = 0.06158780\n",
      "Iteration 1101, loss = 0.06148732\n",
      "Iteration 1102, loss = 0.06138869\n",
      "Iteration 1103, loss = 0.06129251\n",
      "Iteration 1104, loss = 0.06119838\n",
      "Iteration 1105, loss = 0.06110522\n",
      "Iteration 1106, loss = 0.06101192\n",
      "Iteration 1107, loss = 0.06091779\n",
      "Iteration 1108, loss = 0.06082259\n",
      "Iteration 1109, loss = 0.06072660\n",
      "Iteration 1110, loss = 0.06063028\n",
      "Iteration 1111, loss = 0.06053420\n",
      "Iteration 1112, loss = 0.06043877\n",
      "Iteration 1113, loss = 0.06034414\n",
      "Iteration 1114, loss = 0.06025025\n",
      "Iteration 1115, loss = 0.06015693\n",
      "Iteration 1116, loss = 0.06006396\n",
      "Iteration 1117, loss = 0.05997112\n",
      "Iteration 1118, loss = 0.05987826\n",
      "Iteration 1119, loss = 0.05978532\n",
      "Iteration 1120, loss = 0.05969230\n",
      "Iteration 1121, loss = 0.05959923\n",
      "Iteration 1122, loss = 0.05950618\n",
      "Iteration 1123, loss = 0.05941320\n",
      "Iteration 1124, loss = 0.05932036\n",
      "Iteration 1125, loss = 0.05922769\n",
      "Iteration 1126, loss = 0.05913522\n",
      "Iteration 1127, loss = 0.05904295\n",
      "Iteration 1128, loss = 0.05895089\n",
      "Iteration 1129, loss = 0.05885903\n",
      "Iteration 1130, loss = 0.05876736\n",
      "Iteration 1131, loss = 0.05867586\n",
      "Iteration 1132, loss = 0.05858454\n",
      "Iteration 1133, loss = 0.05849337\n",
      "Iteration 1134, loss = 0.05840236\n",
      "Iteration 1135, loss = 0.05831150\n",
      "Iteration 1136, loss = 0.05822079\n",
      "Iteration 1137, loss = 0.05813022\n",
      "Iteration 1138, loss = 0.05803980\n",
      "Iteration 1139, loss = 0.05794953\n",
      "Iteration 1140, loss = 0.05785942\n",
      "Iteration 1141, loss = 0.05776946\n",
      "Iteration 1142, loss = 0.05767968\n",
      "Iteration 1143, loss = 0.05759009\n",
      "Iteration 1144, loss = 0.05750073\n",
      "Iteration 1145, loss = 0.05741165\n",
      "Iteration 1146, loss = 0.05732294\n",
      "Iteration 1147, loss = 0.05723475\n",
      "Iteration 1148, loss = 0.05714732\n",
      "Iteration 1149, loss = 0.05706109\n",
      "Iteration 1150, loss = 0.05697672\n",
      "Iteration 1151, loss = 0.05689556\n",
      "Iteration 1152, loss = 0.05681945\n",
      "Iteration 1153, loss = 0.05675251\n",
      "Iteration 1154, loss = 0.05669947\n",
      "Iteration 1155, loss = 0.05667218\n",
      "Iteration 1156, loss = 0.05667817\n",
      "Iteration 1157, loss = 0.05674256\n",
      "Iteration 1158, loss = 0.05684452\n",
      "Iteration 1159, loss = 0.05698869\n",
      "Iteration 1160, loss = 0.05700720\n",
      "Iteration 1161, loss = 0.05685369\n",
      "Iteration 1162, loss = 0.05642768\n",
      "Iteration 1163, loss = 0.05598241\n",
      "Iteration 1164, loss = 0.05574479\n",
      "Iteration 1165, loss = 0.05577540\n",
      "Iteration 1166, loss = 0.05591475\n",
      "Iteration 1167, loss = 0.05593248\n",
      "Iteration 1168, loss = 0.05575016\n",
      "Iteration 1169, loss = 0.05544746\n",
      "Iteration 1170, loss = 0.05524026\n",
      "Iteration 1171, loss = 0.05521200\n",
      "Iteration 1172, loss = 0.05525786\n",
      "Iteration 1173, loss = 0.05522537\n",
      "Iteration 1174, loss = 0.05505813\n",
      "Iteration 1175, loss = 0.05485751\n",
      "Iteration 1176, loss = 0.05473723\n",
      "Iteration 1177, loss = 0.05471152\n",
      "Iteration 1178, loss = 0.05469459\n",
      "Iteration 1179, loss = 0.05460561\n",
      "Iteration 1180, loss = 0.05445945\n",
      "Iteration 1181, loss = 0.05432643\n",
      "Iteration 1182, loss = 0.05425249\n",
      "Iteration 1183, loss = 0.05421286\n",
      "Iteration 1184, loss = 0.05415198\n",
      "Iteration 1185, loss = 0.05404880\n",
      "Iteration 1186, loss = 0.05392876\n",
      "Iteration 1187, loss = 0.05383204\n",
      "Iteration 1188, loss = 0.05376704\n",
      "Iteration 1189, loss = 0.05370822\n",
      "Iteration 1190, loss = 0.05362952\n",
      "Iteration 1191, loss = 0.05352989\n",
      "Iteration 1192, loss = 0.05343063\n",
      "Iteration 1193, loss = 0.05334845\n",
      "Iteration 1194, loss = 0.05328028\n",
      "Iteration 1195, loss = 0.05321006\n",
      "Iteration 1196, loss = 0.05312695\n",
      "Iteration 1197, loss = 0.05303550\n",
      "Iteration 1198, loss = 0.05294739\n",
      "Iteration 1199, loss = 0.05286913\n",
      "Iteration 1200, loss = 0.05279680\n",
      "Iteration 1201, loss = 0.05272186\n",
      "Iteration 1202, loss = 0.05264014\n",
      "Iteration 1203, loss = 0.05255467\n",
      "Iteration 1204, loss = 0.05247163\n",
      "Iteration 1205, loss = 0.05239390\n",
      "Iteration 1206, loss = 0.05231917\n",
      "Iteration 1207, loss = 0.05224309\n",
      "Iteration 1208, loss = 0.05216350\n",
      "Iteration 1209, loss = 0.05208194\n",
      "Iteration 1210, loss = 0.05200144\n",
      "Iteration 1211, loss = 0.05192370\n",
      "Iteration 1212, loss = 0.05184788\n",
      "Iteration 1213, loss = 0.05177181\n",
      "Iteration 1214, loss = 0.05169413\n",
      "Iteration 1215, loss = 0.05161519\n",
      "Iteration 1216, loss = 0.05153639\n",
      "Iteration 1217, loss = 0.05145889\n",
      "Iteration 1218, loss = 0.05138271\n",
      "Iteration 1219, loss = 0.05130692\n",
      "Iteration 1220, loss = 0.05123060\n",
      "Iteration 1221, loss = 0.05115356\n",
      "Iteration 1222, loss = 0.05107626\n",
      "Iteration 1223, loss = 0.05099939\n",
      "Iteration 1224, loss = 0.05092329\n",
      "Iteration 1225, loss = 0.05084776\n",
      "Iteration 1226, loss = 0.05077234\n",
      "Iteration 1227, loss = 0.05069668\n",
      "Iteration 1228, loss = 0.05062077\n",
      "Iteration 1229, loss = 0.05054486\n",
      "Iteration 1230, loss = 0.05046925\n",
      "Iteration 1231, loss = 0.05039405\n",
      "Iteration 1232, loss = 0.05031919\n",
      "Iteration 1233, loss = 0.05024446\n",
      "Iteration 1234, loss = 0.05016971\n",
      "Iteration 1235, loss = 0.05009490\n",
      "Iteration 1236, loss = 0.05002013\n",
      "Iteration 1237, loss = 0.04994553\n",
      "Iteration 1238, loss = 0.04987118\n",
      "Iteration 1239, loss = 0.04979707\n",
      "Iteration 1240, loss = 0.04972314\n",
      "Iteration 1241, loss = 0.04964928\n",
      "Iteration 1242, loss = 0.04957548\n",
      "Iteration 1243, loss = 0.04950174\n",
      "Iteration 1244, loss = 0.04942810\n",
      "Iteration 1245, loss = 0.04935462\n",
      "Iteration 1246, loss = 0.04928131\n",
      "Iteration 1247, loss = 0.04920818\n",
      "Iteration 1248, loss = 0.04913518\n",
      "Iteration 1249, loss = 0.04906230\n",
      "Iteration 1250, loss = 0.04898951\n",
      "Iteration 1251, loss = 0.04891681\n",
      "Iteration 1252, loss = 0.04884423\n",
      "Iteration 1253, loss = 0.04877177\n",
      "Iteration 1254, loss = 0.04869946\n",
      "Iteration 1255, loss = 0.04862729\n",
      "Iteration 1256, loss = 0.04855525\n",
      "Iteration 1257, loss = 0.04848334\n",
      "Iteration 1258, loss = 0.04841155\n",
      "Iteration 1259, loss = 0.04833986\n",
      "Iteration 1260, loss = 0.04826829\n",
      "Iteration 1261, loss = 0.04819684\n",
      "Iteration 1262, loss = 0.04812551\n",
      "Iteration 1263, loss = 0.04805430\n",
      "Iteration 1264, loss = 0.04798322\n",
      "Iteration 1265, loss = 0.04791227\n",
      "Iteration 1266, loss = 0.04784144\n",
      "Iteration 1267, loss = 0.04777073\n",
      "Iteration 1268, loss = 0.04770014\n",
      "Iteration 1269, loss = 0.04762967\n",
      "Iteration 1270, loss = 0.04755931\n",
      "Iteration 1271, loss = 0.04748908\n",
      "Iteration 1272, loss = 0.04741895\n",
      "Iteration 1273, loss = 0.04734895\n",
      "Iteration 1274, loss = 0.04727907\n",
      "Iteration 1275, loss = 0.04720929\n",
      "Iteration 1276, loss = 0.04713948\n",
      "Iteration 1277, loss = 0.04706858\n",
      "Iteration 1278, loss = 0.04698980\n",
      "Iteration 1279, loss = 0.04691584\n",
      "Iteration 1280, loss = 0.04684952\n",
      "Iteration 1281, loss = 0.04679554\n",
      "Iteration 1282, loss = 0.04673846\n",
      "Iteration 1283, loss = 0.04667382\n",
      "Iteration 1284, loss = 0.04661243\n",
      "Iteration 1285, loss = 0.04654989\n",
      "Iteration 1286, loss = 0.04647610\n",
      "Iteration 1287, loss = 0.04642376\n",
      "Iteration 1288, loss = 0.04634685\n",
      "Iteration 1289, loss = 0.04624950\n",
      "Iteration 1290, loss = 0.04616306\n",
      "Iteration 1291, loss = 0.04607933\n",
      "Iteration 1292, loss = 0.04597612\n",
      "Iteration 1293, loss = 0.04591112\n",
      "Iteration 1294, loss = 0.04583845\n",
      "Iteration 1295, loss = 0.04575523\n",
      "Iteration 1296, loss = 0.04567655\n",
      "Iteration 1297, loss = 0.04561907\n",
      "Iteration 1298, loss = 0.04554344\n",
      "Iteration 1299, loss = 0.04547111\n",
      "Iteration 1300, loss = 0.04540974\n",
      "Iteration 1301, loss = 0.04534086\n",
      "Iteration 1302, loss = 0.04526609\n",
      "Iteration 1303, loss = 0.04520096\n",
      "Iteration 1304, loss = 0.04513315\n",
      "Iteration 1305, loss = 0.04505815\n",
      "Iteration 1306, loss = 0.04499271\n",
      "Iteration 1307, loss = 0.04492510\n",
      "Iteration 1308, loss = 0.04485276\n",
      "Iteration 1309, loss = 0.04478449\n",
      "Iteration 1310, loss = 0.04471886\n",
      "Iteration 1311, loss = 0.04464768\n",
      "Iteration 1312, loss = 0.04458104\n",
      "Iteration 1313, loss = 0.04451524\n",
      "Iteration 1314, loss = 0.04444635\n",
      "Iteration 1315, loss = 0.04437939\n",
      "Iteration 1316, loss = 0.04431426\n",
      "Iteration 1317, loss = 0.04424634\n",
      "Iteration 1318, loss = 0.04418009\n",
      "Iteration 1319, loss = 0.04411500\n",
      "Iteration 1320, loss = 0.04404792\n",
      "Iteration 1321, loss = 0.04398210\n",
      "Iteration 1322, loss = 0.04391717\n",
      "Iteration 1323, loss = 0.04385081\n",
      "Iteration 1324, loss = 0.04378546\n",
      "Iteration 1325, loss = 0.04372075\n",
      "Iteration 1326, loss = 0.04365509\n",
      "Iteration 1327, loss = 0.04359029\n",
      "Iteration 1328, loss = 0.04352601\n",
      "Iteration 1329, loss = 0.04346112\n",
      "Iteration 1330, loss = 0.04339710\n",
      "Iteration 1331, loss = 0.04333346\n",
      "Iteration 1332, loss = 0.04326969\n",
      "Iteration 1333, loss = 0.04320690\n",
      "Iteration 1334, loss = 0.04314475\n",
      "Iteration 1335, loss = 0.04308308\n",
      "Iteration 1336, loss = 0.04302291\n",
      "Iteration 1337, loss = 0.04296405\n",
      "Iteration 1338, loss = 0.04290713\n",
      "Iteration 1339, loss = 0.04285308\n",
      "Iteration 1340, loss = 0.04280288\n",
      "Iteration 1341, loss = 0.04275699\n",
      "Iteration 1342, loss = 0.04271803\n",
      "Iteration 1343, loss = 0.04268478\n",
      "Iteration 1344, loss = 0.04266079\n",
      "Iteration 1345, loss = 0.04264029\n",
      "Iteration 1346, loss = 0.04262554\n",
      "Iteration 1347, loss = 0.04259832\n",
      "Iteration 1348, loss = 0.04255714\n",
      "Iteration 1349, loss = 0.04247660\n",
      "Iteration 1350, loss = 0.04236614\n",
      "Iteration 1351, loss = 0.04222508\n",
      "Iteration 1352, loss = 0.04208370\n",
      "Iteration 1353, loss = 0.04196324\n",
      "Iteration 1354, loss = 0.04187940\n",
      "Iteration 1355, loss = 0.04183031\n",
      "Iteration 1356, loss = 0.04180189\n",
      "Iteration 1357, loss = 0.04177600\n",
      "Iteration 1358, loss = 0.04173539\n",
      "Iteration 1359, loss = 0.04167377\n",
      "Iteration 1360, loss = 0.04159047\n",
      "Iteration 1361, loss = 0.04149799\n",
      "Iteration 1362, loss = 0.04140879\n",
      "Iteration 1363, loss = 0.04133334\n",
      "Iteration 1364, loss = 0.04127394\n",
      "Iteration 1365, loss = 0.04122586\n",
      "Iteration 1366, loss = 0.04118088\n",
      "Iteration 1367, loss = 0.04113098\n",
      "Iteration 1368, loss = 0.04107239\n",
      "Iteration 1369, loss = 0.04100485\n",
      "Iteration 1370, loss = 0.04093291\n",
      "Iteration 1371, loss = 0.04086157\n",
      "Iteration 1372, loss = 0.04079510\n",
      "Iteration 1373, loss = 0.04073487\n",
      "Iteration 1374, loss = 0.04067959\n",
      "Iteration 1375, loss = 0.04062639\n",
      "Iteration 1376, loss = 0.04057223\n",
      "Iteration 1377, loss = 0.04051521\n",
      "Iteration 1378, loss = 0.04045476\n",
      "Iteration 1379, loss = 0.04039205\n",
      "Iteration 1380, loss = 0.04032871\n",
      "Iteration 1381, loss = 0.04026645\n",
      "Iteration 1382, loss = 0.04020624\n",
      "Iteration 1383, loss = 0.04014819\n",
      "Iteration 1384, loss = 0.04009169\n",
      "Iteration 1385, loss = 0.04003577\n",
      "Iteration 1386, loss = 0.03997953\n",
      "Iteration 1387, loss = 0.03992238\n",
      "Iteration 1388, loss = 0.03986424\n",
      "Iteration 1389, loss = 0.03980534\n",
      "Iteration 1390, loss = 0.03974619\n",
      "Iteration 1391, loss = 0.03968726\n",
      "Iteration 1392, loss = 0.03962892\n",
      "Iteration 1393, loss = 0.03957131\n",
      "Iteration 1394, loss = 0.03951436\n",
      "Iteration 1395, loss = 0.03945788\n",
      "Iteration 1396, loss = 0.03940160\n",
      "Iteration 1397, loss = 0.03934531\n",
      "Iteration 1398, loss = 0.03928886\n",
      "Iteration 1399, loss = 0.03923220\n",
      "Iteration 1400, loss = 0.03917538\n",
      "Iteration 1401, loss = 0.03911849\n",
      "Iteration 1402, loss = 0.03906164\n",
      "Iteration 1403, loss = 0.03900493\n",
      "Iteration 1404, loss = 0.03894844\n",
      "Iteration 1405, loss = 0.03889219\n",
      "Iteration 1406, loss = 0.03883617\n",
      "Iteration 1407, loss = 0.03878035\n",
      "Iteration 1408, loss = 0.03872469\n",
      "Iteration 1409, loss = 0.03866914\n",
      "Iteration 1410, loss = 0.03861366\n",
      "Iteration 1411, loss = 0.03855822\n",
      "Iteration 1412, loss = 0.03850282\n",
      "Iteration 1413, loss = 0.03844745\n",
      "Iteration 1414, loss = 0.03839212\n",
      "Iteration 1415, loss = 0.03833686\n",
      "Iteration 1416, loss = 0.03828166\n",
      "Iteration 1417, loss = 0.03822656\n",
      "Iteration 1418, loss = 0.03817155\n",
      "Iteration 1419, loss = 0.03811664\n",
      "Iteration 1420, loss = 0.03806185\n",
      "Iteration 1421, loss = 0.03800716\n",
      "Iteration 1422, loss = 0.03795257\n",
      "Iteration 1423, loss = 0.03789809\n",
      "Iteration 1424, loss = 0.03784370\n",
      "Iteration 1425, loss = 0.03778941\n",
      "Iteration 1426, loss = 0.03773522\n",
      "Iteration 1427, loss = 0.03768111\n",
      "Iteration 1428, loss = 0.03762708\n",
      "Iteration 1429, loss = 0.03757314\n",
      "Iteration 1430, loss = 0.03751929\n",
      "Iteration 1431, loss = 0.03746552\n",
      "Iteration 1432, loss = 0.03741184\n",
      "Iteration 1433, loss = 0.03735824\n",
      "Iteration 1434, loss = 0.03730473\n",
      "Iteration 1435, loss = 0.03725130\n",
      "Iteration 1436, loss = 0.03719796\n",
      "Iteration 1437, loss = 0.03714471\n",
      "Iteration 1438, loss = 0.03709155\n",
      "Iteration 1439, loss = 0.03703848\n",
      "Iteration 1440, loss = 0.03698550\n",
      "Iteration 1441, loss = 0.03693262\n",
      "Iteration 1442, loss = 0.03687983\n",
      "Iteration 1443, loss = 0.03682714\n",
      "Iteration 1444, loss = 0.03677456\n",
      "Iteration 1445, loss = 0.03672209\n",
      "Iteration 1446, loss = 0.03666973\n",
      "Iteration 1447, loss = 0.03661750\n",
      "Iteration 1448, loss = 0.03656540\n",
      "Iteration 1449, loss = 0.03651347\n",
      "Iteration 1450, loss = 0.03646170\n",
      "Iteration 1451, loss = 0.03641015\n",
      "Iteration 1452, loss = 0.03635884\n",
      "Iteration 1453, loss = 0.03630786\n",
      "Iteration 1454, loss = 0.03625726\n",
      "Iteration 1455, loss = 0.03620719\n",
      "Iteration 1456, loss = 0.03615779\n",
      "Iteration 1457, loss = 0.03610933\n",
      "Iteration 1458, loss = 0.03606205\n",
      "Iteration 1459, loss = 0.03601653\n",
      "Iteration 1460, loss = 0.03597317\n",
      "Iteration 1461, loss = 0.03593308\n",
      "Iteration 1462, loss = 0.03589672\n",
      "Iteration 1463, loss = 0.03586613\n",
      "Iteration 1464, loss = 0.03584107\n",
      "Iteration 1465, loss = 0.03582449\n",
      "Iteration 1466, loss = 0.03581261\n",
      "Iteration 1467, loss = 0.03580766\n",
      "Iteration 1468, loss = 0.03579633\n",
      "Iteration 1469, loss = 0.03577733\n",
      "Iteration 1470, loss = 0.03572738\n",
      "Iteration 1471, loss = 0.03564915\n",
      "Iteration 1472, loss = 0.03553402\n",
      "Iteration 1473, loss = 0.03540622\n",
      "Iteration 1474, loss = 0.03528575\n",
      "Iteration 1475, loss = 0.03519560\n",
      "Iteration 1476, loss = 0.03514206\n",
      "Iteration 1477, loss = 0.03511674\n",
      "Iteration 1478, loss = 0.03510251\n",
      "Iteration 1479, loss = 0.03508052\n",
      "Iteration 1480, loss = 0.03503997\n",
      "Iteration 1481, loss = 0.03497638\n",
      "Iteration 1482, loss = 0.03489906\n",
      "Iteration 1483, loss = 0.03481996\n",
      "Iteration 1484, loss = 0.03475181\n",
      "Iteration 1485, loss = 0.03469971\n",
      "Iteration 1486, loss = 0.03466083\n",
      "Iteration 1487, loss = 0.03462718\n",
      "Iteration 1488, loss = 0.03458996\n",
      "Iteration 1489, loss = 0.03454414\n",
      "Iteration 1490, loss = 0.03448890\n",
      "Iteration 1491, loss = 0.03442868\n",
      "Iteration 1492, loss = 0.03436901\n",
      "Iteration 1493, loss = 0.03431456\n",
      "Iteration 1494, loss = 0.03426667\n",
      "Iteration 1495, loss = 0.03422362\n",
      "Iteration 1496, loss = 0.03418197\n",
      "Iteration 1497, loss = 0.03413837\n",
      "Iteration 1498, loss = 0.03409118\n",
      "Iteration 1499, loss = 0.03404050\n",
      "Iteration 1500, loss = 0.03398822\n",
      "Iteration 1501, loss = 0.03393650\n",
      "Iteration 1502, loss = 0.03388695\n",
      "Iteration 1503, loss = 0.03383998\n",
      "Iteration 1504, loss = 0.03379486\n",
      "Iteration 1505, loss = 0.03375032\n",
      "Iteration 1506, loss = 0.03370512\n",
      "Iteration 1507, loss = 0.03365860\n",
      "Iteration 1508, loss = 0.03361081\n",
      "Iteration 1509, loss = 0.03356236\n",
      "Iteration 1510, loss = 0.03351405\n",
      "Iteration 1511, loss = 0.03346649\n",
      "Iteration 1512, loss = 0.03341992\n",
      "Iteration 1513, loss = 0.03337419\n",
      "Iteration 1514, loss = 0.03332888\n",
      "Iteration 1515, loss = 0.03328355\n",
      "Iteration 1516, loss = 0.03323787\n",
      "Iteration 1517, loss = 0.03319175\n",
      "Iteration 1518, loss = 0.03314532\n",
      "Iteration 1519, loss = 0.03309883\n",
      "Iteration 1520, loss = 0.03305251\n",
      "Iteration 1521, loss = 0.03300655\n",
      "Iteration 1522, loss = 0.03296097\n",
      "Iteration 1523, loss = 0.03291571\n",
      "Iteration 1524, loss = 0.03287061\n",
      "Iteration 1525, loss = 0.03282555\n",
      "Iteration 1526, loss = 0.03278042\n",
      "Iteration 1527, loss = 0.03273521\n",
      "Iteration 1528, loss = 0.03268993\n",
      "Iteration 1529, loss = 0.03264467\n",
      "Iteration 1530, loss = 0.03259949\n",
      "Iteration 1531, loss = 0.03255447\n",
      "Iteration 1532, loss = 0.03250962\n",
      "Iteration 1533, loss = 0.03246493\n",
      "Iteration 1534, loss = 0.03242036\n",
      "Iteration 1535, loss = 0.03237589\n",
      "Iteration 1536, loss = 0.03233146\n",
      "Iteration 1537, loss = 0.03228706\n",
      "Iteration 1538, loss = 0.03224269\n",
      "Iteration 1539, loss = 0.03219835\n",
      "Iteration 1540, loss = 0.03215406\n",
      "Iteration 1541, loss = 0.03210986\n",
      "Iteration 1542, loss = 0.03206574\n",
      "Iteration 1543, loss = 0.03202173\n",
      "Iteration 1544, loss = 0.03197781\n",
      "Iteration 1545, loss = 0.03193399\n",
      "Iteration 1546, loss = 0.03189025\n",
      "Iteration 1547, loss = 0.03184658\n",
      "Iteration 1548, loss = 0.03180297\n",
      "Iteration 1549, loss = 0.03175942\n",
      "Iteration 1550, loss = 0.03171593\n",
      "Iteration 1551, loss = 0.03167250\n",
      "Iteration 1552, loss = 0.03162914\n",
      "Iteration 1553, loss = 0.03158585\n",
      "Iteration 1554, loss = 0.03154263\n",
      "Iteration 1555, loss = 0.03149949\n",
      "Iteration 1556, loss = 0.03145643\n",
      "Iteration 1557, loss = 0.03141345\n",
      "Iteration 1558, loss = 0.03137054\n",
      "Iteration 1559, loss = 0.03132770\n",
      "Iteration 1560, loss = 0.03128493\n",
      "Iteration 1561, loss = 0.03124223\n",
      "Iteration 1562, loss = 0.03119960\n",
      "Iteration 1563, loss = 0.03115703\n",
      "Iteration 1564, loss = 0.03111453\n",
      "Iteration 1565, loss = 0.03107210\n",
      "Iteration 1566, loss = 0.03102974\n",
      "Iteration 1567, loss = 0.03098744\n",
      "Iteration 1568, loss = 0.03094522\n",
      "Iteration 1569, loss = 0.03090307\n",
      "Iteration 1570, loss = 0.03086098\n",
      "Iteration 1571, loss = 0.03081897\n",
      "Iteration 1572, loss = 0.03077703\n",
      "Iteration 1573, loss = 0.03073515\n",
      "Iteration 1574, loss = 0.03069335\n",
      "Iteration 1575, loss = 0.03065161\n",
      "Iteration 1576, loss = 0.03060994\n",
      "Iteration 1577, loss = 0.03056833\n",
      "Iteration 1578, loss = 0.03052680\n",
      "Iteration 1579, loss = 0.03048533\n",
      "Iteration 1580, loss = 0.03044393\n",
      "Iteration 1581, loss = 0.03040259\n",
      "Iteration 1582, loss = 0.03036132\n",
      "Iteration 1583, loss = 0.03032012\n",
      "Iteration 1584, loss = 0.03027899\n",
      "Iteration 1585, loss = 0.03023792\n",
      "Iteration 1586, loss = 0.03019692\n",
      "Iteration 1587, loss = 0.03015599\n",
      "Iteration 1588, loss = 0.03011512\n",
      "Iteration 1589, loss = 0.03007432\n",
      "Iteration 1590, loss = 0.03003358\n",
      "Iteration 1591, loss = 0.02999291\n",
      "Iteration 1592, loss = 0.02995231\n",
      "Iteration 1593, loss = 0.02991177\n",
      "Iteration 1594, loss = 0.02987130\n",
      "Iteration 1595, loss = 0.02983090\n",
      "Iteration 1596, loss = 0.02979055\n",
      "Iteration 1597, loss = 0.02975028\n",
      "Iteration 1598, loss = 0.02971007\n",
      "Iteration 1599, loss = 0.02966992\n",
      "Iteration 1600, loss = 0.02962984\n",
      "Iteration 1601, loss = 0.02958982\n",
      "Iteration 1602, loss = 0.02954987\n",
      "Iteration 1603, loss = 0.02950998\n",
      "Iteration 1604, loss = 0.02947016\n",
      "Iteration 1605, loss = 0.02943040\n",
      "Iteration 1606, loss = 0.02939070\n",
      "Iteration 1607, loss = 0.02935107\n",
      "Iteration 1608, loss = 0.02931150\n",
      "Iteration 1609, loss = 0.02927200\n",
      "Iteration 1610, loss = 0.02923255\n",
      "Iteration 1611, loss = 0.02919318\n",
      "Iteration 1612, loss = 0.02915386\n",
      "Iteration 1613, loss = 0.02911461\n",
      "Iteration 1614, loss = 0.02907542\n",
      "Iteration 1615, loss = 0.02903630\n",
      "Iteration 1616, loss = 0.02899723\n",
      "Iteration 1617, loss = 0.02895823\n",
      "Iteration 1618, loss = 0.02891929\n",
      "Iteration 1619, loss = 0.02888042\n",
      "Iteration 1620, loss = 0.02884161\n",
      "Iteration 1621, loss = 0.02880285\n",
      "Iteration 1622, loss = 0.02876417\n",
      "Iteration 1623, loss = 0.02872554\n",
      "Iteration 1624, loss = 0.02868697\n",
      "Iteration 1625, loss = 0.02864847\n",
      "Iteration 1626, loss = 0.02861003\n",
      "Iteration 1627, loss = 0.02857165\n",
      "Iteration 1628, loss = 0.02853333\n",
      "Iteration 1629, loss = 0.02849507\n",
      "Iteration 1630, loss = 0.02845687\n",
      "Iteration 1631, loss = 0.02841874\n",
      "Iteration 1632, loss = 0.02838066\n",
      "Iteration 1633, loss = 0.02834265\n",
      "Iteration 1634, loss = 0.02830470\n",
      "Iteration 1635, loss = 0.02826680\n",
      "Iteration 1636, loss = 0.02822897\n",
      "Iteration 1637, loss = 0.02819120\n",
      "Iteration 1638, loss = 0.02815349\n",
      "Iteration 1639, loss = 0.02811585\n",
      "Iteration 1640, loss = 0.02807826\n",
      "Iteration 1641, loss = 0.02804074\n",
      "Iteration 1642, loss = 0.02800328\n",
      "Iteration 1643, loss = 0.02796588\n",
      "Iteration 1644, loss = 0.02792855\n",
      "Iteration 1645, loss = 0.02789129\n",
      "Iteration 1646, loss = 0.02785410\n",
      "Iteration 1647, loss = 0.02781698\n",
      "Iteration 1648, loss = 0.02777995\n",
      "Iteration 1649, loss = 0.02774299\n",
      "Iteration 1650, loss = 0.02770614\n",
      "Iteration 1651, loss = 0.02766940\n",
      "Iteration 1652, loss = 0.02763279\n",
      "Iteration 1653, loss = 0.02759634\n",
      "Iteration 1654, loss = 0.02756008\n",
      "Iteration 1655, loss = 0.02752408\n",
      "Iteration 1656, loss = 0.02748840\n",
      "Iteration 1657, loss = 0.02745315\n",
      "Iteration 1658, loss = 0.02741849\n",
      "Iteration 1659, loss = 0.02738458\n",
      "Iteration 1660, loss = 0.02735177\n",
      "Iteration 1661, loss = 0.02732034\n",
      "Iteration 1662, loss = 0.02729096\n",
      "Iteration 1663, loss = 0.02726407\n",
      "Iteration 1664, loss = 0.02724088\n",
      "Iteration 1665, loss = 0.02722179\n",
      "Iteration 1666, loss = 0.02720878\n",
      "Iteration 1667, loss = 0.02720108\n",
      "Iteration 1668, loss = 0.02720099\n",
      "Iteration 1669, loss = 0.02720317\n",
      "Iteration 1670, loss = 0.02720807\n",
      "Iteration 1671, loss = 0.02720076\n",
      "Iteration 1672, loss = 0.02717882\n",
      "Iteration 1673, loss = 0.02712317\n",
      "Iteration 1674, loss = 0.02704052\n",
      "Iteration 1675, loss = 0.02693352\n",
      "Iteration 1676, loss = 0.02682814\n",
      "Iteration 1677, loss = 0.02674426\n",
      "Iteration 1678, loss = 0.02669452\n",
      "Iteration 1679, loss = 0.02667527\n",
      "Iteration 1680, loss = 0.02667140\n",
      "Iteration 1681, loss = 0.02666514\n",
      "Iteration 1682, loss = 0.02664156\n",
      "Iteration 1683, loss = 0.02659757\n",
      "Iteration 1684, loss = 0.02653717\n",
      "Iteration 1685, loss = 0.02647392\n",
      "Iteration 1686, loss = 0.02641966\n",
      "Iteration 1687, loss = 0.02638075\n",
      "Iteration 1688, loss = 0.02635495\n",
      "Iteration 1689, loss = 0.02633425\n",
      "Iteration 1690, loss = 0.02631003\n",
      "Iteration 1691, loss = 0.02627666\n",
      "Iteration 1692, loss = 0.02623468\n",
      "Iteration 1693, loss = 0.02618831\n",
      "Iteration 1694, loss = 0.02614383\n",
      "Iteration 1695, loss = 0.02610527\n",
      "Iteration 1696, loss = 0.02607312\n",
      "Iteration 1697, loss = 0.02604462\n",
      "Iteration 1698, loss = 0.02601580\n",
      "Iteration 1699, loss = 0.02598375\n",
      "Iteration 1700, loss = 0.02594765\n",
      "Iteration 1701, loss = 0.02590905\n",
      "Iteration 1702, loss = 0.02587042\n",
      "Iteration 1703, loss = 0.02583391\n",
      "Iteration 1704, loss = 0.02580023\n",
      "Iteration 1705, loss = 0.02576859\n",
      "Iteration 1706, loss = 0.02573742\n",
      "Iteration 1707, loss = 0.02570522\n",
      "Iteration 1708, loss = 0.02567139\n",
      "Iteration 1709, loss = 0.02563623\n",
      "Iteration 1710, loss = 0.02560073\n",
      "Iteration 1711, loss = 0.02556585\n",
      "Iteration 1712, loss = 0.02553213\n",
      "Iteration 1713, loss = 0.02549947\n",
      "Iteration 1714, loss = 0.02546730\n",
      "Iteration 1715, loss = 0.02543498\n",
      "Iteration 1716, loss = 0.02540207\n",
      "Iteration 1717, loss = 0.02536856\n",
      "Iteration 1718, loss = 0.02533473\n",
      "Iteration 1719, loss = 0.02530101\n",
      "Iteration 1720, loss = 0.02526771\n",
      "Iteration 1721, loss = 0.02523493\n",
      "Iteration 1722, loss = 0.02520250\n",
      "Iteration 1723, loss = 0.02517018\n",
      "Iteration 1724, loss = 0.02513774\n",
      "Iteration 1725, loss = 0.02510507\n",
      "Iteration 1726, loss = 0.02507223\n",
      "Iteration 1727, loss = 0.02503935\n",
      "Iteration 1728, loss = 0.02500660\n",
      "Iteration 1729, loss = 0.02497407\n",
      "Iteration 1730, loss = 0.02494176\n",
      "Iteration 1731, loss = 0.02490960\n",
      "Iteration 1732, loss = 0.02487748\n",
      "Iteration 1733, loss = 0.02484532\n",
      "Iteration 1734, loss = 0.02481310\n",
      "Iteration 1735, loss = 0.02478086\n",
      "Iteration 1736, loss = 0.02474865\n",
      "Iteration 1737, loss = 0.02471652\n",
      "Iteration 1738, loss = 0.02468452\n",
      "Iteration 1739, loss = 0.02465262\n",
      "Iteration 1740, loss = 0.02462081\n",
      "Iteration 1741, loss = 0.02458904\n",
      "Iteration 1742, loss = 0.02455728\n",
      "Iteration 1743, loss = 0.02452553\n",
      "Iteration 1744, loss = 0.02449380\n",
      "Iteration 1745, loss = 0.02446211\n",
      "Iteration 1746, loss = 0.02443049\n",
      "Iteration 1747, loss = 0.02439894\n",
      "Iteration 1748, loss = 0.02436746\n",
      "Iteration 1749, loss = 0.02433605\n",
      "Iteration 1750, loss = 0.02430468\n",
      "Iteration 1751, loss = 0.02427335\n",
      "Iteration 1752, loss = 0.02424205\n",
      "Iteration 1753, loss = 0.02421078\n",
      "Iteration 1754, loss = 0.02417956\n",
      "Iteration 1755, loss = 0.02414840\n",
      "Iteration 1756, loss = 0.02411728\n",
      "Iteration 1757, loss = 0.02408623\n",
      "Iteration 1758, loss = 0.02405523\n",
      "Iteration 1759, loss = 0.02402428\n",
      "Iteration 1760, loss = 0.02399337\n",
      "Iteration 1761, loss = 0.02396251\n",
      "Iteration 1762, loss = 0.02393168\n",
      "Iteration 1763, loss = 0.02390090\n",
      "Iteration 1764, loss = 0.02387017\n",
      "Iteration 1765, loss = 0.02383949\n",
      "Iteration 1766, loss = 0.02380886\n",
      "Iteration 1767, loss = 0.02377828\n",
      "Iteration 1768, loss = 0.02374775\n",
      "Iteration 1769, loss = 0.02371726\n",
      "Iteration 1770, loss = 0.02368682\n",
      "Iteration 1771, loss = 0.02365642\n",
      "Iteration 1772, loss = 0.02362607\n",
      "Iteration 1773, loss = 0.02359577\n",
      "Iteration 1774, loss = 0.02356551\n",
      "Iteration 1775, loss = 0.02353530\n",
      "Iteration 1776, loss = 0.02350513\n",
      "Iteration 1777, loss = 0.02347502\n",
      "Iteration 1778, loss = 0.02344494\n",
      "Iteration 1779, loss = 0.02341492\n",
      "Iteration 1780, loss = 0.02338494\n",
      "Iteration 1781, loss = 0.02335501\n",
      "Iteration 1782, loss = 0.02332512\n",
      "Iteration 1783, loss = 0.02329527\n",
      "Iteration 1784, loss = 0.02326548\n",
      "Iteration 1785, loss = 0.02323572\n",
      "Iteration 1786, loss = 0.02320602\n",
      "Iteration 1787, loss = 0.02317636\n",
      "Iteration 1788, loss = 0.02314675\n",
      "Iteration 1789, loss = 0.02311718\n",
      "Iteration 1790, loss = 0.02308765\n",
      "Iteration 1791, loss = 0.02305817\n",
      "Iteration 1792, loss = 0.02302874\n",
      "Iteration 1793, loss = 0.02299935\n",
      "Iteration 1794, loss = 0.02297001\n",
      "Iteration 1795, loss = 0.02294071\n",
      "Iteration 1796, loss = 0.02291145\n",
      "Iteration 1797, loss = 0.02288224\n",
      "Iteration 1798, loss = 0.02285308\n",
      "Iteration 1799, loss = 0.02282396\n",
      "Iteration 1800, loss = 0.02279488\n",
      "Iteration 1801, loss = 0.02276585\n",
      "Iteration 1802, loss = 0.02273686\n",
      "Iteration 1803, loss = 0.02270792\n",
      "Iteration 1804, loss = 0.02267902\n",
      "Iteration 1805, loss = 0.02265017\n",
      "Iteration 1806, loss = 0.02262136\n",
      "Iteration 1807, loss = 0.02259259\n",
      "Iteration 1808, loss = 0.02256387\n",
      "Iteration 1809, loss = 0.02253519\n",
      "Iteration 1810, loss = 0.02250656\n",
      "Iteration 1811, loss = 0.02247797\n",
      "Iteration 1812, loss = 0.02244942\n",
      "Iteration 1813, loss = 0.02242092\n",
      "Iteration 1814, loss = 0.02239246\n",
      "Iteration 1815, loss = 0.02236405\n",
      "Iteration 1816, loss = 0.02233567\n",
      "Iteration 1817, loss = 0.02230734\n",
      "Iteration 1818, loss = 0.02227906\n",
      "Iteration 1819, loss = 0.02225082\n",
      "Iteration 1820, loss = 0.02222262\n",
      "Iteration 1821, loss = 0.02219446\n",
      "Iteration 1822, loss = 0.02216635\n",
      "Iteration 1823, loss = 0.02213828\n",
      "Iteration 1824, loss = 0.02211025\n",
      "Iteration 1825, loss = 0.02208226\n",
      "Iteration 1826, loss = 0.02205432\n",
      "Iteration 1827, loss = 0.02202642\n",
      "Iteration 1828, loss = 0.02199857\n",
      "Iteration 1829, loss = 0.02197075\n",
      "Iteration 1830, loss = 0.02194298\n",
      "Iteration 1831, loss = 0.02191525\n",
      "Iteration 1832, loss = 0.02188757\n",
      "Iteration 1833, loss = 0.02185992\n",
      "Iteration 1834, loss = 0.02183232\n",
      "Iteration 1835, loss = 0.02180476\n",
      "Iteration 1836, loss = 0.02177724\n",
      "Iteration 1837, loss = 0.02174977\n",
      "Iteration 1838, loss = 0.02172233\n",
      "Iteration 1839, loss = 0.02169494\n",
      "Iteration 1840, loss = 0.02166759\n",
      "Iteration 1841, loss = 0.02164028\n",
      "Iteration 1842, loss = 0.02161302\n",
      "Iteration 1843, loss = 0.02158579\n",
      "Iteration 1844, loss = 0.02155861\n",
      "Iteration 1845, loss = 0.02153147\n",
      "Iteration 1846, loss = 0.02150437\n",
      "Iteration 1847, loss = 0.02147731\n",
      "Iteration 1848, loss = 0.02145029\n",
      "Iteration 1849, loss = 0.02142331\n",
      "Iteration 1850, loss = 0.02139638\n",
      "Iteration 1851, loss = 0.02136948\n",
      "Iteration 1852, loss = 0.02134263\n",
      "Iteration 1853, loss = 0.02131582\n",
      "Iteration 1854, loss = 0.02128905\n",
      "Iteration 1855, loss = 0.02126232\n",
      "Iteration 1856, loss = 0.02123563\n",
      "Iteration 1857, loss = 0.02120898\n",
      "Iteration 1858, loss = 0.02118237\n",
      "Iteration 1859, loss = 0.02115580\n",
      "Iteration 1860, loss = 0.02112927\n",
      "Iteration 1861, loss = 0.02110279\n",
      "Iteration 1862, loss = 0.02107634\n",
      "Iteration 1863, loss = 0.02104993\n",
      "Iteration 1864, loss = 0.02102357\n",
      "Iteration 1865, loss = 0.02099724\n",
      "Iteration 1866, loss = 0.02097096\n",
      "Iteration 1867, loss = 0.02094471\n",
      "Iteration 1868, loss = 0.02091851\n",
      "Iteration 1869, loss = 0.02089234\n",
      "Iteration 1870, loss = 0.02086622\n",
      "Iteration 1871, loss = 0.02084013\n",
      "Iteration 1872, loss = 0.02081408\n",
      "Iteration 1873, loss = 0.02078808\n",
      "Iteration 1874, loss = 0.02076211\n",
      "Iteration 1875, loss = 0.02073618\n",
      "Iteration 1876, loss = 0.02071030\n",
      "Iteration 1877, loss = 0.02068445\n",
      "Iteration 1878, loss = 0.02065864\n",
      "Iteration 1879, loss = 0.02063287\n",
      "Iteration 1880, loss = 0.02060714\n",
      "Iteration 1881, loss = 0.02058145\n",
      "Iteration 1882, loss = 0.02055580\n",
      "Iteration 1883, loss = 0.02053018\n",
      "Iteration 1884, loss = 0.02050461\n",
      "Iteration 1885, loss = 0.02047908\n",
      "Iteration 1886, loss = 0.02045358\n",
      "Iteration 1887, loss = 0.02042812\n",
      "Iteration 1888, loss = 0.02040270\n",
      "Iteration 1889, loss = 0.02037732\n",
      "Iteration 1890, loss = 0.02035198\n",
      "Iteration 1891, loss = 0.02032668\n",
      "Iteration 1892, loss = 0.02030142\n",
      "Iteration 1893, loss = 0.02027619\n",
      "Iteration 1894, loss = 0.02025100\n",
      "Iteration 1895, loss = 0.02022585\n",
      "Iteration 1896, loss = 0.02020074\n",
      "Iteration 1897, loss = 0.02017567\n",
      "Iteration 1898, loss = 0.02015064\n",
      "Iteration 1899, loss = 0.02012564\n",
      "Iteration 1900, loss = 0.02010068\n",
      "Iteration 1901, loss = 0.02007576\n",
      "Iteration 1902, loss = 0.02005088\n",
      "Iteration 1903, loss = 0.02002603\n",
      "Iteration 1904, loss = 0.02000123\n",
      "Iteration 1905, loss = 0.01997646\n",
      "Iteration 1906, loss = 0.01995173\n",
      "Iteration 1907, loss = 0.01992703\n",
      "Iteration 1908, loss = 0.01990237\n",
      "Iteration 1909, loss = 0.01987775\n",
      "Iteration 1910, loss = 0.01985317\n",
      "Iteration 1911, loss = 0.01982863\n",
      "Iteration 1912, loss = 0.01980412\n",
      "Iteration 1913, loss = 0.01977965\n",
      "Iteration 1914, loss = 0.01975522\n",
      "Iteration 1915, loss = 0.01973082\n",
      "Iteration 1916, loss = 0.01970646\n",
      "Iteration 1917, loss = 0.01968214\n",
      "Iteration 1918, loss = 0.01965786\n",
      "Iteration 1919, loss = 0.01963361\n",
      "Iteration 1920, loss = 0.01960940\n",
      "Iteration 1921, loss = 0.01958522\n",
      "Iteration 1922, loss = 0.01956108\n",
      "Iteration 1923, loss = 0.01953698\n",
      "Iteration 1924, loss = 0.01951292\n",
      "Iteration 1925, loss = 0.01948889\n",
      "Iteration 1926, loss = 0.01946490\n",
      "Iteration 1927, loss = 0.01944094\n",
      "Iteration 1928, loss = 0.01941702\n",
      "Iteration 1929, loss = 0.01939314\n",
      "Iteration 1930, loss = 0.01936929\n",
      "Iteration 1931, loss = 0.01934548\n",
      "Iteration 1932, loss = 0.01932171\n",
      "Iteration 1933, loss = 0.01929797\n",
      "Iteration 1934, loss = 0.01927426\n",
      "Iteration 1935, loss = 0.01925060\n",
      "Iteration 1936, loss = 0.01922697\n",
      "Iteration 1937, loss = 0.01920337\n",
      "Iteration 1938, loss = 0.01917981\n",
      "Iteration 1939, loss = 0.01915629\n",
      "Iteration 1940, loss = 0.01913280\n",
      "Iteration 1941, loss = 0.01910935\n",
      "Iteration 1942, loss = 0.01908593\n",
      "Iteration 1943, loss = 0.01906255\n",
      "Iteration 1944, loss = 0.01903920\n",
      "Iteration 1945, loss = 0.01901589\n",
      "Iteration 1946, loss = 0.01899261\n",
      "Iteration 1947, loss = 0.01896937\n",
      "Iteration 1948, loss = 0.01894616\n",
      "Iteration 1949, loss = 0.01892299\n",
      "Iteration 1950, loss = 0.01889986\n",
      "Iteration 1951, loss = 0.01887676\n",
      "Iteration 1952, loss = 0.01885369\n",
      "Iteration 1953, loss = 0.01883066\n",
      "Iteration 1954, loss = 0.01880766\n",
      "Iteration 1955, loss = 0.01878470\n",
      "Iteration 1956, loss = 0.01876177\n",
      "Iteration 1957, loss = 0.01873888\n",
      "Iteration 1958, loss = 0.01871602\n",
      "Iteration 1959, loss = 0.01869320\n",
      "Iteration 1960, loss = 0.01867041\n",
      "Iteration 1961, loss = 0.01864766\n",
      "Iteration 1962, loss = 0.01862493\n",
      "Iteration 1963, loss = 0.01860225\n",
      "Iteration 1964, loss = 0.01857960\n",
      "Iteration 1965, loss = 0.01855698\n",
      "Iteration 1966, loss = 0.01853439\n",
      "Iteration 1967, loss = 0.01851185\n",
      "Iteration 1968, loss = 0.01848933\n",
      "Iteration 1969, loss = 0.01846685\n",
      "Iteration 1970, loss = 0.01844440\n",
      "Iteration 1971, loss = 0.01842199\n",
      "Iteration 1972, loss = 0.01839960\n",
      "Iteration 1973, loss = 0.01837726\n",
      "Iteration 1974, loss = 0.01835494\n",
      "Iteration 1975, loss = 0.01833267\n",
      "Iteration 1976, loss = 0.01831042\n",
      "Iteration 1977, loss = 0.01828821\n",
      "Iteration 1978, loss = 0.01826603\n",
      "Iteration 1979, loss = 0.01824388\n",
      "Iteration 1980, loss = 0.01822177\n",
      "Iteration 1981, loss = 0.01819969\n",
      "Iteration 1982, loss = 0.01817764\n",
      "Iteration 1983, loss = 0.01815563\n",
      "Iteration 1984, loss = 0.01813365\n",
      "Iteration 1985, loss = 0.01811170\n",
      "Iteration 1986, loss = 0.01808979\n",
      "Iteration 1987, loss = 0.01806791\n",
      "Iteration 1988, loss = 0.01804606\n",
      "Iteration 1989, loss = 0.01802424\n",
      "Iteration 1990, loss = 0.01800246\n",
      "Iteration 1991, loss = 0.01798071\n",
      "Iteration 1992, loss = 0.01795899\n",
      "Iteration 1993, loss = 0.01793731\n",
      "Iteration 1994, loss = 0.01791566\n",
      "Iteration 1995, loss = 0.01789404\n",
      "Iteration 1996, loss = 0.01787245\n",
      "Iteration 1997, loss = 0.01785089\n",
      "Iteration 1998, loss = 0.01782937\n",
      "Iteration 1999, loss = 0.01780788\n",
      "Iteration 2000, loss = 0.01778642\n",
      "Iteration 2001, loss = 0.01776500\n",
      "Iteration 2002, loss = 0.01774360\n",
      "Iteration 2003, loss = 0.01772224\n",
      "Iteration 2004, loss = 0.01770091\n",
      "Iteration 2005, loss = 0.01767961\n",
      "Iteration 2006, loss = 0.01765835\n",
      "Iteration 2007, loss = 0.01763711\n",
      "Iteration 2008, loss = 0.01761591\n",
      "Iteration 2009, loss = 0.01759474\n",
      "Iteration 2010, loss = 0.01757360\n",
      "Iteration 2011, loss = 0.01755249\n",
      "Iteration 2012, loss = 0.01753141\n",
      "Iteration 2013, loss = 0.01751037\n",
      "Iteration 2014, loss = 0.01748936\n",
      "Iteration 2015, loss = 0.01746837\n",
      "Iteration 2016, loss = 0.01744742\n",
      "Iteration 2017, loss = 0.01742650\n",
      "Iteration 2018, loss = 0.01740562\n",
      "Iteration 2019, loss = 0.01738476\n",
      "Iteration 2020, loss = 0.01736393\n",
      "Iteration 2021, loss = 0.01734314\n",
      "Iteration 2022, loss = 0.01732238\n",
      "Iteration 2023, loss = 0.01730164\n",
      "Iteration 2024, loss = 0.01728094\n",
      "Iteration 2025, loss = 0.01726027\n",
      "Iteration 2026, loss = 0.01723963\n",
      "Iteration 2027, loss = 0.01721902\n",
      "Iteration 2028, loss = 0.01719844\n",
      "Iteration 2029, loss = 0.01717790\n",
      "Iteration 2030, loss = 0.01715738\n",
      "Iteration 2031, loss = 0.01713689\n",
      "Iteration 2032, loss = 0.01711644\n",
      "Iteration 2033, loss = 0.01709601\n",
      "Iteration 2034, loss = 0.01707561\n",
      "Iteration 2035, loss = 0.01705525\n",
      "Iteration 2036, loss = 0.01703492\n",
      "Iteration 2037, loss = 0.01701461\n",
      "Iteration 2038, loss = 0.01699434\n",
      "Iteration 2039, loss = 0.01697409\n",
      "Iteration 2040, loss = 0.01695388\n",
      "Iteration 2041, loss = 0.01693370\n",
      "Iteration 2042, loss = 0.01691354\n",
      "Iteration 2043, loss = 0.01689342\n",
      "Iteration 2044, loss = 0.01687332\n",
      "Iteration 2045, loss = 0.01685326\n",
      "Iteration 2046, loss = 0.01683322\n",
      "Iteration 2047, loss = 0.01681322\n",
      "Iteration 2048, loss = 0.01679325\n",
      "Iteration 2049, loss = 0.01677330\n",
      "Iteration 2050, loss = 0.01675338\n",
      "Iteration 2051, loss = 0.01673350\n",
      "Iteration 2052, loss = 0.01671364\n",
      "Iteration 2053, loss = 0.01669381\n",
      "Iteration 2054, loss = 0.01667402\n",
      "Iteration 2055, loss = 0.01665425\n",
      "Iteration 2056, loss = 0.01663451\n",
      "Iteration 2057, loss = 0.01661480\n",
      "Iteration 2058, loss = 0.01659512\n",
      "Iteration 2059, loss = 0.01657547\n",
      "Iteration 2060, loss = 0.01655584\n",
      "Iteration 2061, loss = 0.01653625\n",
      "Iteration 2062, loss = 0.01651669\n",
      "Iteration 2063, loss = 0.01649715\n",
      "Iteration 2064, loss = 0.01647765\n",
      "Iteration 2065, loss = 0.01645817\n",
      "Iteration 2066, loss = 0.01643872\n",
      "Iteration 2067, loss = 0.01641930\n",
      "Iteration 2068, loss = 0.01639991\n",
      "Iteration 2069, loss = 0.01638054\n",
      "Iteration 2070, loss = 0.01636121\n",
      "Iteration 2071, loss = 0.01634190\n",
      "Iteration 2072, loss = 0.01632263\n",
      "Iteration 2073, loss = 0.01630338\n",
      "Iteration 2074, loss = 0.01628416\n",
      "Iteration 2075, loss = 0.01626497\n",
      "Iteration 2076, loss = 0.01624580\n",
      "Iteration 2077, loss = 0.01622667\n",
      "Iteration 2078, loss = 0.01620756\n",
      "Iteration 2079, loss = 0.01618848\n",
      "Iteration 2080, loss = 0.01616943\n",
      "Iteration 2081, loss = 0.01615041\n",
      "Iteration 2082, loss = 0.01613142\n",
      "Iteration 2083, loss = 0.01611245\n",
      "Iteration 2084, loss = 0.01609351\n",
      "Iteration 2085, loss = 0.01607460\n",
      "Iteration 2086, loss = 0.01605572\n",
      "Iteration 2087, loss = 0.01603686\n",
      "Iteration 2088, loss = 0.01601804\n",
      "Iteration 2089, loss = 0.01599924\n",
      "Iteration 2090, loss = 0.01598047\n",
      "Iteration 2091, loss = 0.01596172\n",
      "Iteration 2092, loss = 0.01594300\n",
      "Iteration 2093, loss = 0.01592432\n",
      "Iteration 2094, loss = 0.01590565\n",
      "Iteration 2095, loss = 0.01588702\n",
      "Iteration 2096, loss = 0.01586841\n",
      "Iteration 2097, loss = 0.01584983\n",
      "Iteration 2098, loss = 0.01583128\n",
      "Iteration 2099, loss = 0.01581276\n",
      "Iteration 2100, loss = 0.01579426\n",
      "Iteration 2101, loss = 0.01577579\n",
      "Iteration 2102, loss = 0.01575735\n",
      "Iteration 2103, loss = 0.01573893\n",
      "Iteration 2104, loss = 0.01572054\n",
      "Iteration 2105, loss = 0.01570218\n",
      "Iteration 2106, loss = 0.01568384\n",
      "Iteration 2107, loss = 0.01566553\n",
      "Iteration 2108, loss = 0.01564725\n",
      "Iteration 2109, loss = 0.01562900\n",
      "Iteration 2110, loss = 0.01561077\n",
      "Iteration 2111, loss = 0.01559257\n",
      "Iteration 2112, loss = 0.01557439\n",
      "Iteration 2113, loss = 0.01555625\n",
      "Iteration 2114, loss = 0.01553812\n",
      "Iteration 2115, loss = 0.01552003\n",
      "Iteration 2116, loss = 0.01550196\n",
      "Iteration 2117, loss = 0.01548392\n",
      "Iteration 2118, loss = 0.01546590\n",
      "Iteration 2119, loss = 0.01544791\n",
      "Iteration 2120, loss = 0.01542995\n",
      "Iteration 2121, loss = 0.01541201\n",
      "Iteration 2122, loss = 0.01539410\n",
      "Iteration 2123, loss = 0.01537622\n",
      "Iteration 2124, loss = 0.01535836\n",
      "Iteration 2125, loss = 0.01534053\n",
      "Iteration 2126, loss = 0.01532272\n",
      "Iteration 2127, loss = 0.01530494\n",
      "Iteration 2128, loss = 0.01528719\n",
      "Iteration 2129, loss = 0.01526946\n",
      "Iteration 2130, loss = 0.01525176\n",
      "Iteration 2131, loss = 0.01523408\n",
      "Iteration 2132, loss = 0.01521643\n",
      "Iteration 2133, loss = 0.01519880\n",
      "Iteration 2134, loss = 0.01518121\n",
      "Iteration 2135, loss = 0.01516363\n",
      "Iteration 2136, loss = 0.01514608\n",
      "Iteration 2137, loss = 0.01512856\n",
      "Iteration 2138, loss = 0.01511106\n",
      "Iteration 2139, loss = 0.01509359\n",
      "Iteration 2140, loss = 0.01507615\n",
      "Iteration 2141, loss = 0.01505873\n",
      "Iteration 2142, loss = 0.01504133\n",
      "Iteration 2143, loss = 0.01502396\n",
      "Iteration 2144, loss = 0.01500662\n",
      "Iteration 2145, loss = 0.01498930\n",
      "Iteration 2146, loss = 0.01497200\n",
      "Iteration 2147, loss = 0.01495473\n",
      "Iteration 2148, loss = 0.01493749\n",
      "Iteration 2149, loss = 0.01492027\n",
      "Iteration 2150, loss = 0.01490307\n",
      "Iteration 2151, loss = 0.01488591\n",
      "Iteration 2152, loss = 0.01486876\n",
      "Iteration 2153, loss = 0.01485164\n",
      "Iteration 2154, loss = 0.01483455\n",
      "Iteration 2155, loss = 0.01481748\n",
      "Iteration 2156, loss = 0.01480043\n",
      "Iteration 2157, loss = 0.01478341\n",
      "Iteration 2158, loss = 0.01476642\n",
      "Iteration 2159, loss = 0.01474945\n",
      "Iteration 2160, loss = 0.01473250\n",
      "Iteration 2161, loss = 0.01471558\n",
      "Iteration 2162, loss = 0.01469868\n",
      "Iteration 2163, loss = 0.01468181\n",
      "Iteration 2164, loss = 0.01466496\n",
      "Iteration 2165, loss = 0.01464814\n",
      "Iteration 2166, loss = 0.01463134\n",
      "Iteration 2167, loss = 0.01461456\n",
      "Iteration 2168, loss = 0.01459781\n",
      "Iteration 2169, loss = 0.01458108\n",
      "Iteration 2170, loss = 0.01456438\n",
      "Iteration 2171, loss = 0.01454770\n",
      "Iteration 2172, loss = 0.01453105\n",
      "Iteration 2173, loss = 0.01451442\n",
      "Iteration 2174, loss = 0.01449781\n",
      "Iteration 2175, loss = 0.01448123\n",
      "Iteration 2176, loss = 0.01446467\n",
      "Iteration 2177, loss = 0.01444814\n",
      "Iteration 2178, loss = 0.01443163\n",
      "Iteration 2179, loss = 0.01441514\n",
      "Iteration 2180, loss = 0.01439868\n",
      "Iteration 2181, loss = 0.01438224\n",
      "Iteration 2182, loss = 0.01436582\n",
      "Iteration 2183, loss = 0.01434943\n",
      "Iteration 2184, loss = 0.01433306\n",
      "Iteration 2185, loss = 0.01431672\n",
      "Iteration 2186, loss = 0.01430040\n",
      "Iteration 2187, loss = 0.01428410\n",
      "Iteration 2188, loss = 0.01426783\n",
      "Iteration 2189, loss = 0.01425157\n",
      "Iteration 2190, loss = 0.01423535\n",
      "Iteration 2191, loss = 0.01421914\n",
      "Iteration 2192, loss = 0.01420296\n",
      "Iteration 2193, loss = 0.01418681\n",
      "Iteration 2194, loss = 0.01417067\n",
      "Iteration 2195, loss = 0.01415456\n",
      "Iteration 2196, loss = 0.01413847\n",
      "Iteration 2197, loss = 0.01412241\n",
      "Iteration 2198, loss = 0.01410637\n",
      "Iteration 2199, loss = 0.01409035\n",
      "Iteration 2200, loss = 0.01407435\n",
      "Iteration 2201, loss = 0.01405838\n",
      "Iteration 2202, loss = 0.01404243\n",
      "Iteration 2203, loss = 0.01402650\n",
      "Iteration 2204, loss = 0.01401060\n",
      "Iteration 2205, loss = 0.01399472\n",
      "Iteration 2206, loss = 0.01397886\n",
      "Iteration 2207, loss = 0.01396303\n",
      "Iteration 2208, loss = 0.01394721\n",
      "Iteration 2209, loss = 0.01393142\n",
      "Iteration 2210, loss = 0.01391565\n",
      "Iteration 2211, loss = 0.01389991\n",
      "Iteration 2212, loss = 0.01388419\n",
      "Iteration 2213, loss = 0.01386849\n",
      "Iteration 2214, loss = 0.01385281\n",
      "Iteration 2215, loss = 0.01383715\n",
      "Iteration 2216, loss = 0.01382152\n",
      "Iteration 2217, loss = 0.01380591\n",
      "Iteration 2218, loss = 0.01379032\n",
      "Iteration 2219, loss = 0.01377476\n",
      "Iteration 2220, loss = 0.01375921\n",
      "Iteration 2221, loss = 0.01374369\n",
      "Iteration 2222, loss = 0.01372819\n",
      "Iteration 2223, loss = 0.01371272\n",
      "Iteration 2224, loss = 0.01369726\n",
      "Iteration 2225, loss = 0.01368183\n",
      "Iteration 2226, loss = 0.01366642\n",
      "Iteration 2227, loss = 0.01365103\n",
      "Iteration 2228, loss = 0.01363566\n",
      "Iteration 2229, loss = 0.01362032\n",
      "Iteration 2230, loss = 0.01360500\n",
      "Iteration 2231, loss = 0.01358970\n",
      "Iteration 2232, loss = 0.01357442\n",
      "Iteration 2233, loss = 0.01355916\n",
      "Iteration 2234, loss = 0.01354392\n",
      "Iteration 2235, loss = 0.01352871\n",
      "Iteration 2236, loss = 0.01351352\n",
      "Iteration 2237, loss = 0.01349835\n",
      "Iteration 2238, loss = 0.01348320\n",
      "Iteration 2239, loss = 0.01346807\n",
      "Iteration 2240, loss = 0.01345296\n",
      "Iteration 2241, loss = 0.01343788\n",
      "Iteration 2242, loss = 0.01342281\n",
      "Iteration 2243, loss = 0.01340777\n",
      "Iteration 2244, loss = 0.01339275\n",
      "Iteration 2245, loss = 0.01337775\n",
      "Iteration 2246, loss = 0.01336278\n",
      "Iteration 2247, loss = 0.01334782\n",
      "Iteration 2248, loss = 0.01333288\n",
      "Iteration 2249, loss = 0.01331797\n",
      "Iteration 2250, loss = 0.01330308\n",
      "Iteration 2251, loss = 0.01328820\n",
      "Iteration 2252, loss = 0.01327335\n",
      "Iteration 2253, loss = 0.01325852\n",
      "Iteration 2254, loss = 0.01324372\n",
      "Iteration 2255, loss = 0.01322893\n",
      "Iteration 2256, loss = 0.01321416\n",
      "Iteration 2257, loss = 0.01319942\n",
      "Iteration 2258, loss = 0.01318469\n",
      "Iteration 2259, loss = 0.01316999\n",
      "Iteration 2260, loss = 0.01315530\n",
      "Iteration 2261, loss = 0.01314064\n",
      "Iteration 2262, loss = 0.01312600\n",
      "Iteration 2263, loss = 0.01311138\n",
      "Iteration 2264, loss = 0.01309678\n",
      "Iteration 2265, loss = 0.01308220\n",
      "Iteration 2266, loss = 0.01306764\n",
      "Iteration 2267, loss = 0.01305310\n",
      "Iteration 2268, loss = 0.01303858\n",
      "Iteration 2269, loss = 0.01302409\n",
      "Iteration 2270, loss = 0.01300961\n",
      "Iteration 2271, loss = 0.01299515\n",
      "Iteration 2272, loss = 0.01298072\n",
      "Iteration 2273, loss = 0.01296630\n",
      "Iteration 2274, loss = 0.01295191\n",
      "Iteration 2275, loss = 0.01293753\n",
      "Iteration 2276, loss = 0.01292318\n",
      "Iteration 2277, loss = 0.01290884\n",
      "Iteration 2278, loss = 0.01289453\n",
      "Iteration 2279, loss = 0.01288023\n",
      "Iteration 2280, loss = 0.01286596\n",
      "Iteration 2281, loss = 0.01285171\n",
      "Iteration 2282, loss = 0.01283747\n",
      "Iteration 2283, loss = 0.01282326\n",
      "Iteration 2284, loss = 0.01280907\n",
      "Iteration 2285, loss = 0.01279489\n",
      "Iteration 2286, loss = 0.01278074\n",
      "Iteration 2287, loss = 0.01276660\n",
      "Iteration 2288, loss = 0.01275249\n",
      "Iteration 2289, loss = 0.01273840\n",
      "Iteration 2290, loss = 0.01272432\n",
      "Iteration 2291, loss = 0.01271027\n",
      "Iteration 2292, loss = 0.01269623\n",
      "Iteration 2293, loss = 0.01268222\n",
      "Iteration 2294, loss = 0.01266822\n",
      "Iteration 2295, loss = 0.01265425\n",
      "Iteration 2296, loss = 0.01264029\n",
      "Iteration 2297, loss = 0.01262635\n",
      "Iteration 2298, loss = 0.01261244\n",
      "Iteration 2299, loss = 0.01259854\n",
      "Iteration 2300, loss = 0.01258466\n",
      "Iteration 2301, loss = 0.01257080\n",
      "Iteration 2302, loss = 0.01255696\n",
      "Iteration 2303, loss = 0.01254314\n",
      "Iteration 2304, loss = 0.01252934\n",
      "Iteration 2305, loss = 0.01251556\n",
      "Iteration 2306, loss = 0.01250180\n",
      "Iteration 2307, loss = 0.01248806\n",
      "Iteration 2308, loss = 0.01247433\n",
      "Iteration 2309, loss = 0.01246063\n",
      "Iteration 2310, loss = 0.01244694\n",
      "Iteration 2311, loss = 0.01243328\n",
      "Iteration 2312, loss = 0.01241963\n",
      "Iteration 2313, loss = 0.01240600\n",
      "Iteration 2314, loss = 0.01239240\n",
      "Iteration 2315, loss = 0.01237881\n",
      "Iteration 2316, loss = 0.01236524\n",
      "Iteration 2317, loss = 0.01235168\n",
      "Iteration 2318, loss = 0.01233815\n",
      "Iteration 2319, loss = 0.01232464\n",
      "Iteration 2320, loss = 0.01231114\n",
      "Iteration 2321, loss = 0.01229767\n",
      "Iteration 2322, loss = 0.01228421\n",
      "Iteration 2323, loss = 0.01227077\n",
      "Iteration 2324, loss = 0.01225735\n",
      "Iteration 2325, loss = 0.01224395\n",
      "Iteration 2326, loss = 0.01223057\n",
      "Iteration 2327, loss = 0.01221720\n",
      "Iteration 2328, loss = 0.01220386\n",
      "Iteration 2329, loss = 0.01219053\n",
      "Iteration 2330, loss = 0.01217722\n",
      "Iteration 2331, loss = 0.01216393\n",
      "Iteration 2332, loss = 0.01215066\n",
      "Iteration 2333, loss = 0.01213741\n",
      "Iteration 2334, loss = 0.01212418\n",
      "Iteration 2335, loss = 0.01211096\n",
      "Iteration 2336, loss = 0.01209776\n",
      "Iteration 2337, loss = 0.01208458\n",
      "Iteration 2338, loss = 0.01207142\n",
      "Iteration 2339, loss = 0.01205828\n",
      "Iteration 2340, loss = 0.01204516\n",
      "Iteration 2341, loss = 0.01203205\n",
      "Iteration 2342, loss = 0.01201896\n",
      "Iteration 2343, loss = 0.01200589\n",
      "Iteration 2344, loss = 0.01199284\n",
      "Iteration 2345, loss = 0.01197981\n",
      "Iteration 2346, loss = 0.01196679\n",
      "Iteration 2347, loss = 0.01195380\n",
      "Iteration 2348, loss = 0.01194082\n",
      "Iteration 2349, loss = 0.01192786\n",
      "Iteration 2350, loss = 0.01191491\n",
      "Iteration 2351, loss = 0.01190199\n",
      "Iteration 2352, loss = 0.01188908\n",
      "Iteration 2353, loss = 0.01187619\n",
      "Iteration 2354, loss = 0.01186332\n",
      "Iteration 2355, loss = 0.01185046\n",
      "Iteration 2356, loss = 0.01183763\n",
      "Iteration 2357, loss = 0.01182481\n",
      "Iteration 2358, loss = 0.01181201\n",
      "Iteration 2359, loss = 0.01179922\n",
      "Iteration 2360, loss = 0.01178646\n",
      "Iteration 2361, loss = 0.01177371\n",
      "Iteration 2362, loss = 0.01176098\n",
      "Iteration 2363, loss = 0.01174827\n",
      "Iteration 2364, loss = 0.01173557\n",
      "Iteration 2365, loss = 0.01172290\n",
      "Iteration 2366, loss = 0.01171024\n",
      "Iteration 2367, loss = 0.01169759\n",
      "Iteration 2368, loss = 0.01168497\n",
      "Iteration 2369, loss = 0.01167236\n",
      "Iteration 2370, loss = 0.01165977\n",
      "Iteration 2371, loss = 0.01164720\n",
      "Iteration 2372, loss = 0.01163464\n",
      "Iteration 2373, loss = 0.01162210\n",
      "Iteration 2374, loss = 0.01160958\n",
      "Iteration 2375, loss = 0.01159708\n",
      "Iteration 2376, loss = 0.01158459\n",
      "Iteration 2377, loss = 0.01157212\n",
      "Iteration 2378, loss = 0.01155967\n",
      "Iteration 2379, loss = 0.01154723\n",
      "Iteration 2380, loss = 0.01153481\n",
      "Iteration 2381, loss = 0.01152241\n",
      "Iteration 2382, loss = 0.01151003\n",
      "Iteration 2383, loss = 0.01149766\n",
      "Iteration 2384, loss = 0.01148531\n",
      "Iteration 2385, loss = 0.01147298\n",
      "Iteration 2386, loss = 0.01146066\n",
      "Iteration 2387, loss = 0.01144836\n",
      "Iteration 2388, loss = 0.01143608\n",
      "Iteration 2389, loss = 0.01142382\n",
      "Iteration 2390, loss = 0.01141157\n",
      "Iteration 2391, loss = 0.01139934\n",
      "Iteration 2392, loss = 0.01138712\n",
      "Iteration 2393, loss = 0.01137492\n",
      "Iteration 2394, loss = 0.01136274\n",
      "Iteration 2395, loss = 0.01135057\n",
      "Iteration 2396, loss = 0.01133843\n",
      "Iteration 2397, loss = 0.01132629\n",
      "Iteration 2398, loss = 0.01131418\n",
      "Iteration 2399, loss = 0.01130208\n",
      "Iteration 2400, loss = 0.01129000\n",
      "Iteration 2401, loss = 0.01127793\n",
      "Iteration 2402, loss = 0.01126588\n",
      "Iteration 2403, loss = 0.01125385\n",
      "Iteration 2404, loss = 0.01124184\n",
      "Iteration 2405, loss = 0.01122984\n",
      "Iteration 2406, loss = 0.01121785\n",
      "Iteration 2407, loss = 0.01120589\n",
      "Iteration 2408, loss = 0.01119393\n",
      "Iteration 2409, loss = 0.01118200\n",
      "Iteration 2410, loss = 0.01117008\n",
      "Iteration 2411, loss = 0.01115818\n",
      "Iteration 2412, loss = 0.01114630\n",
      "Iteration 2413, loss = 0.01113443\n",
      "Iteration 2414, loss = 0.01112257\n",
      "Iteration 2415, loss = 0.01111074\n",
      "Iteration 2416, loss = 0.01109891\n",
      "Iteration 2417, loss = 0.01108711\n",
      "Iteration 2418, loss = 0.01107532\n",
      "Iteration 2419, loss = 0.01106355\n",
      "Iteration 2420, loss = 0.01105179\n",
      "Iteration 2421, loss = 0.01104005\n",
      "Iteration 2422, loss = 0.01102833\n",
      "Iteration 2423, loss = 0.01101662\n",
      "Iteration 2424, loss = 0.01100493\n",
      "Iteration 2425, loss = 0.01099325\n",
      "Iteration 2426, loss = 0.01098159\n",
      "Iteration 2427, loss = 0.01096994\n",
      "Iteration 2428, loss = 0.01095831\n",
      "Iteration 2429, loss = 0.01094670\n",
      "Iteration 2430, loss = 0.01093510\n",
      "Iteration 2431, loss = 0.01092352\n",
      "Iteration 2432, loss = 0.01091195\n",
      "Iteration 2433, loss = 0.01090040\n",
      "Iteration 2434, loss = 0.01088887\n",
      "Iteration 2435, loss = 0.01087735\n",
      "Iteration 2436, loss = 0.01086584\n",
      "Iteration 2437, loss = 0.01085436\n",
      "Iteration 2438, loss = 0.01084288\n",
      "Iteration 2439, loss = 0.01083143\n",
      "Iteration 2440, loss = 0.01081999\n",
      "Iteration 2441, loss = 0.01080856\n",
      "Iteration 2442, loss = 0.01079715\n",
      "Iteration 2443, loss = 0.01078575\n",
      "Iteration 2444, loss = 0.01077437\n",
      "Iteration 2445, loss = 0.01076301\n",
      "Iteration 2446, loss = 0.01075166\n",
      "Iteration 2447, loss = 0.01074033\n",
      "Iteration 2448, loss = 0.01072901\n",
      "Iteration 2449, loss = 0.01071771\n",
      "Iteration 2450, loss = 0.01070642\n",
      "Iteration 2451, loss = 0.01069514\n",
      "Iteration 2452, loss = 0.01068389\n",
      "Iteration 2453, loss = 0.01067265\n",
      "Iteration 2454, loss = 0.01066142\n",
      "Iteration 2455, loss = 0.01065021\n",
      "Iteration 2456, loss = 0.01063901\n",
      "Iteration 2457, loss = 0.01062783\n",
      "Iteration 2458, loss = 0.01061666\n",
      "Iteration 2459, loss = 0.01060551\n",
      "Iteration 2460, loss = 0.01059437\n",
      "Iteration 2461, loss = 0.01058325\n",
      "Iteration 2462, loss = 0.01057215\n",
      "Iteration 2463, loss = 0.01056105\n",
      "Iteration 2464, loss = 0.01054998\n",
      "Iteration 2465, loss = 0.01053892\n",
      "Iteration 2466, loss = 0.01052787\n",
      "Iteration 2467, loss = 0.01051684\n",
      "Iteration 2468, loss = 0.01050582\n",
      "Iteration 2469, loss = 0.01049482\n",
      "Iteration 2470, loss = 0.01048383\n",
      "Iteration 2471, loss = 0.01047286\n",
      "Iteration 2472, loss = 0.01046190\n",
      "Iteration 2473, loss = 0.01045096\n",
      "Iteration 2474, loss = 0.01044003\n",
      "Iteration 2475, loss = 0.01042911\n",
      "Iteration 2476, loss = 0.01041821\n",
      "Iteration 2477, loss = 0.01040733\n",
      "Iteration 2478, loss = 0.01039646\n",
      "Iteration 2479, loss = 0.01038560\n",
      "Iteration 2480, loss = 0.01037476\n",
      "Iteration 2481, loss = 0.01036394\n",
      "Iteration 2482, loss = 0.01035313\n",
      "Iteration 2483, loss = 0.01034233\n",
      "Iteration 2484, loss = 0.01033154\n",
      "Iteration 2485, loss = 0.01032078\n",
      "Iteration 2486, loss = 0.01031002\n",
      "Iteration 2487, loss = 0.01029928\n",
      "Iteration 2488, loss = 0.01028856\n",
      "Iteration 2489, loss = 0.01027785\n",
      "Iteration 2490, loss = 0.01026715\n",
      "Iteration 2491, loss = 0.01025647\n",
      "Iteration 2492, loss = 0.01024580\n",
      "Iteration 2493, loss = 0.01023514\n",
      "Iteration 2494, loss = 0.01022450\n",
      "Iteration 2495, loss = 0.01021388\n",
      "Iteration 2496, loss = 0.01020327\n",
      "Iteration 2497, loss = 0.01019267\n",
      "Iteration 2498, loss = 0.01018209\n",
      "Iteration 2499, loss = 0.01017152\n",
      "Iteration 2500, loss = 0.01016096\n",
      "Iteration 2501, loss = 0.01015042\n",
      "Iteration 2502, loss = 0.01013990\n",
      "Iteration 2503, loss = 0.01012938\n",
      "Iteration 2504, loss = 0.01011889\n",
      "Iteration 2505, loss = 0.01010840\n",
      "Iteration 2506, loss = 0.01009793\n",
      "Iteration 2507, loss = 0.01008747\n",
      "Iteration 2508, loss = 0.01007703\n",
      "Iteration 2509, loss = 0.01006660\n",
      "Iteration 2510, loss = 0.01005619\n",
      "Iteration 2511, loss = 0.01004578\n",
      "Iteration 2512, loss = 0.01003540\n",
      "Iteration 2513, loss = 0.01002502\n",
      "Iteration 2514, loss = 0.01001466\n",
      "Iteration 2515, loss = 0.01000432\n",
      "Iteration 2516, loss = 0.00999399\n",
      "Iteration 2517, loss = 0.00998367\n",
      "Iteration 2518, loss = 0.00997336\n",
      "Iteration 2519, loss = 0.00996307\n",
      "Iteration 2520, loss = 0.00995279\n",
      "Iteration 2521, loss = 0.00994253\n",
      "Iteration 2522, loss = 0.00993228\n",
      "Iteration 2523, loss = 0.00992204\n",
      "Iteration 2524, loss = 0.00991182\n",
      "Iteration 2525, loss = 0.00990161\n",
      "Iteration 2526, loss = 0.00989141\n",
      "Iteration 2527, loss = 0.00988123\n",
      "Iteration 2528, loss = 0.00987106\n",
      "Iteration 2529, loss = 0.00986091\n",
      "Iteration 2530, loss = 0.00985076\n",
      "Iteration 2531, loss = 0.00984063\n",
      "Iteration 2532, loss = 0.00983052\n",
      "Iteration 2533, loss = 0.00982042\n",
      "Iteration 2534, loss = 0.00981033\n",
      "Iteration 2535, loss = 0.00980025\n",
      "Iteration 2536, loss = 0.00979019\n",
      "Iteration 2537, loss = 0.00978014\n",
      "Iteration 2538, loss = 0.00977011\n",
      "Iteration 2539, loss = 0.00976008\n",
      "Iteration 2540, loss = 0.00975007\n",
      "Iteration 2541, loss = 0.00974008\n",
      "Iteration 2542, loss = 0.00973010\n",
      "Iteration 2543, loss = 0.00972013\n",
      "Iteration 2544, loss = 0.00971017\n",
      "Iteration 2545, loss = 0.00970023\n",
      "Iteration 2546, loss = 0.00969030\n",
      "Iteration 2547, loss = 0.00968038\n",
      "Iteration 2548, loss = 0.00967047\n",
      "Iteration 2549, loss = 0.00966058\n",
      "Iteration 2550, loss = 0.00965071\n",
      "Iteration 2551, loss = 0.00964084\n",
      "Iteration 2552, loss = 0.00963099\n",
      "Iteration 2553, loss = 0.00962115\n",
      "Iteration 2554, loss = 0.00961132\n",
      "Iteration 2555, loss = 0.00960151\n",
      "Iteration 2556, loss = 0.00959171\n",
      "Iteration 2557, loss = 0.00958192\n",
      "Iteration 2558, loss = 0.00957215\n",
      "Iteration 2559, loss = 0.00956239\n",
      "Iteration 2560, loss = 0.00955264\n",
      "Iteration 2561, loss = 0.00954290\n",
      "Iteration 2562, loss = 0.00953318\n",
      "Iteration 2563, loss = 0.00952347\n",
      "Iteration 2564, loss = 0.00951377\n",
      "Iteration 2565, loss = 0.00950408\n",
      "Iteration 2566, loss = 0.00949441\n",
      "Iteration 2567, loss = 0.00948475\n",
      "Iteration 2568, loss = 0.00947510\n",
      "Iteration 2569, loss = 0.00946547\n",
      "Iteration 2570, loss = 0.00945585\n",
      "Iteration 2571, loss = 0.00944624\n",
      "Iteration 2572, loss = 0.00943664\n",
      "Iteration 2573, loss = 0.00942706\n",
      "Iteration 2574, loss = 0.00941749\n",
      "Iteration 2575, loss = 0.00940793\n",
      "Iteration 2576, loss = 0.00939838\n",
      "Iteration 2577, loss = 0.00938885\n",
      "Iteration 2578, loss = 0.00937932\n",
      "Iteration 2579, loss = 0.00936982\n",
      "Iteration 2580, loss = 0.00936032\n",
      "Iteration 2581, loss = 0.00935083\n",
      "Iteration 2582, loss = 0.00934136\n",
      "Iteration 2583, loss = 0.00933190\n",
      "Iteration 2584, loss = 0.00932245\n",
      "Iteration 2585, loss = 0.00931302\n",
      "Iteration 2586, loss = 0.00930360\n",
      "Iteration 2587, loss = 0.00929419\n",
      "Iteration 2588, loss = 0.00928479\n",
      "Iteration 2589, loss = 0.00927540\n",
      "Iteration 2590, loss = 0.00926603\n",
      "Iteration 2591, loss = 0.00925667\n",
      "Iteration 2592, loss = 0.00924732\n",
      "Iteration 2593, loss = 0.00923798\n",
      "Iteration 2594, loss = 0.00922865\n",
      "Iteration 2595, loss = 0.00921934\n",
      "Iteration 2596, loss = 0.00921004\n",
      "Iteration 2597, loss = 0.00920075\n",
      "Iteration 2598, loss = 0.00919148\n",
      "Iteration 2599, loss = 0.00918221\n",
      "Iteration 2600, loss = 0.00917296\n",
      "Iteration 2601, loss = 0.00916372\n",
      "Iteration 2602, loss = 0.00915449\n",
      "Iteration 2603, loss = 0.00914527\n",
      "Iteration 2604, loss = 0.00913607\n",
      "Iteration 2605, loss = 0.00912688\n",
      "Iteration 2606, loss = 0.00911769\n",
      "Iteration 2607, loss = 0.00910853\n",
      "Iteration 2608, loss = 0.00909937\n",
      "Iteration 2609, loss = 0.00909022\n",
      "Iteration 2610, loss = 0.00908109\n",
      "Iteration 2611, loss = 0.00907197\n",
      "Iteration 2612, loss = 0.00906286\n",
      "Iteration 2613, loss = 0.00905376\n",
      "Iteration 2614, loss = 0.00904467\n",
      "Iteration 2615, loss = 0.00903560\n",
      "Iteration 2616, loss = 0.00902654\n",
      "Iteration 2617, loss = 0.00901749\n",
      "Iteration 2618, loss = 0.00900845\n",
      "Iteration 2619, loss = 0.00899942\n",
      "Iteration 2620, loss = 0.00899040\n",
      "Iteration 2621, loss = 0.00898140\n",
      "Iteration 2622, loss = 0.00897241\n",
      "Iteration 2623, loss = 0.00896343\n",
      "Iteration 2624, loss = 0.00895446\n",
      "Iteration 2625, loss = 0.00894550\n",
      "Iteration 2626, loss = 0.00893655\n",
      "Iteration 2627, loss = 0.00892762\n",
      "Iteration 2628, loss = 0.00891870\n",
      "Iteration 2629, loss = 0.00890978\n",
      "Iteration 2630, loss = 0.00890088\n",
      "Iteration 2631, loss = 0.00889199\n",
      "Iteration 2632, loss = 0.00888312\n",
      "Iteration 2633, loss = 0.00887425\n",
      "Iteration 2634, loss = 0.00886540\n",
      "Iteration 2635, loss = 0.00885655\n",
      "Iteration 2636, loss = 0.00884772\n",
      "Iteration 2637, loss = 0.00883890\n",
      "Iteration 2638, loss = 0.00883009\n",
      "Iteration 2639, loss = 0.00882129\n",
      "Iteration 2640, loss = 0.00881251\n",
      "Iteration 2641, loss = 0.00880373\n",
      "Iteration 2642, loss = 0.00879497\n",
      "Iteration 2643, loss = 0.00878622\n",
      "Iteration 2644, loss = 0.00877748\n",
      "Iteration 2645, loss = 0.00876875\n",
      "Iteration 2646, loss = 0.00876003\n",
      "Iteration 2647, loss = 0.00875132\n",
      "Iteration 2648, loss = 0.00874262\n",
      "Iteration 2649, loss = 0.00873394\n",
      "Iteration 2650, loss = 0.00872526\n",
      "Iteration 2651, loss = 0.00871660\n",
      "Iteration 2652, loss = 0.00870795\n",
      "Iteration 2653, loss = 0.00869931\n",
      "Iteration 2654, loss = 0.00869068\n",
      "Iteration 2655, loss = 0.00868206\n",
      "Iteration 2656, loss = 0.00867345\n",
      "Iteration 2657, loss = 0.00866485\n",
      "Iteration 2658, loss = 0.00865627\n",
      "Iteration 2659, loss = 0.00864769\n",
      "Iteration 2660, loss = 0.00863913\n",
      "Iteration 2661, loss = 0.00863057\n",
      "Iteration 2662, loss = 0.00862203\n",
      "Iteration 2663, loss = 0.00861350\n",
      "Iteration 2664, loss = 0.00860498\n",
      "Iteration 2665, loss = 0.00859647\n",
      "Iteration 2666, loss = 0.00858797\n",
      "Iteration 2667, loss = 0.00857949\n",
      "Iteration 2668, loss = 0.00857101\n",
      "Iteration 2669, loss = 0.00856254\n",
      "Iteration 2670, loss = 0.00855409\n",
      "Iteration 2671, loss = 0.00854564\n",
      "Iteration 2672, loss = 0.00853721\n",
      "Iteration 2673, loss = 0.00852879\n",
      "Iteration 2674, loss = 0.00852037\n",
      "Iteration 2675, loss = 0.00851197\n",
      "Iteration 2676, loss = 0.00850358\n",
      "Iteration 2677, loss = 0.00849520\n",
      "Iteration 2678, loss = 0.00848683\n",
      "Iteration 2679, loss = 0.00847847\n",
      "Iteration 2680, loss = 0.00847012\n",
      "Iteration 2681, loss = 0.00846179\n",
      "Iteration 2682, loss = 0.00845346\n",
      "Iteration 2683, loss = 0.00844514\n",
      "Iteration 2684, loss = 0.00843684\n",
      "Iteration 2685, loss = 0.00842854\n",
      "Iteration 2686, loss = 0.00842026\n",
      "Iteration 2687, loss = 0.00841198\n",
      "Iteration 2688, loss = 0.00840372\n",
      "Iteration 2689, loss = 0.00839546\n",
      "Iteration 2690, loss = 0.00838722\n",
      "Iteration 2691, loss = 0.00837899\n",
      "Iteration 2692, loss = 0.00837077\n",
      "Iteration 2693, loss = 0.00836256\n",
      "Iteration 2694, loss = 0.00835435\n",
      "Iteration 2695, loss = 0.00834616\n",
      "Iteration 2696, loss = 0.00833798\n",
      "Iteration 2697, loss = 0.00832981\n",
      "Iteration 2698, loss = 0.00832165\n",
      "Iteration 2699, loss = 0.00831350\n",
      "Iteration 2700, loss = 0.00830536\n",
      "Iteration 2701, loss = 0.00829724\n",
      "Iteration 2702, loss = 0.00828912\n",
      "Iteration 2703, loss = 0.00828101\n",
      "Iteration 2704, loss = 0.00827291\n",
      "Iteration 2705, loss = 0.00826482\n",
      "Iteration 2706, loss = 0.00825674\n",
      "Iteration 2707, loss = 0.00824868\n",
      "Iteration 2708, loss = 0.00824062\n",
      "Iteration 2709, loss = 0.00823257\n",
      "Iteration 2710, loss = 0.00822453\n",
      "Iteration 2711, loss = 0.00821651\n",
      "Iteration 2712, loss = 0.00820849\n",
      "Iteration 2713, loss = 0.00820048\n",
      "Iteration 2714, loss = 0.00819249\n",
      "Iteration 2715, loss = 0.00818450\n",
      "Iteration 2716, loss = 0.00817652\n",
      "Iteration 2717, loss = 0.00816856\n",
      "Iteration 2718, loss = 0.00816060\n",
      "Iteration 2719, loss = 0.00815266\n",
      "Iteration 2720, loss = 0.00814472\n",
      "Iteration 2721, loss = 0.00813679\n",
      "Iteration 2722, loss = 0.00812888\n",
      "Iteration 2723, loss = 0.00812097\n",
      "Iteration 2724, loss = 0.00811307\n",
      "Iteration 2725, loss = 0.00810519\n",
      "Iteration 2726, loss = 0.00809731\n",
      "Iteration 2727, loss = 0.00808944\n",
      "Iteration 2728, loss = 0.00808158\n",
      "Iteration 2729, loss = 0.00807374\n",
      "Iteration 2730, loss = 0.00806590\n",
      "Iteration 2731, loss = 0.00805807\n",
      "Iteration 2732, loss = 0.00805025\n",
      "Iteration 2733, loss = 0.00804245\n",
      "Iteration 2734, loss = 0.00803465\n",
      "Iteration 2735, loss = 0.00802686\n",
      "Iteration 2736, loss = 0.00801908\n",
      "Iteration 2737, loss = 0.00801131\n",
      "Iteration 2738, loss = 0.00800355\n",
      "Iteration 2739, loss = 0.00799580\n",
      "Iteration 2740, loss = 0.00798806\n",
      "Iteration 2741, loss = 0.00798033\n",
      "Iteration 2742, loss = 0.00797261\n",
      "Iteration 2743, loss = 0.00796490\n",
      "Iteration 2744, loss = 0.00795720\n",
      "Iteration 2745, loss = 0.00794951\n",
      "Iteration 2746, loss = 0.00794183\n",
      "Iteration 2747, loss = 0.00793416\n",
      "Iteration 2748, loss = 0.00792649\n",
      "Iteration 2749, loss = 0.00791884\n",
      "Iteration 2750, loss = 0.00791120\n",
      "Iteration 2751, loss = 0.00790356\n",
      "Iteration 2752, loss = 0.00789594\n",
      "Iteration 2753, loss = 0.00788832\n",
      "Iteration 2754, loss = 0.00788072\n",
      "Iteration 2755, loss = 0.00787312\n",
      "Iteration 2756, loss = 0.00786553\n",
      "Iteration 2757, loss = 0.00785796\n",
      "Iteration 2758, loss = 0.00785039\n",
      "Iteration 2759, loss = 0.00784283\n",
      "Iteration 2760, loss = 0.00783528\n",
      "Iteration 2761, loss = 0.00782774\n",
      "Iteration 2762, loss = 0.00782021\n",
      "Iteration 2763, loss = 0.00781269\n",
      "Iteration 2764, loss = 0.00780518\n",
      "Iteration 2765, loss = 0.00779767\n",
      "Iteration 2766, loss = 0.00779018\n",
      "Iteration 2767, loss = 0.00778270\n",
      "Iteration 2768, loss = 0.00777522\n",
      "Iteration 2769, loss = 0.00776776\n",
      "Iteration 2770, loss = 0.00776030\n",
      "Iteration 2771, loss = 0.00775286\n",
      "Iteration 2772, loss = 0.00774542\n",
      "Iteration 2773, loss = 0.00773799\n",
      "Iteration 2774, loss = 0.00773057\n",
      "Iteration 2775, loss = 0.00772316\n",
      "Iteration 2776, loss = 0.00771576\n",
      "Iteration 2777, loss = 0.00770837\n",
      "Iteration 2778, loss = 0.00770099\n",
      "Iteration 2779, loss = 0.00769361\n",
      "Iteration 2780, loss = 0.00768625\n",
      "Iteration 2781, loss = 0.00767889\n",
      "Iteration 2782, loss = 0.00767155\n",
      "Iteration 2783, loss = 0.00766421\n",
      "Iteration 2784, loss = 0.00765688\n",
      "Iteration 2785, loss = 0.00764956\n",
      "Iteration 2786, loss = 0.00764225\n",
      "Iteration 2787, loss = 0.00763495\n",
      "Iteration 2788, loss = 0.00762766\n",
      "Iteration 2789, loss = 0.00762038\n",
      "Iteration 2790, loss = 0.00761311\n",
      "Iteration 2791, loss = 0.00760584\n",
      "Iteration 2792, loss = 0.00759858\n",
      "Iteration 2793, loss = 0.00759134\n",
      "Iteration 2794, loss = 0.00758410\n",
      "Iteration 2795, loss = 0.00757687\n",
      "Iteration 2796, loss = 0.00756965\n",
      "Iteration 2797, loss = 0.00756244\n",
      "Iteration 2798, loss = 0.00755524\n",
      "Iteration 2799, loss = 0.00754804\n",
      "Iteration 2800, loss = 0.00754086\n",
      "Iteration 2801, loss = 0.00753368\n",
      "Iteration 2802, loss = 0.00752652\n",
      "Iteration 2803, loss = 0.00751936\n",
      "Iteration 2804, loss = 0.00751221\n",
      "Iteration 2805, loss = 0.00750507\n",
      "Iteration 2806, loss = 0.00749794\n",
      "Iteration 2807, loss = 0.00749081\n",
      "Iteration 2808, loss = 0.00748370\n",
      "Iteration 2809, loss = 0.00747659\n",
      "Iteration 2810, loss = 0.00746950\n",
      "Iteration 2811, loss = 0.00746241\n",
      "Iteration 2812, loss = 0.00745533\n",
      "Iteration 2813, loss = 0.00744826\n",
      "Iteration 2814, loss = 0.00744119\n",
      "Iteration 2815, loss = 0.00743414\n",
      "Iteration 2816, loss = 0.00742710\n",
      "Iteration 2817, loss = 0.00742006\n",
      "Iteration 2818, loss = 0.00741303\n",
      "Iteration 2819, loss = 0.00740601\n",
      "Iteration 2820, loss = 0.00739900\n",
      "Iteration 2821, loss = 0.00739200\n",
      "Iteration 2822, loss = 0.00738501\n",
      "Iteration 2823, loss = 0.00737802\n",
      "Iteration 2824, loss = 0.00737104\n",
      "Iteration 2825, loss = 0.00736408\n",
      "Iteration 2826, loss = 0.00735712\n",
      "Iteration 2827, loss = 0.00735017\n",
      "Iteration 2828, loss = 0.00734322\n",
      "Iteration 2829, loss = 0.00733629\n",
      "Iteration 2830, loss = 0.00732936\n",
      "Iteration 2831, loss = 0.00732245\n",
      "Iteration 2832, loss = 0.00731554\n",
      "Iteration 2833, loss = 0.00730864\n",
      "Iteration 2834, loss = 0.00730175\n",
      "Iteration 2835, loss = 0.00729486\n",
      "Iteration 2836, loss = 0.00728799\n",
      "Iteration 2837, loss = 0.00728112\n",
      "Iteration 2838, loss = 0.00727426\n",
      "Iteration 2839, loss = 0.00726741\n",
      "Iteration 2840, loss = 0.00726057\n",
      "Iteration 2841, loss = 0.00725374\n",
      "Iteration 2842, loss = 0.00724691\n",
      "Iteration 2843, loss = 0.00724010\n",
      "Iteration 2844, loss = 0.00723329\n",
      "Iteration 2845, loss = 0.00722649\n",
      "Iteration 2846, loss = 0.00721969\n",
      "Iteration 2847, loss = 0.00721291\n",
      "Iteration 2848, loss = 0.00720614\n",
      "Iteration 2849, loss = 0.00719937\n",
      "Iteration 2850, loss = 0.00719261\n",
      "Iteration 2851, loss = 0.00718586\n",
      "Iteration 2852, loss = 0.00717912\n",
      "Iteration 2853, loss = 0.00717238\n",
      "Iteration 2854, loss = 0.00716565\n",
      "Iteration 2855, loss = 0.00715894\n",
      "Iteration 2856, loss = 0.00715223\n",
      "Iteration 2857, loss = 0.00714552\n",
      "Iteration 2858, loss = 0.00713883\n",
      "Iteration 2859, loss = 0.00713214\n",
      "Iteration 2860, loss = 0.00712547\n",
      "Iteration 2861, loss = 0.00711880\n",
      "Iteration 2862, loss = 0.00711213\n",
      "Iteration 2863, loss = 0.00710548\n",
      "Iteration 2864, loss = 0.00709884\n",
      "Iteration 2865, loss = 0.00709220\n",
      "Iteration 2866, loss = 0.00708557\n",
      "Iteration 2867, loss = 0.00707895\n",
      "Iteration 2868, loss = 0.00707233\n",
      "Iteration 2869, loss = 0.00706573\n",
      "Iteration 2870, loss = 0.00705913\n",
      "Iteration 2871, loss = 0.00705254\n",
      "Iteration 2872, loss = 0.00704596\n",
      "Iteration 2873, loss = 0.00703938\n",
      "Iteration 2874, loss = 0.00703282\n",
      "Iteration 2875, loss = 0.00702626\n",
      "Iteration 2876, loss = 0.00701971\n",
      "Iteration 2877, loss = 0.00701317\n",
      "Iteration 2878, loss = 0.00700663\n",
      "Iteration 2879, loss = 0.00700011\n",
      "Iteration 2880, loss = 0.00699359\n",
      "Iteration 2881, loss = 0.00698708\n",
      "Iteration 2882, loss = 0.00698057\n",
      "Iteration 2883, loss = 0.00697408\n",
      "Iteration 2884, loss = 0.00696759\n",
      "Iteration 2885, loss = 0.00696111\n",
      "Iteration 2886, loss = 0.00695464\n",
      "Iteration 2887, loss = 0.00694817\n",
      "Iteration 2888, loss = 0.00694172\n",
      "Iteration 2889, loss = 0.00693527\n",
      "Iteration 2890, loss = 0.00692883\n",
      "Iteration 2891, loss = 0.00692240\n",
      "Iteration 2892, loss = 0.00691597\n",
      "Iteration 2893, loss = 0.00690955\n",
      "Iteration 2894, loss = 0.00690314\n",
      "Iteration 2895, loss = 0.00689674\n",
      "Iteration 2896, loss = 0.00689034\n",
      "Iteration 2897, loss = 0.00688396\n",
      "Iteration 2898, loss = 0.00687758\n",
      "Iteration 2899, loss = 0.00687120\n",
      "Iteration 2900, loss = 0.00686484\n",
      "Iteration 2901, loss = 0.00685848\n",
      "Iteration 2902, loss = 0.00685213\n",
      "Iteration 2903, loss = 0.00684579\n",
      "Iteration 2904, loss = 0.00683946\n",
      "Iteration 2905, loss = 0.00683313\n",
      "Iteration 2906, loss = 0.00682681\n",
      "Iteration 2907, loss = 0.00682050\n",
      "Iteration 2908, loss = 0.00681420\n",
      "Iteration 2909, loss = 0.00680790\n",
      "Iteration 2910, loss = 0.00680161\n",
      "Iteration 2911, loss = 0.00679533\n",
      "Iteration 2912, loss = 0.00678906\n",
      "Iteration 2913, loss = 0.00678279\n",
      "Iteration 2914, loss = 0.00677653\n",
      "Iteration 2915, loss = 0.00677028\n",
      "Iteration 2916, loss = 0.00676403\n",
      "Iteration 2917, loss = 0.00675780\n",
      "Iteration 2918, loss = 0.00675157\n",
      "Iteration 2919, loss = 0.00674535\n",
      "Iteration 2920, loss = 0.00673913\n",
      "Iteration 2921, loss = 0.00673292\n",
      "Iteration 2922, loss = 0.00672672\n",
      "Iteration 2923, loss = 0.00672053\n",
      "Iteration 2924, loss = 0.00671435\n",
      "Iteration 2925, loss = 0.00670817\n",
      "Iteration 2926, loss = 0.00670200\n",
      "Iteration 2927, loss = 0.00669583\n",
      "Iteration 2928, loss = 0.00668968\n",
      "Iteration 2929, loss = 0.00668353\n",
      "Iteration 2930, loss = 0.00667739\n",
      "Iteration 2931, loss = 0.00667125\n",
      "Iteration 2932, loss = 0.00666513\n",
      "Iteration 2933, loss = 0.00665901\n",
      "Iteration 2934, loss = 0.00665290\n",
      "Iteration 2935, loss = 0.00664679\n",
      "Iteration 2936, loss = 0.00664069\n",
      "Iteration 2937, loss = 0.00663460\n",
      "Iteration 2938, loss = 0.00662852\n",
      "Iteration 2939, loss = 0.00662244\n",
      "Iteration 2940, loss = 0.00661637\n",
      "Iteration 2941, loss = 0.00661031\n",
      "Iteration 2942, loss = 0.00660425\n",
      "Iteration 2943, loss = 0.00659821\n",
      "Iteration 2944, loss = 0.00659217\n",
      "Iteration 2945, loss = 0.00658613\n",
      "Iteration 2946, loss = 0.00658011\n",
      "Iteration 2947, loss = 0.00657409\n",
      "Iteration 2948, loss = 0.00656807\n",
      "Iteration 2949, loss = 0.00656207\n",
      "Iteration 2950, loss = 0.00655607\n",
      "Iteration 2951, loss = 0.00655008\n",
      "Iteration 2952, loss = 0.00654410\n",
      "Iteration 2953, loss = 0.00653812\n",
      "Iteration 2954, loss = 0.00653215\n",
      "Iteration 2955, loss = 0.00652618\n",
      "Iteration 2956, loss = 0.00652023\n",
      "Iteration 2957, loss = 0.00651428\n",
      "Iteration 2958, loss = 0.00650834\n",
      "Iteration 2959, loss = 0.00650240\n",
      "Iteration 2960, loss = 0.00649647\n",
      "Iteration 2961, loss = 0.00649055\n",
      "Iteration 2962, loss = 0.00648464\n",
      "Iteration 2963, loss = 0.00647873\n",
      "Iteration 2964, loss = 0.00647283\n",
      "Iteration 2965, loss = 0.00646694\n",
      "Iteration 2966, loss = 0.00646105\n",
      "Iteration 2967, loss = 0.00645517\n",
      "Iteration 2968, loss = 0.00644930\n",
      "Iteration 2969, loss = 0.00644343\n",
      "Iteration 2970, loss = 0.00643757\n",
      "Iteration 2971, loss = 0.00643172\n",
      "Iteration 2972, loss = 0.00642587\n",
      "Iteration 2973, loss = 0.00642003\n",
      "Iteration 2974, loss = 0.00641420\n",
      "Iteration 2975, loss = 0.00640838\n",
      "Iteration 2976, loss = 0.00640256\n",
      "Iteration 2977, loss = 0.00639675\n",
      "Iteration 2978, loss = 0.00639094\n",
      "Iteration 2979, loss = 0.00638514\n",
      "Iteration 2980, loss = 0.00637935\n",
      "Iteration 2981, loss = 0.00637357\n",
      "Iteration 2982, loss = 0.00636779\n",
      "Iteration 2983, loss = 0.00636202\n",
      "Iteration 2984, loss = 0.00635625\n",
      "Iteration 2985, loss = 0.00635050\n",
      "Iteration 2986, loss = 0.00634475\n",
      "Iteration 2987, loss = 0.00633900\n",
      "Iteration 2988, loss = 0.00633326\n",
      "Iteration 2989, loss = 0.00632753\n",
      "Iteration 2990, loss = 0.00632181\n",
      "Iteration 2991, loss = 0.00631609\n",
      "Iteration 2992, loss = 0.00631038\n",
      "Iteration 2993, loss = 0.00630467\n",
      "Iteration 2994, loss = 0.00629898\n",
      "Iteration 2995, loss = 0.00629329\n",
      "Iteration 2996, loss = 0.00628760\n",
      "Iteration 2997, loss = 0.00628192\n",
      "Iteration 2998, loss = 0.00627625\n",
      "Iteration 2999, loss = 0.00627059\n",
      "Iteration 3000, loss = 0.00626493\n",
      "Iteration 3001, loss = 0.00625928\n",
      "Iteration 3002, loss = 0.00625363\n",
      "Iteration 3003, loss = 0.00624799\n",
      "Iteration 3004, loss = 0.00624236\n",
      "Iteration 3005, loss = 0.00623673\n",
      "Iteration 3006, loss = 0.00623111\n",
      "Iteration 3007, loss = 0.00622550\n",
      "Iteration 3008, loss = 0.00621989\n",
      "Iteration 3009, loss = 0.00621429\n",
      "Iteration 3010, loss = 0.00620870\n",
      "Iteration 3011, loss = 0.00620311\n",
      "Iteration 3012, loss = 0.00619753\n",
      "Iteration 3013, loss = 0.00619196\n",
      "Iteration 3014, loss = 0.00618639\n",
      "Iteration 3015, loss = 0.00618083\n",
      "Iteration 3016, loss = 0.00617527\n",
      "Iteration 3017, loss = 0.00616972\n",
      "Iteration 3018, loss = 0.00616418\n",
      "Iteration 3019, loss = 0.00615865\n",
      "Iteration 3020, loss = 0.00615312\n",
      "Iteration 3021, loss = 0.00614759\n",
      "Iteration 3022, loss = 0.00614208\n",
      "Iteration 3023, loss = 0.00613657\n",
      "Iteration 3024, loss = 0.00613106\n",
      "Iteration 3025, loss = 0.00612556\n",
      "Iteration 3026, loss = 0.00612007\n",
      "Iteration 3027, loss = 0.00611459\n",
      "Iteration 3028, loss = 0.00610911\n",
      "Iteration 3029, loss = 0.00610363\n",
      "Iteration 3030, loss = 0.00609817\n",
      "Iteration 3031, loss = 0.00609271\n",
      "Iteration 3032, loss = 0.00608725\n",
      "Iteration 3033, loss = 0.00608181\n",
      "Iteration 3034, loss = 0.00607636\n",
      "Iteration 3035, loss = 0.00607093\n",
      "Iteration 3036, loss = 0.00606550\n",
      "Iteration 3037, loss = 0.00606008\n",
      "Iteration 3038, loss = 0.00605466\n",
      "Iteration 3039, loss = 0.00604925\n",
      "Iteration 3040, loss = 0.00604384\n",
      "Iteration 3041, loss = 0.00603845\n",
      "Iteration 3042, loss = 0.00603305\n",
      "Iteration 3043, loss = 0.00602767\n",
      "Iteration 3044, loss = 0.00602229\n",
      "Iteration 3045, loss = 0.00601691\n",
      "Iteration 3046, loss = 0.00601154\n",
      "Iteration 3047, loss = 0.00600618\n",
      "Iteration 3048, loss = 0.00600083\n",
      "Iteration 3049, loss = 0.00599548\n",
      "Iteration 3050, loss = 0.00599013\n",
      "Iteration 3051, loss = 0.00598480\n",
      "Iteration 3052, loss = 0.00597947\n",
      "Iteration 3053, loss = 0.00597414\n",
      "Iteration 3054, loss = 0.00596882\n",
      "Iteration 3055, loss = 0.00596351\n",
      "Iteration 3056, loss = 0.00595820\n",
      "Iteration 3057, loss = 0.00595290\n",
      "Iteration 3058, loss = 0.00594760\n",
      "Iteration 3059, loss = 0.00594231\n",
      "Iteration 3060, loss = 0.00593703\n",
      "Iteration 3061, loss = 0.00593175\n",
      "Iteration 3062, loss = 0.00592648\n",
      "Iteration 3063, loss = 0.00592122\n",
      "Iteration 3064, loss = 0.00591596\n",
      "Iteration 3065, loss = 0.00591070\n",
      "Iteration 3066, loss = 0.00590546\n",
      "Iteration 3067, loss = 0.00590021\n",
      "Iteration 3068, loss = 0.00589498\n",
      "Iteration 3069, loss = 0.00588975\n",
      "Iteration 3070, loss = 0.00588453\n",
      "Iteration 3071, loss = 0.00587931\n",
      "Iteration 3072, loss = 0.00587409\n",
      "Iteration 3073, loss = 0.00586889\n",
      "Iteration 3074, loss = 0.00586369\n",
      "Iteration 3075, loss = 0.00585849\n",
      "Iteration 3076, loss = 0.00585330\n",
      "Iteration 3077, loss = 0.00584812\n",
      "Iteration 3078, loss = 0.00584294\n",
      "Iteration 3079, loss = 0.00583777\n",
      "Iteration 3080, loss = 0.00583261\n",
      "Iteration 3081, loss = 0.00582745\n",
      "Iteration 3082, loss = 0.00582229\n",
      "Iteration 3083, loss = 0.00581714\n",
      "Iteration 3084, loss = 0.00581200\n",
      "Iteration 3085, loss = 0.00580686\n",
      "Iteration 3086, loss = 0.00580173\n",
      "Iteration 3087, loss = 0.00579661\n",
      "Iteration 3088, loss = 0.00579149\n",
      "Iteration 3089, loss = 0.00578637\n",
      "Iteration 3090, loss = 0.00578127\n",
      "Iteration 3091, loss = 0.00577616\n",
      "Iteration 3092, loss = 0.00577107\n",
      "Iteration 3093, loss = 0.00576598\n",
      "Iteration 3094, loss = 0.00576089\n",
      "Iteration 3095, loss = 0.00575581\n",
      "Iteration 3096, loss = 0.00575074\n",
      "Iteration 3097, loss = 0.00574567\n",
      "Iteration 3098, loss = 0.00574061\n",
      "Iteration 3099, loss = 0.00573555\n",
      "Iteration 3100, loss = 0.00573050\n",
      "Iteration 3101, loss = 0.00572545\n",
      "Iteration 3102, loss = 0.00572041\n",
      "Iteration 3103, loss = 0.00571538\n",
      "Iteration 3104, loss = 0.00571035\n",
      "Iteration 3105, loss = 0.00570532\n",
      "Iteration 3106, loss = 0.00570030\n",
      "Iteration 3107, loss = 0.00569529\n",
      "Iteration 3108, loss = 0.00569029\n",
      "Iteration 3109, loss = 0.00568528\n",
      "Iteration 3110, loss = 0.00568029\n",
      "Iteration 3111, loss = 0.00567530\n",
      "Iteration 3112, loss = 0.00567031\n",
      "Iteration 3113, loss = 0.00566533\n",
      "Iteration 3114, loss = 0.00566036\n",
      "Iteration 3115, loss = 0.00565539\n",
      "Iteration 3116, loss = 0.00565043\n",
      "Iteration 3117, loss = 0.00564547\n",
      "Iteration 3118, loss = 0.00564052\n",
      "Iteration 3119, loss = 0.00563557\n",
      "Iteration 3120, loss = 0.00563063\n",
      "Iteration 3121, loss = 0.00562570\n",
      "Iteration 3122, loss = 0.00562077\n",
      "Iteration 3123, loss = 0.00561584\n",
      "Iteration 3124, loss = 0.00561092\n",
      "Iteration 3125, loss = 0.00560601\n",
      "Iteration 3126, loss = 0.00560110\n",
      "Iteration 3127, loss = 0.00559620\n",
      "Iteration 3128, loss = 0.00559130\n",
      "Iteration 3129, loss = 0.00558641\n",
      "Iteration 3130, loss = 0.00558152\n",
      "Iteration 3131, loss = 0.00557664\n",
      "Iteration 3132, loss = 0.00557176\n",
      "Iteration 3133, loss = 0.00556689\n",
      "Iteration 3134, loss = 0.00556202\n",
      "Iteration 3135, loss = 0.00555716\n",
      "Iteration 3136, loss = 0.00555231\n",
      "Iteration 3137, loss = 0.00554746\n",
      "Iteration 3138, loss = 0.00554262\n",
      "Iteration 3139, loss = 0.00553778\n",
      "Iteration 3140, loss = 0.00553294\n",
      "Iteration 3141, loss = 0.00552811\n",
      "Iteration 3142, loss = 0.00552329\n",
      "Iteration 3143, loss = 0.00551847\n",
      "Iteration 3144, loss = 0.00551366\n",
      "Iteration 3145, loss = 0.00550885\n",
      "Iteration 3146, loss = 0.00550405\n",
      "Iteration 3147, loss = 0.00549925\n",
      "Iteration 3148, loss = 0.00549446\n",
      "Iteration 3149, loss = 0.00548967\n",
      "Iteration 3150, loss = 0.00548489\n",
      "Iteration 3151, loss = 0.00548012\n",
      "Iteration 3152, loss = 0.00547535\n",
      "Iteration 3153, loss = 0.00547058\n",
      "Iteration 3154, loss = 0.00546582\n",
      "Iteration 3155, loss = 0.00546106\n",
      "Iteration 3156, loss = 0.00545631\n",
      "Iteration 3157, loss = 0.00545157\n",
      "Iteration 3158, loss = 0.00544683\n",
      "Iteration 3159, loss = 0.00544209\n",
      "Iteration 3160, loss = 0.00543736\n",
      "Iteration 3161, loss = 0.00543264\n",
      "Iteration 3162, loss = 0.00542792\n",
      "Iteration 3163, loss = 0.00542320\n",
      "Iteration 3164, loss = 0.00541849\n",
      "Iteration 3165, loss = 0.00541379\n",
      "Iteration 3166, loss = 0.00540909\n",
      "Iteration 3167, loss = 0.00540440\n",
      "Iteration 3168, loss = 0.00539971\n",
      "Iteration 3169, loss = 0.00539502\n",
      "Iteration 3170, loss = 0.00539034\n",
      "Iteration 3171, loss = 0.00538567\n",
      "Iteration 3172, loss = 0.00538100\n",
      "Iteration 3173, loss = 0.00537634\n",
      "Iteration 3174, loss = 0.00537168\n",
      "Iteration 3175, loss = 0.00536703\n",
      "Iteration 3176, loss = 0.00536238\n",
      "Iteration 3177, loss = 0.00535773\n",
      "Iteration 3178, loss = 0.00535309\n",
      "Iteration 3179, loss = 0.00534846\n",
      "Iteration 3180, loss = 0.00534383\n",
      "Iteration 3181, loss = 0.00533921\n",
      "Iteration 3182, loss = 0.00533459\n",
      "Iteration 3183, loss = 0.00532997\n",
      "Iteration 3184, loss = 0.00532536\n",
      "Iteration 3185, loss = 0.00532076\n",
      "Iteration 3186, loss = 0.00531616\n",
      "Iteration 3187, loss = 0.00531157\n",
      "Iteration 3188, loss = 0.00530698\n",
      "Iteration 3189, loss = 0.00530239\n",
      "Iteration 3190, loss = 0.00529781\n",
      "Iteration 3191, loss = 0.00529324\n",
      "Iteration 3192, loss = 0.00528867\n",
      "Iteration 3193, loss = 0.00528410\n",
      "Iteration 3194, loss = 0.00527954\n",
      "Iteration 3195, loss = 0.00527499\n",
      "Iteration 3196, loss = 0.00527044\n",
      "Iteration 3197, loss = 0.00526589\n",
      "Iteration 3198, loss = 0.00526135\n",
      "Iteration 3199, loss = 0.00525682\n",
      "Iteration 3200, loss = 0.00525229\n",
      "Iteration 3201, loss = 0.00524776\n",
      "Iteration 3202, loss = 0.00524324\n",
      "Iteration 3203, loss = 0.00523872\n",
      "Iteration 3204, loss = 0.00523421\n",
      "Iteration 3205, loss = 0.00522970\n",
      "Iteration 3206, loss = 0.00522520\n",
      "Iteration 3207, loss = 0.00522071\n",
      "Iteration 3208, loss = 0.00521621\n",
      "Iteration 3209, loss = 0.00521173\n",
      "Iteration 3210, loss = 0.00520724\n",
      "Iteration 3211, loss = 0.00520276\n",
      "Iteration 3212, loss = 0.00519829\n",
      "Iteration 3213, loss = 0.00519382\n",
      "Iteration 3214, loss = 0.00518936\n",
      "Iteration 3215, loss = 0.00518490\n",
      "Iteration 3216, loss = 0.00518044\n",
      "Iteration 3217, loss = 0.00517600\n",
      "Iteration 3218, loss = 0.00517155\n",
      "Iteration 3219, loss = 0.00516711\n",
      "Iteration 3220, loss = 0.00516267\n",
      "Iteration 3221, loss = 0.00515824\n",
      "Iteration 3222, loss = 0.00515382\n",
      "Iteration 3223, loss = 0.00514940\n",
      "Iteration 3224, loss = 0.00514498\n",
      "Iteration 3225, loss = 0.00514057\n",
      "Iteration 3226, loss = 0.00513616\n",
      "Iteration 3227, loss = 0.00513176\n",
      "Iteration 3228, loss = 0.00512736\n",
      "Iteration 3229, loss = 0.00512297\n",
      "Iteration 3230, loss = 0.00511858\n",
      "Iteration 3231, loss = 0.00511419\n",
      "Iteration 3232, loss = 0.00510981\n",
      "Iteration 3233, loss = 0.00510544\n",
      "Iteration 3234, loss = 0.00510107\n",
      "Iteration 3235, loss = 0.00509670\n",
      "Iteration 3236, loss = 0.00509234\n",
      "Iteration 3237, loss = 0.00508799\n",
      "Iteration 3238, loss = 0.00508364\n",
      "Iteration 3239, loss = 0.00507929\n",
      "Iteration 3240, loss = 0.00507495\n",
      "Iteration 3241, loss = 0.00507061\n",
      "Iteration 3242, loss = 0.00506628\n",
      "Iteration 3243, loss = 0.00506195\n",
      "Iteration 3244, loss = 0.00505762\n",
      "Iteration 3245, loss = 0.00505330\n",
      "Iteration 3246, loss = 0.00504899\n",
      "Iteration 3247, loss = 0.00504468\n",
      "Iteration 3248, loss = 0.00504037\n",
      "Iteration 3249, loss = 0.00503607\n",
      "Iteration 3250, loss = 0.00503177\n",
      "Iteration 3251, loss = 0.00502748\n",
      "Iteration 3252, loss = 0.00502319\n",
      "Iteration 3253, loss = 0.00501891\n",
      "Iteration 3254, loss = 0.00501463\n",
      "Iteration 3255, loss = 0.00501036\n",
      "Iteration 3256, loss = 0.00500609\n",
      "Iteration 3257, loss = 0.00500182\n",
      "Iteration 3258, loss = 0.00499756\n",
      "Iteration 3259, loss = 0.00499331\n",
      "Iteration 3260, loss = 0.00498906\n",
      "Iteration 3261, loss = 0.00498481\n",
      "Iteration 3262, loss = 0.00498057\n",
      "Iteration 3263, loss = 0.00497633\n",
      "Iteration 3264, loss = 0.00497210\n",
      "Iteration 3265, loss = 0.00496787\n",
      "Iteration 3266, loss = 0.00496364\n",
      "Iteration 3267, loss = 0.00495942\n",
      "Iteration 3268, loss = 0.00495521\n",
      "Iteration 3269, loss = 0.00495100\n",
      "Iteration 3270, loss = 0.00494679\n",
      "Iteration 3271, loss = 0.00494259\n",
      "Iteration 3272, loss = 0.00493839\n",
      "Iteration 3273, loss = 0.00493420\n",
      "Iteration 3274, loss = 0.00493001\n",
      "Iteration 3275, loss = 0.00492582\n",
      "Iteration 3276, loss = 0.00492164\n",
      "Iteration 3277, loss = 0.00491747\n",
      "Iteration 3278, loss = 0.00491330\n",
      "Iteration 3279, loss = 0.00490913\n",
      "Iteration 3280, loss = 0.00490497\n",
      "Iteration 3281, loss = 0.00490081\n",
      "Iteration 3282, loss = 0.00489666\n",
      "Iteration 3283, loss = 0.00489251\n",
      "Iteration 3284, loss = 0.00488836\n",
      "Iteration 3285, loss = 0.00488422\n",
      "Iteration 3286, loss = 0.00488009\n",
      "Iteration 3287, loss = 0.00487596\n",
      "Iteration 3288, loss = 0.00487183\n",
      "Iteration 3289, loss = 0.00486771\n",
      "Iteration 3290, loss = 0.00486359\n",
      "Iteration 3291, loss = 0.00485947\n",
      "Iteration 3292, loss = 0.00485536\n",
      "Iteration 3293, loss = 0.00485126\n",
      "Iteration 3294, loss = 0.00484716\n",
      "Iteration 3295, loss = 0.00484306\n",
      "Iteration 3296, loss = 0.00483897\n",
      "Iteration 3297, loss = 0.00483488\n",
      "Iteration 3298, loss = 0.00483080\n",
      "Iteration 3299, loss = 0.00482672\n",
      "Iteration 3300, loss = 0.00482264\n",
      "Iteration 3301, loss = 0.00481857\n",
      "Iteration 3302, loss = 0.00481451\n",
      "Iteration 3303, loss = 0.00481045\n",
      "Iteration 3304, loss = 0.00480639\n",
      "Iteration 3305, loss = 0.00480233\n",
      "Iteration 3306, loss = 0.00479829\n",
      "Iteration 3307, loss = 0.00479424\n",
      "Iteration 3308, loss = 0.00479020\n",
      "Iteration 3309, loss = 0.00478617\n",
      "Iteration 3310, loss = 0.00478213\n",
      "Iteration 3311, loss = 0.00477811\n",
      "Iteration 3312, loss = 0.00477408\n",
      "Iteration 3313, loss = 0.00477006\n",
      "Iteration 3314, loss = 0.00476605\n",
      "Iteration 3315, loss = 0.00476204\n",
      "Iteration 3316, loss = 0.00475803\n",
      "Iteration 3317, loss = 0.00475403\n",
      "Iteration 3318, loss = 0.00475003\n",
      "Iteration 3319, loss = 0.00474604\n",
      "Iteration 3320, loss = 0.00474205\n",
      "Iteration 3321, loss = 0.00473807\n",
      "Iteration 3322, loss = 0.00473409\n",
      "Iteration 3323, loss = 0.00473011\n",
      "Iteration 3324, loss = 0.00472614\n",
      "Iteration 3325, loss = 0.00472217\n",
      "Iteration 3326, loss = 0.00471821\n",
      "Iteration 3327, loss = 0.00471425\n",
      "Iteration 3328, loss = 0.00471029\n",
      "Iteration 3329, loss = 0.00470634\n",
      "Iteration 3330, loss = 0.00470240\n",
      "Iteration 3331, loss = 0.00469846\n",
      "Iteration 3332, loss = 0.00469452\n",
      "Iteration 3333, loss = 0.00469058\n",
      "Iteration 3334, loss = 0.00468665\n",
      "Iteration 3335, loss = 0.00468273\n",
      "Iteration 3336, loss = 0.00467881\n",
      "Iteration 3337, loss = 0.00467489\n",
      "Iteration 3338, loss = 0.00467098\n",
      "Iteration 3339, loss = 0.00466707\n",
      "Iteration 3340, loss = 0.00466316\n",
      "Iteration 3341, loss = 0.00465926\n",
      "Iteration 3342, loss = 0.00465537\n",
      "Iteration 3343, loss = 0.00465148\n",
      "Iteration 3344, loss = 0.00464759\n",
      "Iteration 3345, loss = 0.00464370\n",
      "Iteration 3346, loss = 0.00463982\n",
      "Iteration 3347, loss = 0.00463595\n",
      "Iteration 3348, loss = 0.00463208\n",
      "Iteration 3349, loss = 0.00462821\n",
      "Iteration 3350, loss = 0.00462435\n",
      "Iteration 3351, loss = 0.00462049\n",
      "Iteration 3352, loss = 0.00461663\n",
      "Iteration 3353, loss = 0.00461278\n",
      "Iteration 3354, loss = 0.00460894\n",
      "Iteration 3355, loss = 0.00460510\n",
      "Iteration 3356, loss = 0.00460126\n",
      "Iteration 3357, loss = 0.00459742\n",
      "Iteration 3358, loss = 0.00459359\n",
      "Iteration 3359, loss = 0.00458977\n",
      "Iteration 3360, loss = 0.00458595\n",
      "Iteration 3361, loss = 0.00458213\n",
      "Iteration 3362, loss = 0.00457831\n",
      "Iteration 3363, loss = 0.00457451\n",
      "Iteration 3364, loss = 0.00457070\n",
      "Iteration 3365, loss = 0.00456690\n",
      "Iteration 3366, loss = 0.00456310\n",
      "Iteration 3367, loss = 0.00455931\n",
      "Iteration 3368, loss = 0.00455552\n",
      "Iteration 3369, loss = 0.00455173\n",
      "Iteration 3370, loss = 0.00454795\n",
      "Iteration 3371, loss = 0.00454418\n",
      "Iteration 3372, loss = 0.00454040\n",
      "Iteration 3373, loss = 0.00453663\n",
      "Iteration 3374, loss = 0.00453287\n",
      "Iteration 3375, loss = 0.00452911\n",
      "Iteration 3376, loss = 0.00452535\n",
      "Iteration 3377, loss = 0.00452160\n",
      "Iteration 3378, loss = 0.00451785\n",
      "Iteration 3379, loss = 0.00451411\n",
      "Iteration 3380, loss = 0.00451037\n",
      "Iteration 3381, loss = 0.00450663\n",
      "Iteration 3382, loss = 0.00450290\n",
      "Iteration 3383, loss = 0.00449917\n",
      "Iteration 3384, loss = 0.00449544\n",
      "Iteration 3385, loss = 0.00449172\n",
      "Iteration 3386, loss = 0.00448801\n",
      "Iteration 3387, loss = 0.00448429\n",
      "Iteration 3388, loss = 0.00448059\n",
      "Iteration 3389, loss = 0.00447688\n",
      "Iteration 3390, loss = 0.00447318\n",
      "Iteration 3391, loss = 0.00446948\n",
      "Iteration 3392, loss = 0.00446579\n",
      "Iteration 3393, loss = 0.00446210\n",
      "Iteration 3394, loss = 0.00445842\n",
      "Iteration 3395, loss = 0.00445474\n",
      "Iteration 3396, loss = 0.00445106\n",
      "Iteration 3397, loss = 0.00444739\n",
      "Iteration 3398, loss = 0.00444372\n",
      "Iteration 3399, loss = 0.00444005\n",
      "Iteration 3400, loss = 0.00443639\n",
      "Iteration 3401, loss = 0.00443273\n",
      "Iteration 3402, loss = 0.00442908\n",
      "Iteration 3403, loss = 0.00442543\n",
      "Iteration 3404, loss = 0.00442179\n",
      "Iteration 3405, loss = 0.00441814\n",
      "Iteration 3406, loss = 0.00441451\n",
      "Iteration 3407, loss = 0.00441087\n",
      "Iteration 3408, loss = 0.00440724\n",
      "Iteration 3409, loss = 0.00440362\n",
      "Iteration 3410, loss = 0.00439999\n",
      "Iteration 3411, loss = 0.00439638\n",
      "Iteration 3412, loss = 0.00439276\n",
      "Iteration 3413, loss = 0.00438915\n",
      "Iteration 3414, loss = 0.00438555\n",
      "Iteration 3415, loss = 0.00438194\n",
      "Iteration 3416, loss = 0.00437834\n",
      "Iteration 3417, loss = 0.00437475\n",
      "Iteration 3418, loss = 0.00437116\n",
      "Iteration 3419, loss = 0.00436757\n",
      "Iteration 3420, loss = 0.00436399\n",
      "Iteration 3421, loss = 0.00436041\n",
      "Iteration 3422, loss = 0.00435683\n",
      "Iteration 3423, loss = 0.00435326\n",
      "Iteration 3424, loss = 0.00434969\n",
      "Iteration 3425, loss = 0.00434613\n",
      "Iteration 3426, loss = 0.00434257\n",
      "Iteration 3427, loss = 0.00433901\n",
      "Iteration 3428, loss = 0.00433546\n",
      "Iteration 3429, loss = 0.00433191\n",
      "Iteration 3430, loss = 0.00432836\n",
      "Iteration 3431, loss = 0.00432482\n",
      "Iteration 3432, loss = 0.00432128\n",
      "Iteration 3433, loss = 0.00431775\n",
      "Iteration 3434, loss = 0.00431422\n",
      "Iteration 3435, loss = 0.00431069\n",
      "Iteration 3436, loss = 0.00430717\n",
      "Iteration 3437, loss = 0.00430365\n",
      "Iteration 3438, loss = 0.00430014\n",
      "Iteration 3439, loss = 0.00429662\n",
      "Iteration 3440, loss = 0.00429312\n",
      "Iteration 3441, loss = 0.00428961\n",
      "Iteration 3442, loss = 0.00428611\n",
      "Iteration 3443, loss = 0.00428262\n",
      "Iteration 3444, loss = 0.00427912\n",
      "Iteration 3445, loss = 0.00427563\n",
      "Iteration 3446, loss = 0.00427215\n",
      "Iteration 3447, loss = 0.00426867\n",
      "Iteration 3448, loss = 0.00426519\n",
      "Iteration 3449, loss = 0.00426172\n",
      "Iteration 3450, loss = 0.00425825\n",
      "Iteration 3451, loss = 0.00425478\n",
      "Iteration 3452, loss = 0.00425132\n",
      "Iteration 3453, loss = 0.00424786\n",
      "Iteration 3454, loss = 0.00424440\n",
      "Iteration 3455, loss = 0.00424095\n",
      "Iteration 3456, loss = 0.00423750\n",
      "Iteration 3457, loss = 0.00423406\n",
      "Iteration 3458, loss = 0.00423062\n",
      "Iteration 3459, loss = 0.00422718\n",
      "Iteration 3460, loss = 0.00422374\n",
      "Iteration 3461, loss = 0.00422031\n",
      "Iteration 3462, loss = 0.00421689\n",
      "Iteration 3463, loss = 0.00421347\n",
      "Iteration 3464, loss = 0.00421005\n",
      "Iteration 3465, loss = 0.00420663\n",
      "Iteration 3466, loss = 0.00420322\n",
      "Iteration 3467, loss = 0.00419981\n",
      "Iteration 3468, loss = 0.00419641\n",
      "Iteration 3469, loss = 0.00419301\n",
      "Iteration 3470, loss = 0.00418961\n",
      "Iteration 3471, loss = 0.00418622\n",
      "Iteration 3472, loss = 0.00418283\n",
      "Iteration 3473, loss = 0.00417944\n",
      "Iteration 3474, loss = 0.00417606\n",
      "Iteration 3475, loss = 0.00417268\n",
      "Iteration 3476, loss = 0.00416930\n",
      "Iteration 3477, loss = 0.00416593\n",
      "Iteration 3478, loss = 0.00416256\n",
      "Iteration 3479, loss = 0.00415920\n",
      "Iteration 3480, loss = 0.00415583\n",
      "Iteration 3481, loss = 0.00415248\n",
      "Iteration 3482, loss = 0.00414912\n",
      "Iteration 3483, loss = 0.00414577\n",
      "Iteration 3484, loss = 0.00414242\n",
      "Iteration 3485, loss = 0.00413908\n",
      "Iteration 3486, loss = 0.00413574\n",
      "Iteration 3487, loss = 0.00413240\n",
      "Iteration 3488, loss = 0.00412907\n",
      "Iteration 3489, loss = 0.00412574\n",
      "Iteration 3490, loss = 0.00412241\n",
      "Iteration 3491, loss = 0.00411909\n",
      "Iteration 3492, loss = 0.00411577\n",
      "Iteration 3493, loss = 0.00411246\n",
      "Iteration 3494, loss = 0.00410915\n",
      "Iteration 3495, loss = 0.00410584\n",
      "Iteration 3496, loss = 0.00410253\n",
      "Iteration 3497, loss = 0.00409923\n",
      "Iteration 3498, loss = 0.00409593\n",
      "Iteration 3499, loss = 0.00409264\n",
      "Iteration 3500, loss = 0.00408935\n",
      "Iteration 3501, loss = 0.00408606\n",
      "Iteration 3502, loss = 0.00408278\n",
      "Iteration 3503, loss = 0.00407949\n",
      "Iteration 3504, loss = 0.00407622\n",
      "Iteration 3505, loss = 0.00407294\n",
      "Iteration 3506, loss = 0.00406967\n",
      "Iteration 3507, loss = 0.00406641\n",
      "Iteration 3508, loss = 0.00406314\n",
      "Iteration 3509, loss = 0.00405988\n",
      "Iteration 3510, loss = 0.00405663\n",
      "Iteration 3511, loss = 0.00405337\n",
      "Iteration 3512, loss = 0.00405012\n",
      "Iteration 3513, loss = 0.00404688\n",
      "Iteration 3514, loss = 0.00404363\n",
      "Iteration 3515, loss = 0.00404039\n",
      "Iteration 3516, loss = 0.00403716\n",
      "Iteration 3517, loss = 0.00403393\n",
      "Iteration 3518, loss = 0.00403070\n",
      "Iteration 3519, loss = 0.00402747\n",
      "Iteration 3520, loss = 0.00402425\n",
      "Iteration 3521, loss = 0.00402103\n",
      "Iteration 3522, loss = 0.00401781\n",
      "Iteration 3523, loss = 0.00401460\n",
      "Iteration 3524, loss = 0.00401139\n",
      "Iteration 3525, loss = 0.00400819\n",
      "Iteration 3526, loss = 0.00400498\n",
      "Iteration 3527, loss = 0.00400178\n",
      "Iteration 3528, loss = 0.00399859\n",
      "Iteration 3529, loss = 0.00399540\n",
      "Iteration 3530, loss = 0.00399221\n",
      "Iteration 3531, loss = 0.00398902\n",
      "Iteration 3532, loss = 0.00398584\n",
      "Iteration 3533, loss = 0.00398266\n",
      "Iteration 3534, loss = 0.00397948\n",
      "Iteration 3535, loss = 0.00397631\n",
      "Iteration 3536, loss = 0.00397314\n",
      "Iteration 3537, loss = 0.00396998\n",
      "Iteration 3538, loss = 0.00396681\n",
      "Iteration 3539, loss = 0.00396366\n",
      "Iteration 3540, loss = 0.00396050\n",
      "Iteration 3541, loss = 0.00395735\n",
      "Iteration 3542, loss = 0.00395420\n",
      "Iteration 3543, loss = 0.00395105\n",
      "Iteration 3544, loss = 0.00394791\n",
      "Iteration 3545, loss = 0.00394477\n",
      "Iteration 3546, loss = 0.00394163\n",
      "Iteration 3547, loss = 0.00393850\n",
      "Iteration 3548, loss = 0.00393537\n",
      "Iteration 3549, loss = 0.00393224\n",
      "Iteration 3550, loss = 0.00392912\n",
      "Iteration 3551, loss = 0.00392600\n",
      "Iteration 3552, loss = 0.00392289\n",
      "Iteration 3553, loss = 0.00391977\n",
      "Iteration 3554, loss = 0.00391666\n",
      "Iteration 3555, loss = 0.00391355\n",
      "Iteration 3556, loss = 0.00391045\n",
      "Iteration 3557, loss = 0.00390735\n",
      "Iteration 3558, loss = 0.00390425\n",
      "Iteration 3559, loss = 0.00390116\n",
      "Iteration 3560, loss = 0.00389807\n",
      "Iteration 3561, loss = 0.00389498\n",
      "Iteration 3562, loss = 0.00389190\n",
      "Iteration 3563, loss = 0.00388881\n",
      "Iteration 3564, loss = 0.00388574\n",
      "Iteration 3565, loss = 0.00388266\n",
      "Iteration 3566, loss = 0.00387959\n",
      "Iteration 3567, loss = 0.00387652\n",
      "Iteration 3568, loss = 0.00387346\n",
      "Iteration 3569, loss = 0.00387039\n",
      "Iteration 3570, loss = 0.00386733\n",
      "Iteration 3571, loss = 0.00386428\n",
      "Iteration 3572, loss = 0.00386123\n",
      "Iteration 3573, loss = 0.00385818\n",
      "Iteration 3574, loss = 0.00385513\n",
      "Iteration 3575, loss = 0.00385209\n",
      "Iteration 3576, loss = 0.00384905\n",
      "Iteration 3577, loss = 0.00384601\n",
      "Iteration 3578, loss = 0.00384298\n",
      "Iteration 3579, loss = 0.00383994\n",
      "Iteration 3580, loss = 0.00383692\n",
      "Iteration 3581, loss = 0.00383389\n",
      "Iteration 3582, loss = 0.00383087\n",
      "Iteration 3583, loss = 0.00382785\n",
      "Iteration 3584, loss = 0.00382484\n",
      "Iteration 3585, loss = 0.00382183\n",
      "Iteration 3586, loss = 0.00381882\n",
      "Iteration 3587, loss = 0.00381581\n",
      "Iteration 3588, loss = 0.00381281\n",
      "Iteration 3589, loss = 0.00380981\n",
      "Iteration 3590, loss = 0.00380681\n",
      "Iteration 3591, loss = 0.00380382\n",
      "Iteration 3592, loss = 0.00380083\n",
      "Iteration 3593, loss = 0.00379784\n",
      "Iteration 3594, loss = 0.00379486\n",
      "Iteration 3595, loss = 0.00379187\n",
      "Iteration 3596, loss = 0.00378890\n",
      "Iteration 3597, loss = 0.00378592\n",
      "Iteration 3598, loss = 0.00378295\n",
      "Iteration 3599, loss = 0.00377998\n",
      "Iteration 3600, loss = 0.00377701\n",
      "Iteration 3601, loss = 0.00377405\n",
      "Iteration 3602, loss = 0.00377109\n",
      "Iteration 3603, loss = 0.00376813\n",
      "Iteration 3604, loss = 0.00376518\n",
      "Iteration 3605, loss = 0.00376223\n",
      "Iteration 3606, loss = 0.00375928\n",
      "Iteration 3607, loss = 0.00375634\n",
      "Iteration 3608, loss = 0.00375339\n",
      "Iteration 3609, loss = 0.00375045\n",
      "Iteration 3610, loss = 0.00374752\n",
      "Iteration 3611, loss = 0.00374459\n",
      "Iteration 3612, loss = 0.00374166\n",
      "Iteration 3613, loss = 0.00373873\n",
      "Iteration 3614, loss = 0.00373580\n",
      "Iteration 3615, loss = 0.00373288\n",
      "Iteration 3616, loss = 0.00372997\n",
      "Iteration 3617, loss = 0.00372705\n",
      "Iteration 3618, loss = 0.00372414\n",
      "Iteration 3619, loss = 0.00372123\n",
      "Iteration 3620, loss = 0.00371832\n",
      "Iteration 3621, loss = 0.00371542\n",
      "Iteration 3622, loss = 0.00371252\n",
      "Iteration 3623, loss = 0.00370962\n",
      "Iteration 3624, loss = 0.00370673\n",
      "Iteration 3625, loss = 0.00370384\n",
      "Iteration 3626, loss = 0.00370095\n",
      "Iteration 3627, loss = 0.00369806\n",
      "Iteration 3628, loss = 0.00369518\n",
      "Iteration 3629, loss = 0.00369230\n",
      "Iteration 3630, loss = 0.00368942\n",
      "Iteration 3631, loss = 0.00368655\n",
      "Iteration 3632, loss = 0.00368368\n",
      "Iteration 3633, loss = 0.00368081\n",
      "Iteration 3634, loss = 0.00367795\n",
      "Iteration 3635, loss = 0.00367508\n",
      "Iteration 3636, loss = 0.00367223\n",
      "Iteration 3637, loss = 0.00366937\n",
      "Iteration 3638, loss = 0.00366652\n",
      "Iteration 3639, loss = 0.00366366\n",
      "Iteration 3640, loss = 0.00366082\n",
      "Iteration 3641, loss = 0.00365797\n",
      "Iteration 3642, loss = 0.00365513\n",
      "Iteration 3643, loss = 0.00365229\n",
      "Iteration 3644, loss = 0.00364945\n",
      "Iteration 3645, loss = 0.00364662\n",
      "Iteration 3646, loss = 0.00364379\n",
      "Iteration 3647, loss = 0.00364096\n",
      "Iteration 3648, loss = 0.00363814\n",
      "Iteration 3649, loss = 0.00363532\n",
      "Iteration 3650, loss = 0.00363250\n",
      "Iteration 3651, loss = 0.00362968\n",
      "Iteration 3652, loss = 0.00362687\n",
      "Iteration 3653, loss = 0.00362406\n",
      "Iteration 3654, loss = 0.00362125\n",
      "Iteration 3655, loss = 0.00361844\n",
      "Iteration 3656, loss = 0.00361564\n",
      "Iteration 3657, loss = 0.00361284\n",
      "Iteration 3658, loss = 0.00361005\n",
      "Iteration 3659, loss = 0.00360725\n",
      "Iteration 3660, loss = 0.00360446\n",
      "Iteration 3661, loss = 0.00360167\n",
      "Iteration 3662, loss = 0.00359889\n",
      "Iteration 3663, loss = 0.00359611\n",
      "Iteration 3664, loss = 0.00359333\n",
      "Iteration 3665, loss = 0.00359055\n",
      "Iteration 3666, loss = 0.00358778\n",
      "Iteration 3667, loss = 0.00358500\n",
      "Iteration 3668, loss = 0.00358224\n",
      "Iteration 3669, loss = 0.00357947\n",
      "Iteration 3670, loss = 0.00357671\n",
      "Iteration 3671, loss = 0.00357395\n",
      "Iteration 3672, loss = 0.00357119\n",
      "Iteration 3673, loss = 0.00356843\n",
      "Iteration 3674, loss = 0.00356568\n",
      "Iteration 3675, loss = 0.00356293\n",
      "Iteration 3676, loss = 0.00356019\n",
      "Iteration 3677, loss = 0.00355744\n",
      "Iteration 3678, loss = 0.00355470\n",
      "Iteration 3679, loss = 0.00355196\n",
      "Iteration 3680, loss = 0.00354923\n",
      "Iteration 3681, loss = 0.00354650\n",
      "Iteration 3682, loss = 0.00354377\n",
      "Iteration 3683, loss = 0.00354104\n",
      "Iteration 3684, loss = 0.00353831\n",
      "Iteration 3685, loss = 0.00353559\n",
      "Iteration 3686, loss = 0.00353287\n",
      "Iteration 3687, loss = 0.00353016\n",
      "Iteration 3688, loss = 0.00352744\n",
      "Iteration 3689, loss = 0.00352473\n",
      "Iteration 3690, loss = 0.00352202\n",
      "Iteration 3691, loss = 0.00351932\n",
      "Iteration 3692, loss = 0.00351661\n",
      "Iteration 3693, loss = 0.00351391\n",
      "Iteration 3694, loss = 0.00351122\n",
      "Iteration 3695, loss = 0.00350852\n",
      "Iteration 3696, loss = 0.00350583\n",
      "Iteration 3697, loss = 0.00350314\n",
      "Iteration 3698, loss = 0.00350045\n",
      "Iteration 3699, loss = 0.00349777\n",
      "Iteration 3700, loss = 0.00349509\n",
      "Iteration 3701, loss = 0.00349241\n",
      "Iteration 3702, loss = 0.00348973\n",
      "Iteration 3703, loss = 0.00348706\n",
      "Iteration 3704, loss = 0.00348439\n",
      "Iteration 3705, loss = 0.00348172\n",
      "Iteration 3706, loss = 0.00347906\n",
      "Iteration 3707, loss = 0.00347639\n",
      "Iteration 3708, loss = 0.00347373\n",
      "Iteration 3709, loss = 0.00347107\n",
      "Iteration 3710, loss = 0.00346842\n",
      "Iteration 3711, loss = 0.00346577\n",
      "Iteration 3712, loss = 0.00346312\n",
      "Iteration 3713, loss = 0.00346047\n",
      "Iteration 3714, loss = 0.00345783\n",
      "Iteration 3715, loss = 0.00345518\n",
      "Iteration 3716, loss = 0.00345254\n",
      "Iteration 3717, loss = 0.00344991\n",
      "Iteration 3718, loss = 0.00344727\n",
      "Iteration 3719, loss = 0.00344464\n",
      "Iteration 3720, loss = 0.00344201\n",
      "Iteration 3721, loss = 0.00343939\n",
      "Iteration 3722, loss = 0.00343676\n",
      "Iteration 3723, loss = 0.00343414\n",
      "Iteration 3724, loss = 0.00343152\n",
      "Iteration 3725, loss = 0.00342891\n",
      "Iteration 3726, loss = 0.00342630\n",
      "Iteration 3727, loss = 0.00342368\n",
      "Iteration 3728, loss = 0.00342108\n",
      "Iteration 3729, loss = 0.00341847\n",
      "Iteration 3730, loss = 0.00341587\n",
      "Iteration 3731, loss = 0.00341327\n",
      "Iteration 3732, loss = 0.00341067\n",
      "Iteration 3733, loss = 0.00340807\n",
      "Iteration 3734, loss = 0.00340548\n",
      "Iteration 3735, loss = 0.00340289\n",
      "Iteration 3736, loss = 0.00340030\n",
      "Iteration 3737, loss = 0.00339772\n",
      "Iteration 3738, loss = 0.00339514\n",
      "Iteration 3739, loss = 0.00339256\n",
      "Iteration 3740, loss = 0.00338998\n",
      "Iteration 3741, loss = 0.00338740\n",
      "Iteration 3742, loss = 0.00338483\n",
      "Iteration 3743, loss = 0.00338226\n",
      "Iteration 3744, loss = 0.00337969\n",
      "Iteration 3745, loss = 0.00337713\n",
      "Iteration 3746, loss = 0.00337457\n",
      "Iteration 3747, loss = 0.00337201\n",
      "Iteration 3748, loss = 0.00336945\n",
      "Iteration 3749, loss = 0.00336689\n",
      "Iteration 3750, loss = 0.00336434\n",
      "Iteration 3751, loss = 0.00336179\n",
      "Iteration 3752, loss = 0.00335924\n",
      "Iteration 3753, loss = 0.00335670\n",
      "Iteration 3754, loss = 0.00335416\n",
      "Iteration 3755, loss = 0.00335162\n",
      "Iteration 3756, loss = 0.00334908\n",
      "Iteration 3757, loss = 0.00334654\n",
      "Iteration 3758, loss = 0.00334401\n",
      "Iteration 3759, loss = 0.00334148\n",
      "Iteration 3760, loss = 0.00333895\n",
      "Iteration 3761, loss = 0.00333643\n",
      "Iteration 3762, loss = 0.00333391\n",
      "Iteration 3763, loss = 0.00333139\n",
      "Iteration 3764, loss = 0.00332887\n",
      "Iteration 3765, loss = 0.00332635\n",
      "Iteration 3766, loss = 0.00332384\n",
      "Iteration 3767, loss = 0.00332133\n",
      "Iteration 3768, loss = 0.00331882\n",
      "Iteration 3769, loss = 0.00331632\n",
      "Iteration 3770, loss = 0.00331382\n",
      "Iteration 3771, loss = 0.00331131\n",
      "Iteration 3772, loss = 0.00330882\n",
      "Iteration 3773, loss = 0.00330632\n",
      "Iteration 3774, loss = 0.00330383\n",
      "Iteration 3775, loss = 0.00330134\n",
      "Iteration 3776, loss = 0.00329885\n",
      "Iteration 3777, loss = 0.00329636\n",
      "Iteration 3778, loss = 0.00329388\n",
      "Iteration 3779, loss = 0.00329140\n",
      "Iteration 3780, loss = 0.00328892\n",
      "Iteration 3781, loss = 0.00328644\n",
      "Iteration 3782, loss = 0.00328397\n",
      "Iteration 3783, loss = 0.00328150\n",
      "Iteration 3784, loss = 0.00327903\n",
      "Iteration 3785, loss = 0.00327656\n",
      "Iteration 3786, loss = 0.00327410\n",
      "Iteration 3787, loss = 0.00327164\n",
      "Iteration 3788, loss = 0.00326918\n",
      "Iteration 3789, loss = 0.00326672\n",
      "Iteration 3790, loss = 0.00326426\n",
      "Iteration 3791, loss = 0.00326181\n",
      "Iteration 3792, loss = 0.00325936\n",
      "Iteration 3793, loss = 0.00325691\n",
      "Iteration 3794, loss = 0.00325447\n",
      "Iteration 3795, loss = 0.00325203\n",
      "Iteration 3796, loss = 0.00324959\n",
      "Iteration 3797, loss = 0.00324715\n",
      "Iteration 3798, loss = 0.00324471\n",
      "Iteration 3799, loss = 0.00324228\n",
      "Iteration 3800, loss = 0.00323985\n",
      "Iteration 3801, loss = 0.00323742\n",
      "Iteration 3802, loss = 0.00323499\n",
      "Iteration 3803, loss = 0.00323257\n",
      "Iteration 3804, loss = 0.00323015\n",
      "Iteration 3805, loss = 0.00322773\n",
      "Iteration 3806, loss = 0.00322531\n",
      "Iteration 3807, loss = 0.00322290\n",
      "Iteration 3808, loss = 0.00322048\n",
      "Iteration 3809, loss = 0.00321807\n",
      "Iteration 3810, loss = 0.00321567\n",
      "Iteration 3811, loss = 0.00321326\n",
      "Iteration 3812, loss = 0.00321086\n",
      "Iteration 3813, loss = 0.00320846\n",
      "Iteration 3814, loss = 0.00320606\n",
      "Iteration 3815, loss = 0.00320366\n",
      "Iteration 3816, loss = 0.00320127\n",
      "Iteration 3817, loss = 0.00319888\n",
      "Iteration 3818, loss = 0.00319649\n",
      "Iteration 3819, loss = 0.00319410\n",
      "Iteration 3820, loss = 0.00319172\n",
      "Iteration 3821, loss = 0.00318934\n",
      "Iteration 3822, loss = 0.00318696\n",
      "Iteration 3823, loss = 0.00318458\n",
      "Iteration 3824, loss = 0.00318220\n",
      "Iteration 3825, loss = 0.00317983\n",
      "Iteration 3826, loss = 0.00317746\n",
      "Iteration 3827, loss = 0.00317509\n",
      "Iteration 3828, loss = 0.00317272\n",
      "Iteration 3829, loss = 0.00317036\n",
      "Iteration 3830, loss = 0.00316800\n",
      "Iteration 3831, loss = 0.00316564\n",
      "Iteration 3832, loss = 0.00316328\n",
      "Iteration 3833, loss = 0.00316093\n",
      "Iteration 3834, loss = 0.00315857\n",
      "Iteration 3835, loss = 0.00315622\n",
      "Iteration 3836, loss = 0.00315388\n",
      "Iteration 3837, loss = 0.00315153\n",
      "Iteration 3838, loss = 0.00314919\n",
      "Iteration 3839, loss = 0.00314684\n",
      "Iteration 3840, loss = 0.00314451\n",
      "Iteration 3841, loss = 0.00314217\n",
      "Iteration 3842, loss = 0.00313983\n",
      "Iteration 3843, loss = 0.00313750\n",
      "Iteration 3844, loss = 0.00313517\n",
      "Iteration 3845, loss = 0.00313284\n",
      "Iteration 3846, loss = 0.00313052\n",
      "Iteration 3847, loss = 0.00312819\n",
      "Iteration 3848, loss = 0.00312587\n",
      "Iteration 3849, loss = 0.00312355\n",
      "Iteration 3850, loss = 0.00312124\n",
      "Iteration 3851, loss = 0.00311892\n",
      "Iteration 3852, loss = 0.00311661\n",
      "Iteration 3853, loss = 0.00311430\n",
      "Iteration 3854, loss = 0.00311199\n",
      "Iteration 3855, loss = 0.00310969\n",
      "Iteration 3856, loss = 0.00310738\n",
      "Iteration 3857, loss = 0.00310508\n",
      "Iteration 3858, loss = 0.00310278\n",
      "Iteration 3859, loss = 0.00310049\n",
      "Iteration 3860, loss = 0.00309819\n",
      "Iteration 3861, loss = 0.00309590\n",
      "Iteration 3862, loss = 0.00309361\n",
      "Iteration 3863, loss = 0.00309132\n",
      "Iteration 3864, loss = 0.00308903\n",
      "Iteration 3865, loss = 0.00308675\n",
      "Iteration 3866, loss = 0.00308447\n",
      "Iteration 3867, loss = 0.00308219\n",
      "Iteration 3868, loss = 0.00307991\n",
      "Iteration 3869, loss = 0.00307764\n",
      "Iteration 3870, loss = 0.00307536\n",
      "Iteration 3871, loss = 0.00307309\n",
      "Iteration 3872, loss = 0.00307082\n",
      "Iteration 3873, loss = 0.00306856\n",
      "Iteration 3874, loss = 0.00306629\n",
      "Iteration 3875, loss = 0.00306403\n",
      "Iteration 3876, loss = 0.00306177\n",
      "Iteration 3877, loss = 0.00305951\n",
      "Iteration 3878, loss = 0.00305726\n",
      "Iteration 3879, loss = 0.00305500\n",
      "Iteration 3880, loss = 0.00305275\n",
      "Iteration 3881, loss = 0.00305050\n",
      "Iteration 3882, loss = 0.00304825\n",
      "Iteration 3883, loss = 0.00304601\n",
      "Iteration 3884, loss = 0.00304377\n",
      "Iteration 3885, loss = 0.00304153\n",
      "Iteration 3886, loss = 0.00303929\n",
      "Iteration 3887, loss = 0.00303705\n",
      "Iteration 3888, loss = 0.00303482\n",
      "Iteration 3889, loss = 0.00303258\n",
      "Iteration 3890, loss = 0.00303035\n",
      "Iteration 3891, loss = 0.00302813\n",
      "Iteration 3892, loss = 0.00302590\n",
      "Iteration 3893, loss = 0.00302368\n",
      "Iteration 3894, loss = 0.00302145\n",
      "Iteration 3895, loss = 0.00301923\n",
      "Iteration 3896, loss = 0.00301702\n",
      "Iteration 3897, loss = 0.00301480\n",
      "Iteration 3898, loss = 0.00301259\n",
      "Iteration 3899, loss = 0.00301038\n",
      "Iteration 3900, loss = 0.00300817\n",
      "Iteration 3901, loss = 0.00300596\n",
      "Iteration 3902, loss = 0.00300376\n",
      "Iteration 3903, loss = 0.00300155\n",
      "Iteration 3904, loss = 0.00299935\n",
      "Iteration 3905, loss = 0.00299715\n",
      "Iteration 3906, loss = 0.00299496\n",
      "Iteration 3907, loss = 0.00299276\n",
      "Iteration 3908, loss = 0.00299057\n",
      "Iteration 3909, loss = 0.00298838\n",
      "Iteration 3910, loss = 0.00298619\n",
      "Iteration 3911, loss = 0.00298401\n",
      "Iteration 3912, loss = 0.00298182\n",
      "Iteration 3913, loss = 0.00297964\n",
      "Iteration 3914, loss = 0.00297746\n",
      "Iteration 3915, loss = 0.00297528\n",
      "Iteration 3916, loss = 0.00297311\n",
      "Iteration 3917, loss = 0.00297093\n",
      "Iteration 3918, loss = 0.00296876\n",
      "Iteration 3919, loss = 0.00296659\n",
      "Iteration 3920, loss = 0.00296442\n",
      "Iteration 3921, loss = 0.00296226\n",
      "Iteration 3922, loss = 0.00296009\n",
      "Iteration 3923, loss = 0.00295793\n",
      "Iteration 3924, loss = 0.00295577\n",
      "Iteration 3925, loss = 0.00295361\n",
      "Iteration 3926, loss = 0.00295146\n",
      "Iteration 3927, loss = 0.00294930\n",
      "Iteration 3928, loss = 0.00294715\n",
      "Iteration 3929, loss = 0.00294500\n",
      "Iteration 3930, loss = 0.00294286\n",
      "Iteration 3931, loss = 0.00294071\n",
      "Iteration 3932, loss = 0.00293857\n",
      "Iteration 3933, loss = 0.00293643\n",
      "Iteration 3934, loss = 0.00293429\n",
      "Iteration 3935, loss = 0.00293215\n",
      "Iteration 3936, loss = 0.00293001\n",
      "Iteration 3937, loss = 0.00292788\n",
      "Iteration 3938, loss = 0.00292575\n",
      "Iteration 3939, loss = 0.00292362\n",
      "Iteration 3940, loss = 0.00292149\n",
      "Iteration 3941, loss = 0.00291937\n",
      "Iteration 3942, loss = 0.00291724\n",
      "Iteration 3943, loss = 0.00291512\n",
      "Iteration 3944, loss = 0.00291300\n",
      "Iteration 3945, loss = 0.00291089\n",
      "Iteration 3946, loss = 0.00290877\n",
      "Iteration 3947, loss = 0.00290666\n",
      "Iteration 3948, loss = 0.00290455\n",
      "Iteration 3949, loss = 0.00290244\n",
      "Iteration 3950, loss = 0.00290033\n",
      "Iteration 3951, loss = 0.00289822\n",
      "Iteration 3952, loss = 0.00289612\n",
      "Iteration 3953, loss = 0.00289402\n",
      "Iteration 3954, loss = 0.00289192\n",
      "Iteration 3955, loss = 0.00288982\n",
      "Iteration 3956, loss = 0.00288773\n",
      "Iteration 3957, loss = 0.00288563\n",
      "Iteration 3958, loss = 0.00288354\n",
      "Iteration 3959, loss = 0.00288145\n",
      "Iteration 3960, loss = 0.00287936\n",
      "Iteration 3961, loss = 0.00287728\n",
      "Iteration 3962, loss = 0.00287519\n",
      "Iteration 3963, loss = 0.00287311\n",
      "Iteration 3964, loss = 0.00287103\n",
      "Iteration 3965, loss = 0.00286895\n",
      "Iteration 3966, loss = 0.00286688\n",
      "Iteration 3967, loss = 0.00286480\n",
      "Iteration 3968, loss = 0.00286273\n",
      "Iteration 3969, loss = 0.00286066\n",
      "Iteration 3970, loss = 0.00285859\n",
      "Iteration 3971, loss = 0.00285653\n",
      "Iteration 3972, loss = 0.00285446\n",
      "Iteration 3973, loss = 0.00285240\n",
      "Iteration 3974, loss = 0.00285034\n",
      "Iteration 3975, loss = 0.00284828\n",
      "Iteration 3976, loss = 0.00284622\n",
      "Iteration 3977, loss = 0.00284417\n",
      "Iteration 3978, loss = 0.00284212\n",
      "Iteration 3979, loss = 0.00284006\n",
      "Iteration 3980, loss = 0.00283802\n",
      "Iteration 3981, loss = 0.00283597\n",
      "Iteration 3982, loss = 0.00283392\n",
      "Iteration 3983, loss = 0.00283188\n",
      "Iteration 3984, loss = 0.00282984\n",
      "Iteration 3985, loss = 0.00282780\n",
      "Iteration 3986, loss = 0.00282576\n",
      "Iteration 3987, loss = 0.00282373\n",
      "Iteration 3988, loss = 0.00282169\n",
      "Iteration 3989, loss = 0.00281966\n",
      "Iteration 3990, loss = 0.00281763\n",
      "Iteration 3991, loss = 0.00281560\n",
      "Iteration 3992, loss = 0.00281357\n",
      "Iteration 3993, loss = 0.00281155\n",
      "Iteration 3994, loss = 0.00280953\n",
      "Iteration 3995, loss = 0.00280751\n",
      "Iteration 3996, loss = 0.00280549\n",
      "Iteration 3997, loss = 0.00280347\n",
      "Iteration 3998, loss = 0.00280146\n",
      "Iteration 3999, loss = 0.00279944\n",
      "Iteration 4000, loss = 0.00279743\n",
      "Iteration 4001, loss = 0.00279542\n",
      "Iteration 4002, loss = 0.00279342\n",
      "Iteration 4003, loss = 0.00279141\n",
      "Iteration 4004, loss = 0.00278941\n",
      "Iteration 4005, loss = 0.00278740\n",
      "Iteration 4006, loss = 0.00278540\n",
      "Iteration 4007, loss = 0.00278341\n",
      "Iteration 4008, loss = 0.00278141\n",
      "Iteration 4009, loss = 0.00277941\n",
      "Iteration 4010, loss = 0.00277742\n",
      "Iteration 4011, loss = 0.00277543\n",
      "Iteration 4012, loss = 0.00277344\n",
      "Iteration 4013, loss = 0.00277146\n",
      "Iteration 4014, loss = 0.00276947\n",
      "Iteration 4015, loss = 0.00276749\n",
      "Iteration 4016, loss = 0.00276551\n",
      "Iteration 4017, loss = 0.00276353\n",
      "Iteration 4018, loss = 0.00276155\n",
      "Iteration 4019, loss = 0.00275957\n",
      "Iteration 4020, loss = 0.00275760\n",
      "Iteration 4021, loss = 0.00275562\n",
      "Iteration 4022, loss = 0.00275365\n",
      "Iteration 4023, loss = 0.00275169\n",
      "Iteration 4024, loss = 0.00274972\n",
      "Iteration 4025, loss = 0.00274775\n",
      "Iteration 4026, loss = 0.00274579\n",
      "Iteration 4027, loss = 0.00274383\n",
      "Iteration 4028, loss = 0.00274187\n",
      "Iteration 4029, loss = 0.00273991\n",
      "Iteration 4030, loss = 0.00273796\n",
      "Iteration 4031, loss = 0.00273600\n",
      "Iteration 4032, loss = 0.00273405\n",
      "Iteration 4033, loss = 0.00273210\n",
      "Iteration 4034, loss = 0.00273015\n",
      "Iteration 4035, loss = 0.00272820\n",
      "Iteration 4036, loss = 0.00272626\n",
      "Iteration 4037, loss = 0.00272431\n",
      "Iteration 4038, loss = 0.00272237\n",
      "Iteration 4039, loss = 0.00272043\n",
      "Iteration 4040, loss = 0.00271849\n",
      "Iteration 4041, loss = 0.00271656\n",
      "Iteration 4042, loss = 0.00271462\n",
      "Iteration 4043, loss = 0.00271269\n",
      "Iteration 4044, loss = 0.00271076\n",
      "Iteration 4045, loss = 0.00270883\n",
      "Iteration 4046, loss = 0.00270690\n",
      "Iteration 4047, loss = 0.00270498\n",
      "Iteration 4048, loss = 0.00270305\n",
      "Iteration 4049, loss = 0.00270113\n",
      "Iteration 4050, loss = 0.00269921\n",
      "Iteration 4051, loss = 0.00269729\n",
      "Iteration 4052, loss = 0.00269538\n",
      "Iteration 4053, loss = 0.00269346\n",
      "Iteration 4054, loss = 0.00269155\n",
      "Iteration 4055, loss = 0.00268964\n",
      "Iteration 4056, loss = 0.00268773\n",
      "Iteration 4057, loss = 0.00268582\n",
      "Iteration 4058, loss = 0.00268391\n",
      "Iteration 4059, loss = 0.00268201\n",
      "Iteration 4060, loss = 0.00268011\n",
      "Iteration 4061, loss = 0.00267821\n",
      "Iteration 4062, loss = 0.00267631\n",
      "Iteration 4063, loss = 0.00267441\n",
      "Iteration 4064, loss = 0.00267251\n",
      "Iteration 4065, loss = 0.00267062\n",
      "Iteration 4066, loss = 0.00266873\n",
      "Iteration 4067, loss = 0.00266684\n",
      "Iteration 4068, loss = 0.00266495\n",
      "Iteration 4069, loss = 0.00266306\n",
      "Iteration 4070, loss = 0.00266118\n",
      "Iteration 4071, loss = 0.00265929\n",
      "Iteration 4072, loss = 0.00265741\n",
      "Iteration 4073, loss = 0.00265553\n",
      "Iteration 4074, loss = 0.00265365\n",
      "Iteration 4075, loss = 0.00265178\n",
      "Iteration 4076, loss = 0.00264990\n",
      "Iteration 4077, loss = 0.00264803\n",
      "Iteration 4078, loss = 0.00264616\n",
      "Iteration 4079, loss = 0.00264429\n",
      "Iteration 4080, loss = 0.00264242\n",
      "Iteration 4081, loss = 0.00264056\n",
      "Iteration 4082, loss = 0.00263869\n",
      "Iteration 4083, loss = 0.00263683\n",
      "Iteration 4084, loss = 0.00263497\n",
      "Iteration 4085, loss = 0.00263311\n",
      "Iteration 4086, loss = 0.00263125\n",
      "Iteration 4087, loss = 0.00262939\n",
      "Iteration 4088, loss = 0.00262754\n",
      "Iteration 4089, loss = 0.00262569\n",
      "Iteration 4090, loss = 0.00262384\n",
      "Iteration 4091, loss = 0.00262199\n",
      "Iteration 4092, loss = 0.00262014\n",
      "Iteration 4093, loss = 0.00261829\n",
      "Iteration 4094, loss = 0.00261645\n",
      "Iteration 4095, loss = 0.00261461\n",
      "Iteration 4096, loss = 0.00261277\n",
      "Iteration 4097, loss = 0.00261093\n",
      "Iteration 4098, loss = 0.00260909\n",
      "Iteration 4099, loss = 0.00260726\n",
      "Iteration 4100, loss = 0.00260542\n",
      "Iteration 4101, loss = 0.00260359\n",
      "Iteration 4102, loss = 0.00260176\n",
      "Iteration 4103, loss = 0.00259993\n",
      "Iteration 4104, loss = 0.00259810\n",
      "Iteration 4105, loss = 0.00259628\n",
      "Iteration 4106, loss = 0.00259445\n",
      "Iteration 4107, loss = 0.00259263\n",
      "Iteration 4108, loss = 0.00259081\n",
      "Iteration 4109, loss = 0.00258899\n",
      "Iteration 4110, loss = 0.00258717\n",
      "Iteration 4111, loss = 0.00258536\n",
      "Iteration 4112, loss = 0.00258355\n",
      "Iteration 4113, loss = 0.00258173\n",
      "Iteration 4114, loss = 0.00257992\n",
      "Iteration 4115, loss = 0.00257811\n",
      "Iteration 4116, loss = 0.00257631\n",
      "Iteration 4117, loss = 0.00257450\n",
      "Iteration 4118, loss = 0.00257270\n",
      "Iteration 4119, loss = 0.00257089\n",
      "Iteration 4120, loss = 0.00256909\n",
      "Iteration 4121, loss = 0.00256729\n",
      "Iteration 4122, loss = 0.00256550\n",
      "Iteration 4123, loss = 0.00256370\n",
      "Iteration 4124, loss = 0.00256191\n",
      "Iteration 4125, loss = 0.00256012\n",
      "Iteration 4126, loss = 0.00255832\n",
      "Iteration 4127, loss = 0.00255653\n",
      "Iteration 4128, loss = 0.00255475\n",
      "Iteration 4129, loss = 0.00255296\n",
      "Iteration 4130, loss = 0.00255118\n",
      "Iteration 4131, loss = 0.00254939\n",
      "Iteration 4132, loss = 0.00254761\n",
      "Iteration 4133, loss = 0.00254583\n",
      "Iteration 4134, loss = 0.00254406\n",
      "Iteration 4135, loss = 0.00254228\n",
      "Iteration 4136, loss = 0.00254051\n",
      "Iteration 4137, loss = 0.00253873\n",
      "Iteration 4138, loss = 0.00253696\n",
      "Iteration 4139, loss = 0.00253519\n",
      "Iteration 4140, loss = 0.00253342\n",
      "Iteration 4141, loss = 0.00253166\n",
      "Iteration 4142, loss = 0.00252989\n",
      "Iteration 4143, loss = 0.00252813\n",
      "Iteration 4144, loss = 0.00252637\n",
      "Iteration 4145, loss = 0.00252461\n",
      "Iteration 4146, loss = 0.00252285\n",
      "Iteration 4147, loss = 0.00252109\n",
      "Iteration 4148, loss = 0.00251934\n",
      "Iteration 4149, loss = 0.00251758\n",
      "Iteration 4150, loss = 0.00251583\n",
      "Iteration 4151, loss = 0.00251408\n",
      "Iteration 4152, loss = 0.00251233\n",
      "Iteration 4153, loss = 0.00251058\n",
      "Iteration 4154, loss = 0.00250884\n",
      "Iteration 4155, loss = 0.00250709\n",
      "Iteration 4156, loss = 0.00250535\n",
      "Iteration 4157, loss = 0.00250361\n",
      "Iteration 4158, loss = 0.00250187\n",
      "Iteration 4159, loss = 0.00250013\n",
      "Iteration 4160, loss = 0.00249840\n",
      "Iteration 4161, loss = 0.00249666\n",
      "Iteration 4162, loss = 0.00249493\n",
      "Iteration 4163, loss = 0.00249320\n",
      "Iteration 4164, loss = 0.00249147\n",
      "Iteration 4165, loss = 0.00248974\n",
      "Iteration 4166, loss = 0.00248801\n",
      "Iteration 4167, loss = 0.00248629\n",
      "Iteration 4168, loss = 0.00248456\n",
      "Iteration 4169, loss = 0.00248284\n",
      "Iteration 4170, loss = 0.00248112\n",
      "Iteration 4171, loss = 0.00247940\n",
      "Iteration 4172, loss = 0.00247768\n",
      "Iteration 4173, loss = 0.00247597\n",
      "Iteration 4174, loss = 0.00247425\n",
      "Iteration 4175, loss = 0.00247254\n",
      "Iteration 4176, loss = 0.00247083\n",
      "Iteration 4177, loss = 0.00246912\n",
      "Iteration 4178, loss = 0.00246741\n",
      "Iteration 4179, loss = 0.00246571\n",
      "Iteration 4180, loss = 0.00246400\n",
      "Iteration 4181, loss = 0.00246230\n",
      "Iteration 4182, loss = 0.00246059\n",
      "Iteration 4183, loss = 0.00245889\n",
      "Iteration 4184, loss = 0.00245720\n",
      "Iteration 4185, loss = 0.00245550\n",
      "Iteration 4186, loss = 0.00245380\n",
      "Iteration 4187, loss = 0.00245211\n",
      "Iteration 4188, loss = 0.00245042\n",
      "Iteration 4189, loss = 0.00244872\n",
      "Iteration 4190, loss = 0.00244703\n",
      "Iteration 4191, loss = 0.00244535\n",
      "Iteration 4192, loss = 0.00244366\n",
      "Iteration 4193, loss = 0.00244197\n",
      "Iteration 4194, loss = 0.00244029\n",
      "Iteration 4195, loss = 0.00243861\n",
      "Iteration 4196, loss = 0.00243693\n",
      "Iteration 4197, loss = 0.00243525\n",
      "Iteration 4198, loss = 0.00243357\n",
      "Iteration 4199, loss = 0.00243190\n",
      "Iteration 4200, loss = 0.00243022\n",
      "Iteration 4201, loss = 0.00242855\n",
      "Iteration 4202, loss = 0.00242688\n",
      "Iteration 4203, loss = 0.00242521\n",
      "Iteration 4204, loss = 0.00242354\n",
      "Iteration 4205, loss = 0.00242187\n",
      "Iteration 4206, loss = 0.00242021\n",
      "Iteration 4207, loss = 0.00241854\n",
      "Iteration 4208, loss = 0.00241688\n",
      "Iteration 4209, loss = 0.00241522\n",
      "Iteration 4210, loss = 0.00241356\n",
      "Iteration 4211, loss = 0.00241190\n",
      "Iteration 4212, loss = 0.00241024\n",
      "Iteration 4213, loss = 0.00240859\n",
      "Iteration 4214, loss = 0.00240694\n",
      "Iteration 4215, loss = 0.00240528\n",
      "Iteration 4216, loss = 0.00240363\n",
      "Iteration 4217, loss = 0.00240198\n",
      "Iteration 4218, loss = 0.00240034\n",
      "Iteration 4219, loss = 0.00239869\n",
      "Iteration 4220, loss = 0.00239705\n",
      "Iteration 4221, loss = 0.00239540\n",
      "Iteration 4222, loss = 0.00239376\n",
      "Iteration 4223, loss = 0.00239212\n",
      "Iteration 4224, loss = 0.00239048\n",
      "Iteration 4225, loss = 0.00238885\n",
      "Iteration 4226, loss = 0.00238721\n",
      "Iteration 4227, loss = 0.00238558\n",
      "Iteration 4228, loss = 0.00238394\n",
      "Iteration 4229, loss = 0.00238231\n",
      "Iteration 4230, loss = 0.00238068\n",
      "Iteration 4231, loss = 0.00237905\n",
      "Iteration 4232, loss = 0.00237743\n",
      "Iteration 4233, loss = 0.00237580\n",
      "Iteration 4234, loss = 0.00237418\n",
      "Iteration 4235, loss = 0.00237255\n",
      "Iteration 4236, loss = 0.00237093\n",
      "Iteration 4237, loss = 0.00236931\n",
      "Iteration 4238, loss = 0.00236770\n",
      "Iteration 4239, loss = 0.00236608\n",
      "Iteration 4240, loss = 0.00236446\n",
      "Iteration 4241, loss = 0.00236285\n",
      "Iteration 4242, loss = 0.00236124\n",
      "Iteration 4243, loss = 0.00235963\n",
      "Iteration 4244, loss = 0.00235802\n",
      "Iteration 4245, loss = 0.00235641\n",
      "Iteration 4246, loss = 0.00235480\n",
      "Iteration 4247, loss = 0.00235320\n",
      "Iteration 4248, loss = 0.00235159\n",
      "Iteration 4249, loss = 0.00234999\n",
      "Iteration 4250, loss = 0.00234839\n",
      "Iteration 4251, loss = 0.00234679\n",
      "Iteration 4252, loss = 0.00234519\n",
      "Iteration 4253, loss = 0.00234360\n",
      "Iteration 4254, loss = 0.00234200\n",
      "Iteration 4255, loss = 0.00234041\n",
      "Iteration 4256, loss = 0.00233881\n",
      "Iteration 4257, loss = 0.00233722\n",
      "Iteration 4258, loss = 0.00233563\n",
      "Iteration 4259, loss = 0.00233405\n",
      "Iteration 4260, loss = 0.00233246\n",
      "Iteration 4261, loss = 0.00233087\n",
      "Iteration 4262, loss = 0.00232929\n",
      "Iteration 4263, loss = 0.00232771\n",
      "Iteration 4264, loss = 0.00232613\n",
      "Iteration 4265, loss = 0.00232455\n",
      "Iteration 4266, loss = 0.00232297\n",
      "Iteration 4267, loss = 0.00232139\n",
      "Iteration 4268, loss = 0.00231982\n",
      "Iteration 4269, loss = 0.00231824\n",
      "Iteration 4270, loss = 0.00231667\n",
      "Iteration 4271, loss = 0.00231510\n",
      "Iteration 4272, loss = 0.00231353\n",
      "Iteration 4273, loss = 0.00231196\n",
      "Iteration 4274, loss = 0.00231040\n",
      "Iteration 4275, loss = 0.00230883\n",
      "Iteration 4276, loss = 0.00230727\n",
      "Iteration 4277, loss = 0.00230570\n",
      "Iteration 4278, loss = 0.00230414\n",
      "Iteration 4279, loss = 0.00230258\n",
      "Iteration 4280, loss = 0.00230102\n",
      "Iteration 4281, loss = 0.00229947\n",
      "Iteration 4282, loss = 0.00229791\n",
      "Iteration 4283, loss = 0.00229636\n",
      "Iteration 4284, loss = 0.00229480\n",
      "Iteration 4285, loss = 0.00229325\n",
      "Iteration 4286, loss = 0.00229170\n",
      "Iteration 4287, loss = 0.00229015\n",
      "Iteration 4288, loss = 0.00228861\n",
      "Iteration 4289, loss = 0.00228706\n",
      "Iteration 4290, loss = 0.00228552\n",
      "Iteration 4291, loss = 0.00228397\n",
      "Iteration 4292, loss = 0.00228243\n",
      "Iteration 4293, loss = 0.00228089\n",
      "Iteration 4294, loss = 0.00227935\n",
      "Iteration 4295, loss = 0.00227781\n",
      "Iteration 4296, loss = 0.00227628\n",
      "Iteration 4297, loss = 0.00227474\n",
      "Iteration 4298, loss = 0.00227321\n",
      "Iteration 4299, loss = 0.00227168\n",
      "Iteration 4300, loss = 0.00227014\n",
      "Iteration 4301, loss = 0.00226861\n",
      "Iteration 4302, loss = 0.00226709\n",
      "Iteration 4303, loss = 0.00226556\n",
      "Iteration 4304, loss = 0.00226403\n",
      "Iteration 4305, loss = 0.00226251\n",
      "Iteration 4306, loss = 0.00226099\n",
      "Iteration 4307, loss = 0.00225947\n",
      "Iteration 4308, loss = 0.00225795\n",
      "Iteration 4309, loss = 0.00225643\n",
      "Iteration 4310, loss = 0.00225491\n",
      "Iteration 4311, loss = 0.00225339\n",
      "Iteration 4312, loss = 0.00225188\n",
      "Iteration 4313, loss = 0.00225036\n",
      "Iteration 4314, loss = 0.00224885\n",
      "Iteration 4315, loss = 0.00224734\n",
      "Iteration 4316, loss = 0.00224583\n",
      "Iteration 4317, loss = 0.00224432\n",
      "Iteration 4318, loss = 0.00224282\n",
      "Iteration 4319, loss = 0.00224131\n",
      "Iteration 4320, loss = 0.00223981\n",
      "Iteration 4321, loss = 0.00223831\n",
      "Iteration 4322, loss = 0.00223680\n",
      "Iteration 4323, loss = 0.00223530\n",
      "Iteration 4324, loss = 0.00223381\n",
      "Iteration 4325, loss = 0.00223231\n",
      "Iteration 4326, loss = 0.00223081\n",
      "Iteration 4327, loss = 0.00222932\n",
      "Iteration 4328, loss = 0.00222782\n",
      "Iteration 4329, loss = 0.00222633\n",
      "Iteration 4330, loss = 0.00222484\n",
      "Iteration 4331, loss = 0.00222335\n",
      "Iteration 4332, loss = 0.00222186\n",
      "Iteration 4333, loss = 0.00222038\n",
      "Iteration 4334, loss = 0.00221889\n",
      "Iteration 4335, loss = 0.00221741\n",
      "Iteration 4336, loss = 0.00221592\n",
      "Iteration 4337, loss = 0.00221444\n",
      "Iteration 4338, loss = 0.00221296\n",
      "Iteration 4339, loss = 0.00221148\n",
      "Iteration 4340, loss = 0.00221001\n",
      "Iteration 4341, loss = 0.00220853\n",
      "Iteration 4342, loss = 0.00220706\n",
      "Iteration 4343, loss = 0.00220558\n",
      "Iteration 4344, loss = 0.00220411\n",
      "Iteration 4345, loss = 0.00220264\n",
      "Iteration 4346, loss = 0.00220117\n",
      "Iteration 4347, loss = 0.00219970\n",
      "Iteration 4348, loss = 0.00219823\n",
      "Iteration 4349, loss = 0.00219677\n",
      "Iteration 4350, loss = 0.00219530\n",
      "Iteration 4351, loss = 0.00219384\n",
      "Iteration 4352, loss = 0.00219238\n",
      "Iteration 4353, loss = 0.00219092\n",
      "Iteration 4354, loss = 0.00218946\n",
      "Iteration 4355, loss = 0.00218800\n",
      "Iteration 4356, loss = 0.00218654\n",
      "Iteration 4357, loss = 0.00218509\n",
      "Iteration 4358, loss = 0.00218363\n",
      "Iteration 4359, loss = 0.00218218\n",
      "Iteration 4360, loss = 0.00218073\n",
      "Iteration 4361, loss = 0.00217928\n",
      "Iteration 4362, loss = 0.00217783\n",
      "Iteration 4363, loss = 0.00217638\n",
      "Iteration 4364, loss = 0.00217493\n",
      "Iteration 4365, loss = 0.00217349\n",
      "Iteration 4366, loss = 0.00217204\n",
      "Iteration 4367, loss = 0.00217060\n",
      "Iteration 4368, loss = 0.00216916\n",
      "Iteration 4369, loss = 0.00216772\n",
      "Iteration 4370, loss = 0.00216628\n",
      "Iteration 4371, loss = 0.00216484\n",
      "Iteration 4372, loss = 0.00216340\n",
      "Iteration 4373, loss = 0.00216197\n",
      "Iteration 4374, loss = 0.00216054\n",
      "Iteration 4375, loss = 0.00215910\n",
      "Iteration 4376, loss = 0.00215767\n",
      "Iteration 4377, loss = 0.00215624\n",
      "Iteration 4378, loss = 0.00215481\n",
      "Iteration 4379, loss = 0.00215338\n",
      "Iteration 4380, loss = 0.00215196\n",
      "Iteration 4381, loss = 0.00215053\n",
      "Iteration 4382, loss = 0.00214911\n",
      "Iteration 4383, loss = 0.00214769\n",
      "Iteration 4384, loss = 0.00214626\n",
      "Iteration 4385, loss = 0.00214484\n",
      "Iteration 4386, loss = 0.00214343\n",
      "Iteration 4387, loss = 0.00214201\n",
      "Iteration 4388, loss = 0.00214059\n",
      "Iteration 4389, loss = 0.00213918\n",
      "Iteration 4390, loss = 0.00213776\n",
      "Iteration 4391, loss = 0.00213635\n",
      "Iteration 4392, loss = 0.00213494\n",
      "Iteration 4393, loss = 0.00213353\n",
      "Iteration 4394, loss = 0.00213212\n",
      "Iteration 4395, loss = 0.00213071\n",
      "Iteration 4396, loss = 0.00212930\n",
      "Iteration 4397, loss = 0.00212790\n",
      "Iteration 4398, loss = 0.00212649\n",
      "Iteration 4399, loss = 0.00212509\n",
      "Iteration 4400, loss = 0.00212369\n",
      "Iteration 4401, loss = 0.00212229\n",
      "Iteration 4402, loss = 0.00212089\n",
      "Iteration 4403, loss = 0.00211949\n",
      "Iteration 4404, loss = 0.00211810\n",
      "Iteration 4405, loss = 0.00211670\n",
      "Iteration 4406, loss = 0.00211531\n",
      "Iteration 4407, loss = 0.00211391\n",
      "Iteration 4408, loss = 0.00211252\n",
      "Iteration 4409, loss = 0.00211113\n",
      "Iteration 4410, loss = 0.00210974\n",
      "Iteration 4411, loss = 0.00210835\n",
      "Iteration 4412, loss = 0.00210697\n",
      "Iteration 4413, loss = 0.00210558\n",
      "Iteration 4414, loss = 0.00210420\n",
      "Iteration 4415, loss = 0.00210281\n",
      "Iteration 4416, loss = 0.00210143\n",
      "Iteration 4417, loss = 0.00210005\n",
      "Iteration 4418, loss = 0.00209867\n",
      "Iteration 4419, loss = 0.00209729\n",
      "Iteration 4420, loss = 0.00209591\n",
      "Iteration 4421, loss = 0.00209454\n",
      "Iteration 4422, loss = 0.00209316\n",
      "Iteration 4423, loss = 0.00209179\n",
      "Iteration 4424, loss = 0.00209042\n",
      "Iteration 4425, loss = 0.00208904\n",
      "Iteration 4426, loss = 0.00208767\n",
      "Iteration 4427, loss = 0.00208631\n",
      "Iteration 4428, loss = 0.00208494\n",
      "Iteration 4429, loss = 0.00208357\n",
      "Iteration 4430, loss = 0.00208221\n",
      "Iteration 4431, loss = 0.00208084\n",
      "Iteration 4432, loss = 0.00207948\n",
      "Iteration 4433, loss = 0.00207812\n",
      "Iteration 4434, loss = 0.00207676\n",
      "Iteration 4435, loss = 0.00207540\n",
      "Iteration 4436, loss = 0.00207404\n",
      "Iteration 4437, loss = 0.00207268\n",
      "Iteration 4438, loss = 0.00207133\n",
      "Iteration 4439, loss = 0.00206997\n",
      "Iteration 4440, loss = 0.00206862\n",
      "Iteration 4441, loss = 0.00206726\n",
      "Iteration 4442, loss = 0.00206591\n",
      "Iteration 4443, loss = 0.00206456\n",
      "Iteration 4444, loss = 0.00206321\n",
      "Iteration 4445, loss = 0.00206187\n",
      "Iteration 4446, loss = 0.00206052\n",
      "Iteration 4447, loss = 0.00205918\n",
      "Iteration 4448, loss = 0.00205783\n",
      "Iteration 4449, loss = 0.00205649\n",
      "Iteration 4450, loss = 0.00205515\n",
      "Iteration 4451, loss = 0.00205381\n",
      "Iteration 4452, loss = 0.00205247\n",
      "Iteration 4453, loss = 0.00205113\n",
      "Iteration 4454, loss = 0.00204979\n",
      "Iteration 4455, loss = 0.00204845\n",
      "Iteration 4456, loss = 0.00204712\n",
      "Iteration 4457, loss = 0.00204579\n",
      "Iteration 4458, loss = 0.00204445\n",
      "Iteration 4459, loss = 0.00204312\n",
      "Iteration 4460, loss = 0.00204179\n",
      "Iteration 4461, loss = 0.00204046\n",
      "Iteration 4462, loss = 0.00203913\n",
      "Iteration 4463, loss = 0.00203781\n",
      "Iteration 4464, loss = 0.00203648\n",
      "Iteration 4465, loss = 0.00203516\n",
      "Iteration 4466, loss = 0.00203383\n",
      "Iteration 4467, loss = 0.00203251\n",
      "Iteration 4468, loss = 0.00203119\n",
      "Iteration 4469, loss = 0.00202987\n",
      "Iteration 4470, loss = 0.00202855\n",
      "Iteration 4471, loss = 0.00202723\n",
      "Iteration 4472, loss = 0.00202592\n",
      "Iteration 4473, loss = 0.00202460\n",
      "Iteration 4474, loss = 0.00202329\n",
      "Iteration 4475, loss = 0.00202198\n",
      "Iteration 4476, loss = 0.00202066\n",
      "Iteration 4477, loss = 0.00201935\n",
      "Iteration 4478, loss = 0.00201804\n",
      "Iteration 4479, loss = 0.00201673\n",
      "Iteration 4480, loss = 0.00201543\n",
      "Iteration 4481, loss = 0.00201412\n",
      "Iteration 4482, loss = 0.00201282\n",
      "Iteration 4483, loss = 0.00201151\n",
      "Iteration 4484, loss = 0.00201021\n",
      "Iteration 4485, loss = 0.00200891\n",
      "Iteration 4486, loss = 0.00200761\n",
      "Iteration 4487, loss = 0.00200631\n",
      "Iteration 4488, loss = 0.00200501\n",
      "Iteration 4489, loss = 0.00200371\n",
      "Iteration 4490, loss = 0.00200242\n",
      "Iteration 4491, loss = 0.00200112\n",
      "Iteration 4492, loss = 0.00199983\n",
      "Iteration 4493, loss = 0.00199853\n",
      "Iteration 4494, loss = 0.00199724\n",
      "Iteration 4495, loss = 0.00199595\n",
      "Iteration 4496, loss = 0.00199466\n",
      "Iteration 4497, loss = 0.00199337\n",
      "Iteration 4498, loss = 0.00199209\n",
      "Iteration 4499, loss = 0.00199080\n",
      "Iteration 4500, loss = 0.00198952\n",
      "Iteration 4501, loss = 0.00198823\n",
      "Iteration 4502, loss = 0.00198695\n",
      "Iteration 4503, loss = 0.00198567\n",
      "Iteration 4504, loss = 0.00198439\n",
      "Iteration 4505, loss = 0.00198311\n",
      "Iteration 4506, loss = 0.00198183\n",
      "Iteration 4507, loss = 0.00198055\n",
      "Iteration 4508, loss = 0.00197927\n",
      "Iteration 4509, loss = 0.00197800\n",
      "Iteration 4510, loss = 0.00197673\n",
      "Iteration 4511, loss = 0.00197545\n",
      "Iteration 4512, loss = 0.00197418\n",
      "Iteration 4513, loss = 0.00197291\n",
      "Iteration 4514, loss = 0.00197164\n",
      "Iteration 4515, loss = 0.00197037\n",
      "Iteration 4516, loss = 0.00196910\n",
      "Iteration 4517, loss = 0.00196784\n",
      "Iteration 4518, loss = 0.00196657\n",
      "Iteration 4519, loss = 0.00196531\n",
      "Iteration 4520, loss = 0.00196405\n",
      "Iteration 4521, loss = 0.00196278\n",
      "Iteration 4522, loss = 0.00196152\n",
      "Iteration 4523, loss = 0.00196026\n",
      "Iteration 4524, loss = 0.00195900\n",
      "Iteration 4525, loss = 0.00195775\n",
      "Iteration 4526, loss = 0.00195649\n",
      "Iteration 4527, loss = 0.00195523\n",
      "Iteration 4528, loss = 0.00195398\n",
      "Iteration 4529, loss = 0.00195273\n",
      "Iteration 4530, loss = 0.00195147\n",
      "Iteration 4531, loss = 0.00195022\n",
      "Iteration 4532, loss = 0.00194897\n",
      "Iteration 4533, loss = 0.00194772\n",
      "Iteration 4534, loss = 0.00194647\n",
      "Iteration 4535, loss = 0.00194523\n",
      "Iteration 4536, loss = 0.00194398\n",
      "Iteration 4537, loss = 0.00194274\n",
      "Iteration 4538, loss = 0.00194149\n",
      "Iteration 4539, loss = 0.00194025\n",
      "Iteration 4540, loss = 0.00193901\n",
      "Iteration 4541, loss = 0.00193777\n",
      "Iteration 4542, loss = 0.00193653\n",
      "Iteration 4543, loss = 0.00193529\n",
      "Iteration 4544, loss = 0.00193405\n",
      "Iteration 4545, loss = 0.00193282\n",
      "Iteration 4546, loss = 0.00193158\n",
      "Iteration 4547, loss = 0.00193035\n",
      "Iteration 4548, loss = 0.00192911\n",
      "Iteration 4549, loss = 0.00192788\n",
      "Iteration 4550, loss = 0.00192665\n",
      "Iteration 4551, loss = 0.00192542\n",
      "Iteration 4552, loss = 0.00192419\n",
      "Iteration 4553, loss = 0.00192296\n",
      "Iteration 4554, loss = 0.00192174\n",
      "Iteration 4555, loss = 0.00192051\n",
      "Iteration 4556, loss = 0.00191929\n",
      "Iteration 4557, loss = 0.00191806\n",
      "Iteration 4558, loss = 0.00191684\n",
      "Iteration 4559, loss = 0.00191562\n",
      "Iteration 4560, loss = 0.00191440\n",
      "Iteration 4561, loss = 0.00191318\n",
      "Iteration 4562, loss = 0.00191196\n",
      "Iteration 4563, loss = 0.00191074\n",
      "Iteration 4564, loss = 0.00190953\n",
      "Iteration 4565, loss = 0.00190831\n",
      "Iteration 4566, loss = 0.00190710\n",
      "Iteration 4567, loss = 0.00190588\n",
      "Iteration 4568, loss = 0.00190467\n",
      "Iteration 4569, loss = 0.00190346\n",
      "Iteration 4570, loss = 0.00190225\n",
      "Iteration 4571, loss = 0.00190104\n",
      "Iteration 4572, loss = 0.00189983\n",
      "Iteration 4573, loss = 0.00189862\n",
      "Iteration 4574, loss = 0.00189742\n",
      "Iteration 4575, loss = 0.00189621\n",
      "Iteration 4576, loss = 0.00189501\n",
      "Iteration 4577, loss = 0.00189381\n",
      "Iteration 4578, loss = 0.00189260\n",
      "Iteration 4579, loss = 0.00189140\n",
      "Iteration 4580, loss = 0.00189020\n",
      "Iteration 4581, loss = 0.00188900\n",
      "Iteration 4582, loss = 0.00188781\n",
      "Iteration 4583, loss = 0.00188661\n",
      "Iteration 4584, loss = 0.00188541\n",
      "Iteration 4585, loss = 0.00188422\n",
      "Iteration 4586, loss = 0.00188302\n",
      "Iteration 4587, loss = 0.00188183\n",
      "Iteration 4588, loss = 0.00188064\n",
      "Iteration 4589, loss = 0.00187945\n",
      "Iteration 4590, loss = 0.00187826\n",
      "Iteration 4591, loss = 0.00187707\n",
      "Iteration 4592, loss = 0.00187588\n",
      "Iteration 4593, loss = 0.00187469\n",
      "Iteration 4594, loss = 0.00187351\n",
      "Iteration 4595, loss = 0.00187232\n",
      "Iteration 4596, loss = 0.00187114\n",
      "Iteration 4597, loss = 0.00186996\n",
      "Iteration 4598, loss = 0.00186878\n",
      "Iteration 4599, loss = 0.00186759\n",
      "Iteration 4600, loss = 0.00186641\n",
      "Iteration 4601, loss = 0.00186524\n",
      "Iteration 4602, loss = 0.00186406\n",
      "Iteration 4603, loss = 0.00186288\n",
      "Iteration 4604, loss = 0.00186171\n",
      "Iteration 4605, loss = 0.00186053\n",
      "Iteration 4606, loss = 0.00185936\n",
      "Iteration 4607, loss = 0.00185818\n",
      "Iteration 4608, loss = 0.00185701\n",
      "Iteration 4609, loss = 0.00185584\n",
      "Iteration 4610, loss = 0.00185467\n",
      "Iteration 4611, loss = 0.00185350\n",
      "Iteration 4612, loss = 0.00185233\n",
      "Iteration 4613, loss = 0.00185117\n",
      "Iteration 4614, loss = 0.00185000\n",
      "Iteration 4615, loss = 0.00184884\n",
      "Iteration 4616, loss = 0.00184767\n",
      "Iteration 4617, loss = 0.00184651\n",
      "Iteration 4618, loss = 0.00184535\n",
      "Iteration 4619, loss = 0.00184419\n",
      "Iteration 4620, loss = 0.00184303\n",
      "Iteration 4621, loss = 0.00184187\n",
      "Iteration 4622, loss = 0.00184071\n",
      "Iteration 4623, loss = 0.00183955\n",
      "Iteration 4624, loss = 0.00183840\n",
      "Iteration 4625, loss = 0.00183724\n",
      "Iteration 4626, loss = 0.00183609\n",
      "Iteration 4627, loss = 0.00183493\n",
      "Iteration 4628, loss = 0.00183378\n",
      "Iteration 4629, loss = 0.00183263\n",
      "Iteration 4630, loss = 0.00183148\n",
      "Iteration 4631, loss = 0.00183033\n",
      "Iteration 4632, loss = 0.00182918\n",
      "Iteration 4633, loss = 0.00182804\n",
      "Iteration 4634, loss = 0.00182689\n",
      "Iteration 4635, loss = 0.00182574\n",
      "Iteration 4636, loss = 0.00182460\n",
      "Iteration 4637, loss = 0.00182346\n",
      "Iteration 4638, loss = 0.00182231\n",
      "Iteration 4639, loss = 0.00182117\n",
      "Iteration 4640, loss = 0.00182003\n",
      "Iteration 4641, loss = 0.00181889\n",
      "Iteration 4642, loss = 0.00181775\n",
      "Iteration 4643, loss = 0.00181661\n",
      "Iteration 4644, loss = 0.00181548\n",
      "Iteration 4645, loss = 0.00181434\n",
      "Iteration 4646, loss = 0.00181321\n",
      "Iteration 4647, loss = 0.00181207\n",
      "Iteration 4648, loss = 0.00181094\n",
      "Iteration 4649, loss = 0.00180981\n",
      "Iteration 4650, loss = 0.00180868\n",
      "Iteration 4651, loss = 0.00180755\n",
      "Iteration 4652, loss = 0.00180642\n",
      "Iteration 4653, loss = 0.00180529\n",
      "Iteration 4654, loss = 0.00180416\n",
      "Iteration 4655, loss = 0.00180304\n",
      "Iteration 4656, loss = 0.00180191\n",
      "Iteration 4657, loss = 0.00180079\n",
      "Iteration 4658, loss = 0.00179966\n",
      "Iteration 4659, loss = 0.00179854\n",
      "Iteration 4660, loss = 0.00179742\n",
      "Iteration 4661, loss = 0.00179630\n",
      "Iteration 4662, loss = 0.00179518\n",
      "Iteration 4663, loss = 0.00179406\n",
      "Iteration 4664, loss = 0.00179294\n",
      "Iteration 4665, loss = 0.00179182\n",
      "Iteration 4666, loss = 0.00179071\n",
      "Iteration 4667, loss = 0.00178959\n",
      "Iteration 4668, loss = 0.00178848\n",
      "Iteration 4669, loss = 0.00178737\n",
      "Iteration 4670, loss = 0.00178625\n",
      "Iteration 4671, loss = 0.00178514\n",
      "Iteration 4672, loss = 0.00178403\n",
      "Iteration 4673, loss = 0.00178292\n",
      "Iteration 4674, loss = 0.00178181\n",
      "Iteration 4675, loss = 0.00178071\n",
      "Iteration 4676, loss = 0.00177960\n",
      "Iteration 4677, loss = 0.00177849\n",
      "Iteration 4678, loss = 0.00177739\n",
      "Iteration 4679, loss = 0.00177628\n",
      "Iteration 4680, loss = 0.00177518\n",
      "Iteration 4681, loss = 0.00177408\n",
      "Iteration 4682, loss = 0.00177298\n",
      "Iteration 4683, loss = 0.00177188\n",
      "Iteration 4684, loss = 0.00177078\n",
      "Iteration 4685, loss = 0.00176968\n",
      "Iteration 4686, loss = 0.00176858\n",
      "Iteration 4687, loss = 0.00176749\n",
      "Iteration 4688, loss = 0.00176639\n",
      "Iteration 4689, loss = 0.00176530\n",
      "Iteration 4690, loss = 0.00176420\n",
      "Iteration 4691, loss = 0.00176311\n",
      "Iteration 4692, loss = 0.00176202\n",
      "Iteration 4693, loss = 0.00176093\n",
      "Iteration 4694, loss = 0.00175984\n",
      "Iteration 4695, loss = 0.00175875\n",
      "Iteration 4696, loss = 0.00175766\n",
      "Iteration 4697, loss = 0.00175657\n",
      "Iteration 4698, loss = 0.00175548\n",
      "Iteration 4699, loss = 0.00175440\n",
      "Iteration 4700, loss = 0.00175331\n",
      "Iteration 4701, loss = 0.00175223\n",
      "Iteration 4702, loss = 0.00175115\n",
      "Iteration 4703, loss = 0.00175006\n",
      "Iteration 4704, loss = 0.00174898\n",
      "Iteration 4705, loss = 0.00174790\n",
      "Iteration 4706, loss = 0.00174682\n",
      "Iteration 4707, loss = 0.00174575\n",
      "Iteration 4708, loss = 0.00174467\n",
      "Iteration 4709, loss = 0.00174359\n",
      "Iteration 4710, loss = 0.00174252\n",
      "Iteration 4711, loss = 0.00174144\n",
      "Iteration 4712, loss = 0.00174037\n",
      "Iteration 4713, loss = 0.00173929\n",
      "Iteration 4714, loss = 0.00173822\n",
      "Iteration 4715, loss = 0.00173715\n",
      "Iteration 4716, loss = 0.00173608\n",
      "Iteration 4717, loss = 0.00173501\n",
      "Iteration 4718, loss = 0.00173394\n",
      "Iteration 4719, loss = 0.00173287\n",
      "Iteration 4720, loss = 0.00173181\n",
      "Iteration 4721, loss = 0.00173074\n",
      "Iteration 4722, loss = 0.00172968\n",
      "Iteration 4723, loss = 0.00172861\n",
      "Iteration 4724, loss = 0.00172755\n",
      "Iteration 4725, loss = 0.00172649\n",
      "Iteration 4726, loss = 0.00172542\n",
      "Iteration 4727, loss = 0.00172436\n",
      "Iteration 4728, loss = 0.00172330\n",
      "Iteration 4729, loss = 0.00172224\n",
      "Iteration 4730, loss = 0.00172119\n",
      "Iteration 4731, loss = 0.00172013\n",
      "Iteration 4732, loss = 0.00171907\n",
      "Iteration 4733, loss = 0.00171802\n",
      "Iteration 4734, loss = 0.00171696\n",
      "Iteration 4735, loss = 0.00171591\n",
      "Iteration 4736, loss = 0.00171486\n",
      "Iteration 4737, loss = 0.00171380\n",
      "Iteration 4738, loss = 0.00171275\n",
      "Iteration 4739, loss = 0.00171170\n",
      "Iteration 4740, loss = 0.00171065\n",
      "Iteration 4741, loss = 0.00170961\n",
      "Iteration 4742, loss = 0.00170856\n",
      "Iteration 4743, loss = 0.00170751\n",
      "Iteration 4744, loss = 0.00170647\n",
      "Iteration 4745, loss = 0.00170542\n",
      "Iteration 4746, loss = 0.00170438\n",
      "Iteration 4747, loss = 0.00170333\n",
      "Iteration 4748, loss = 0.00170229\n",
      "Iteration 4749, loss = 0.00170125\n",
      "Iteration 4750, loss = 0.00170021\n",
      "Iteration 4751, loss = 0.00169917\n",
      "Iteration 4752, loss = 0.00169813\n",
      "Iteration 4753, loss = 0.00169709\n",
      "Iteration 4754, loss = 0.00169605\n",
      "Iteration 4755, loss = 0.00169502\n",
      "Iteration 4756, loss = 0.00169398\n",
      "Iteration 4757, loss = 0.00169295\n",
      "Iteration 4758, loss = 0.00169191\n",
      "Iteration 4759, loss = 0.00169088\n",
      "Iteration 4760, loss = 0.00168985\n",
      "Iteration 4761, loss = 0.00168882\n",
      "Iteration 4762, loss = 0.00168779\n",
      "Iteration 4763, loss = 0.00168676\n",
      "Iteration 4764, loss = 0.00168573\n",
      "Iteration 4765, loss = 0.00168470\n",
      "Iteration 4766, loss = 0.00168367\n",
      "Iteration 4767, loss = 0.00168265\n",
      "Iteration 4768, loss = 0.00168162\n",
      "Iteration 4769, loss = 0.00168060\n",
      "Iteration 4770, loss = 0.00167957\n",
      "Iteration 4771, loss = 0.00167855\n",
      "Iteration 4772, loss = 0.00167753\n",
      "Iteration 4773, loss = 0.00167651\n",
      "Iteration 4774, loss = 0.00167549\n",
      "Iteration 4775, loss = 0.00167447\n",
      "Iteration 4776, loss = 0.00167345\n",
      "Iteration 4777, loss = 0.00167243\n",
      "Iteration 4778, loss = 0.00167142\n",
      "Iteration 4779, loss = 0.00167040\n",
      "Iteration 4780, loss = 0.00166938\n",
      "Iteration 4781, loss = 0.00166837\n",
      "Iteration 4782, loss = 0.00166736\n",
      "Iteration 4783, loss = 0.00166634\n",
      "Iteration 4784, loss = 0.00166533\n",
      "Iteration 4785, loss = 0.00166432\n",
      "Iteration 4786, loss = 0.00166331\n",
      "Iteration 4787, loss = 0.00166230\n",
      "Iteration 4788, loss = 0.00166129\n",
      "Iteration 4789, loss = 0.00166028\n",
      "Iteration 4790, loss = 0.00165928\n",
      "Iteration 4791, loss = 0.00165827\n",
      "Iteration 4792, loss = 0.00165727\n",
      "Iteration 4793, loss = 0.00165626\n",
      "Iteration 4794, loss = 0.00165526\n",
      "Iteration 4795, loss = 0.00165425\n",
      "Iteration 4796, loss = 0.00165325\n",
      "Iteration 4797, loss = 0.00165225\n",
      "Iteration 4798, loss = 0.00165125\n",
      "Iteration 4799, loss = 0.00165025\n",
      "Iteration 4800, loss = 0.00164925\n",
      "Iteration 4801, loss = 0.00164825\n",
      "Iteration 4802, loss = 0.00164726\n",
      "Iteration 4803, loss = 0.00164626\n",
      "Iteration 4804, loss = 0.00164527\n",
      "Iteration 4805, loss = 0.00164427\n",
      "Iteration 4806, loss = 0.00164328\n",
      "Iteration 4807, loss = 0.00164228\n",
      "Iteration 4808, loss = 0.00164129\n",
      "Iteration 4809, loss = 0.00164030\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "accuracy for test data: 92.59%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.92      1.00      0.96        22\n",
      "           2       0.93      0.81      0.87        16\n",
      "           3       0.94      0.94      0.94        16\n",
      "\n",
      "    accuracy                           0.93        54\n",
      "   macro avg       0.93      0.92      0.92        54\n",
      "weighted avg       0.93      0.93      0.92        54\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# hidden_layers = (512,) # one hidden layer\n",
    "# activation = 'relu' # the default\n",
    "hidden_layers = (20,) # one hidden layer\n",
    "activation = 'logistic' # the default= 'relu'\n",
    "opts = dict(hidden_layer_sizes = hidden_layers, verbose = True, \\\n",
    "        activation = activation, tol = 1e-6, max_iter = int(1e6))\n",
    "# solver = 'sgd' # not efficient, need more tuning\n",
    "# solver = 'lbfgs' # not suitable here\n",
    "solver = 'adam' # default solver\n",
    "clf_MLP = MLPClassifier(solver = solver, **opts) \n",
    "clf_MLP.fit(X_train, y_train) # X_train = (samples, features), y_train = (samples,)\n",
    "predictions = clf_MLP.predict(X_test) #output a class label for each sample\n",
    "# print(\"accuracy for tested data: {:.2f}%\".format(100*np.mean(y_test_hat == y_test)))\n",
    "print(\"accuracy for test data: {:.2f}%\".format(100*clf_MLP.score(X_test, y_test)))\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4,3))\n",
    "plt.plot(clf_MLP.loss_curve_)\n",
    "plt.grid(True)\n",
    "plt.title('Training Loss Curve')\n",
    "plt.xlabel('Iter.')\n",
    "plt.ylabel('Fitting Loss')\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrix\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4,4))\n",
    "# Confusion matrix\n",
    "title = 'Testing score ={:.2f}%'.format(100*clf_MLP.score(X_test, y_test))\n",
    "disp = ConfusionMatrixDisplay.from_estimator(\n",
    "        clf_MLP,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        xticks_rotation=45, #'vertical',\n",
    "        # display_labels=class_names,\n",
    "        cmap=plt.cm.Blues,\n",
    "        normalize='true',\n",
    "        ax = ax\n",
    "    )\n",
    "disp.ax_.set_title(title)\n",
    "plt.show() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
